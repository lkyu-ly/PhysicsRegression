# symbolicregression - ç¬¦å·å›å½’æ ¸å¿ƒæ¨¡å—

ğŸ“ **Root** > **symbolicregression**

---

## ğŸ“‹ ç›®å½•

- [æ¨¡å—æ¦‚è§ˆ](#æ¨¡å—æ¦‚è§ˆ)
- [ç›®å½•ç»“æ„](#ç›®å½•ç»“æ„)
- [æ ¸å¿ƒæ¶æ„](#æ ¸å¿ƒæ¶æ„)
- [å…¬å…±æ¥å£](#å…¬å…±æ¥å£)
- [å­æ¨¡å—è¯¦è§£](#å­æ¨¡å—è¯¦è§£)
- [ä¾èµ–å…³ç³»](#ä¾èµ–å…³ç³»)
- [å¼€å‘æŒ‡å—](#å¼€å‘æŒ‡å—)

---

## æ¨¡å—æ¦‚è§ˆ

### èŒè´£èŒƒå›´

`symbolicregression` æ˜¯é¡¹ç›®çš„æ ¸å¿ƒæ¨¡å—,è´Ÿè´£:

1. **ç«¯åˆ°ç«¯ç¬¦å·å›å½’**: ä»æ•°æ®ç‚¹é¢„æµ‹æ•°å­¦å…¬å¼
2. **Transformer æ¨¡å‹å®šä¹‰**: ç¼–ç å™¨-è§£ç å™¨æ¶æ„
3. **è®­ç»ƒç¯å¢ƒ**: æ•°æ®ç”Ÿæˆã€å…¬å¼ç¼–ç ã€è¿ç®—ç¬¦å®šä¹‰
4. **ä¼˜åŒ–ç­–ç•¥**: MCTS (è’™ç‰¹å¡æ´›æ ‘æœç´¢) å’Œ GP (é—ä¼ ç¼–ç¨‹)
5. **æ¨¡å‹è®­ç»ƒ**: å®Œæ•´çš„è®­ç»ƒå¾ªç¯å’Œæ£€æŸ¥ç‚¹ç®¡ç†

### æ ¸å¿ƒç‰¹æ€§

- **åºåˆ—åˆ°åºåˆ—å­¦ä¹ **: å°†æ•°æ®ç‚¹åºåˆ—æ˜ å°„åˆ°å…¬å¼æ ‘çš„å‰ç¼€è¡¨ç¤º
- **ç‰©ç†æç¤ºæ”¯æŒ**: é›†æˆç‰©ç†å•ä½ã€å¤æ‚åº¦ã€ä¸€å…ƒè¿ç®—ç­‰å…ˆéªŒçŸ¥è¯†
- **åµŒå…¥ç­–ç•¥**: LinearPointEmbedder (å½“å‰ç‰ˆæœ¬ä»…å®ç°äº†æ­¤åµŒå…¥å™¨)
- **ç¬¦å·è®¡ç®—å¼•æ“**: åŸºäº SymPy çš„å…¬å¼è§£æå’Œè¯„ä¼°
- **çµæ´»çš„ä¼˜åŒ–**: æ”¯æŒ MCTS å’Œ GA çš„ç»„åˆä½¿ç”¨

---

## ç›®å½•ç»“æ„

```
symbolicregression/
â”œâ”€â”€ CLAUDE.md                    # æœ¬æ–‡æ¡£
â”œâ”€â”€ __init__.py                  # æ¨¡å—åˆå§‹åŒ–
â”‚
â”œâ”€â”€ model/                       # ğŸ§  æ¨¡å‹æ¶æ„å®šä¹‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ transformer.py           # Transformer ç¼–ç å™¨-è§£ç å™¨ (1242 è¡Œ)
â”‚   â”œâ”€â”€ embedders.py             # æ•°æ®åµŒå…¥å±‚ (LinearPointEmbedder)
â”‚   â”œâ”€â”€ model_wrapper.py         # æ¨¡å‹åŒ…è£…å™¨ (è®­ç»ƒ/æ¨ç†æ¥å£)
â”‚   â””â”€â”€ sklearn_wrapper.py       # Scikit-learn é£æ ¼ API
â”‚
â”œâ”€â”€ envs/                        # ğŸŒ ç¯å¢ƒä¸æ•°æ®ç”Ÿæˆ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ environment.py           # ä¸»ç¯å¢ƒç±» (946 è¡Œ)
â”‚   â”œâ”€â”€ generators.py            # å…¬å¼ç”Ÿæˆå™¨ (éšæœºé‡‡æ ·)
â”‚   â”œâ”€â”€ encoders.py              # åºåˆ—ç¼–ç å™¨
â”‚   â”œâ”€â”€ operators.py             # æ•°å­¦è¿ç®—ç¬¦å®šä¹‰
â”‚   â””â”€â”€ simplifiers.py           # å…¬å¼ç®€åŒ–å™¨
â”‚
â”œâ”€â”€ MCTS/                        # ğŸŒ² è’™ç‰¹å¡æ´›æ ‘æœç´¢
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ mcts.py                  # MCTS ç®—æ³•å®ç°
â”‚
â”œâ”€â”€ GA/                          # ğŸ§¬ é—ä¼ ç®—æ³•ä¼˜åŒ–
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ga.py                    # ä¸» GA é€»è¾‘
â”‚   â”œâ”€â”€ population.py            # ç§ç¾¤ç®¡ç†
â”‚   â””â”€â”€ operators.py             # é—ä¼ ç®—å­ (äº¤å‰ã€å˜å¼‚)
â”‚
â”œâ”€â”€ trainer.py                   # ğŸ“ è®­ç»ƒå¾ªç¯ (745 è¡Œ)
â”œâ”€â”€ metrics.py                   # ğŸ“Š è¯„ä¼°æŒ‡æ ‡ (RÂ², ç¬¦å·ç²¾åº¦ç­‰)
â”œâ”€â”€ utils.py                     # ğŸ”§ å·¥å…·å‡½æ•°
â”œâ”€â”€ optim.py                     # âš™ï¸ è‡ªå®šä¹‰ä¼˜åŒ–å™¨
â”œâ”€â”€ logger.py                    # ğŸ“ æ—¥å¿—è®°å½•
â””â”€â”€ slurm.py                     # ğŸ–¥ï¸ SLURM é›†ç¾¤æ”¯æŒ

```

---

## æ ¸å¿ƒæ¶æ„

### æ•´ä½“æ•°æ®æµ

```mermaid
graph LR
    A[è®­ç»ƒæ•°æ®ç”Ÿæˆ] --> B[Embedder<br/>æ•°æ®åµŒå…¥]
    B --> C[Encoder<br/>Transformerç¼–ç å™¨]
    C --> D[Decoder<br/>Transformerè§£ç å™¨]
    D --> E[å…¬å¼æ ‘<br/>å‰ç¼€è¡¨ç¤º]
    E --> F[SymPyè§£æ]
    F --> G[è¯„ä¼°ä¸åé¦ˆ]
    G --> H{éœ€è¦ä¼˜åŒ–?}
    H -->|MCTS| I[æ ‘æœç´¢ç²¾ç‚¼]
    H -->|GP| J[é—ä¼ ç¼–ç¨‹ä¼˜åŒ–]
    I --> K[æœ€ç»ˆå…¬å¼]
    J --> K

    style A fill:#e1f5ff
    style E fill:#fff9c4
    style K fill:#c8e6c9
```

### è®­ç»ƒæµç¨‹

```mermaid
sequenceDiagram
    participant Trainer
    participant Env as Environment
    participant Model
    participant Optimizer

    loop æ¯ä¸ª epoch
        loop æ¯ä¸ª batch
            Trainer->>Env: gen_expr(train=True)
            Env-->>Trainer: (x, y, formula, hints)
            Trainer->>Model: forward(x, hints)
            Model-->>Trainer: predicted_formula
            Trainer->>Trainer: compute_loss(pred, target)
            Trainer->>Optimizer: backward + step
        end
        Trainer->>Trainer: validation()
        Trainer->>Trainer: save_checkpoint()
    end
```

---

## å…¬å…±æ¥å£

### æ ¸å¿ƒç±»

#### 1. `TransformerModel` (model/transformer.py)

**ä½ç½®**: `symbolicregression/model/transformer.py:182-1028`

**æ ¸å¿ƒ Transformer æ¨¡å‹ç±»**ï¼Œå®ç°å®Œæ•´çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚

**å…³é”®æ–¹æ³•**:
```python
class TransformerModel(nn.Module):
    def __init__(self, params, id2word, is_encoder, is_decoder, with_output):
        """
        åˆå§‹åŒ– Transformer æ¨¡å‹

        å‚æ•°:
            params: æ¨¡å‹é…ç½®å‚æ•°
            id2word: è¯æ±‡è¡¨ç´¢å¼•åˆ°è¯çš„æ˜ å°„
            is_encoder: æ˜¯å¦ä¸ºç¼–ç å™¨
            is_decoder: æ˜¯å¦ä¸ºè§£ç å™¨
            with_output: æ˜¯å¦åŒ…å«è¾“å‡ºæŠ•å½±å±‚
        """

    def fwd(self, mode, **kwargs):
        """
        é€šç”¨å‰å‘ä¼ æ’­æ–¹æ³•

        å‚æ•°:
            mode: 'fwd'(è®­ç»ƒ) æˆ– 'predict'(æ¨ç†)
            **kwargs: x, lengths, causal, src_enc, src_lenç­‰

        è¿”å›:
            tensor: è¾“å‡ºå¼ é‡ [seq_len, batch, dim] æˆ– [batch, seq_len, vocab]
        """

    def predict(self, tensor, pred_mask, y, get_scores):
        """
        é¢„æµ‹ä¸‹ä¸€ä¸ªtoken

        å‚æ•°:
            tensor: ç¼–ç å™¨è¾“å‡º
            pred_mask: é¢„æµ‹æ©ç 
            y: ç›®æ ‡åºåˆ—
            get_scores: æ˜¯å¦è¿”å›åˆ†æ•°
        """

    def generate(self, src_enc, src_len, max_len=200, sample_temperature=None):
        """
        è‡ªå›å½’ç”Ÿæˆå…¬å¼åºåˆ—

        å‚æ•°:
            src_enc: ç¼–ç å™¨è¾“å‡º [src_len, batch, dim]
            src_len: æºåºåˆ—é•¿åº¦
            max_len: æœ€å¤§ç”Ÿæˆé•¿åº¦
            sample_temperature: é‡‡æ ·æ¸©åº¦

        è¿”å›:
            generated: ç”Ÿæˆçš„åºåˆ— [batch, tgt_len]
            lengths: åºåˆ—é•¿åº¦
        """

    def generate_beam(self, src_enc, src_len, beam_size=10, length_penalty=1.0):
        """
        æŸæœç´¢ç”Ÿæˆï¼ˆæ¨ç†æ—¶çš„ä¸»è¦æ–¹æ³•ï¼‰

        å‚æ•°:
            src_enc: ç¼–ç å™¨è¾“å‡º
            src_len: æºåºåˆ—é•¿åº¦
            beam_size: æŸå¤§å° (é»˜è®¤10)
            length_penalty: é•¿åº¦æƒ©ç½šç³»æ•°

        è¿”å›:
            generated: ç”Ÿæˆçš„åºåˆ— [batch, beam_size, tgt_len]
            lengths: åºåˆ—é•¿åº¦
        """
```

**é‡è¦é…ç½®**:
- `enc_emb_dim` / `dec_emb_dim`: ç¼–ç å™¨/è§£ç å™¨åµŒå…¥ç»´åº¦ (é»˜è®¤ 512)
- `n_enc_layers` / `n_dec_layers`: ç¼–ç å™¨/è§£ç å™¨å±‚æ•° (é»˜è®¤ 2/16)
- `n_enc_heads` / `n_dec_heads`: å¤šå¤´æ³¨æ„åŠ›å¤´æ•° (é»˜è®¤ 16)
- `dropout`: Dropout ç‡ (é»˜è®¤ 0.1)
- `attention_dropout`: æ³¨æ„åŠ› Dropout ç‡

#### 2. `SymbolicTransformerRegressor` (model/sklearn_wrapper.py)

**ä½ç½®**: `symbolicregression/model/sklearn_wrapper.py:43-end`

**Scikit-learn é£æ ¼çš„åŒ…è£…å™¨ç±»**ï¼Œæä¾›æ›´é«˜å±‚çš„ API æ¥å£ã€‚

**å…³é”®æ–¹æ³•**:
```python
class SymbolicTransformerRegressor(BaseEstimator):
    def __init__(self, env, modules, params):
        """
        åˆå§‹åŒ–åŒ…è£…å™¨

        å‚æ•°:
            env: Environment å®ä¾‹
            modules: {'encoder', 'decoder', 'embedder'}
            params: æ¨¡å‹å‚æ•°
        """

    def fit(self, X, y):
        """è®­ç»ƒæ¨¡å‹ (å½“å‰ç‰ˆæœ¬æœªå®ç°)"""

    def predict(self, X, y):
        """
        é¢„æµ‹å…¬å¼

        å‚æ•°:
            X: è¾“å…¥æ•°æ® [n_samples, n_features]
            y: ç›®æ ‡å€¼ [n_samples,]

        è¿”å›:
            predictions: é¢„æµ‹çš„å…¬å¼åˆ—è¡¨
        """
```

**ç”¨é€”**: ä¸»è¦ç”¨äºä¸ PhyReg ç±»é›†æˆï¼Œæä¾›ç»Ÿä¸€çš„æ¨ç†æ¥å£

#### 3. `Environment` (envs/environment.py)

æ•°æ®ç”Ÿæˆå’Œå…¬å¼ç®¡ç†ç¯å¢ƒã€‚

**å…³é”®æ–¹æ³•**:
```python
class Environment:
    def __init__(self, params):
        """
        åˆå§‹åŒ–ç¯å¢ƒ

        è®¾ç½®:
            - è¿ç®—ç¬¦é›†åˆ
            - å˜é‡èŒƒå›´
            - ç‰©ç†å•ä½ç³»ç»Ÿ
        """

    def gen_expr(self, train=True):
        """
        ç”Ÿæˆè®­ç»ƒ/æµ‹è¯•æ•°æ®

        è¿”å›:
            {
                'x': è¾“å…¥æ•°æ®ç‚¹,
                'y': ç›®æ ‡å€¼,
                'tree': å…¬å¼æ ‘,
                'hints': ç‰©ç†æç¤º
            }
        """

    def word_to_infix(self, words, is_float=True):
        """
        å°†å‰ç¼€è¡¨ç¤ºè½¬æ¢ä¸ºä¸­ç¼€è¡¨è¾¾å¼

        å‚æ•°:
            words: å‰ç¼€åºåˆ— ['add', 'x_0', 'sin', 'x_1']

        è¿”å›:
            infix: ä¸­ç¼€è¡¨è¾¾å¼ "x_0 + sin(x_1)"
        """
```

**é…ç½®é€‰é¡¹**:
- `operators`: å¯ç”¨è¿ç®—ç¬¦ (add, mul, sin, exp, etc.)
- `max_ops`: æœ€å¤§è¿ç®—ç¬¦æ•°é‡
- `variables`: å˜é‡åˆ—è¡¨ ['x_0', 'x_1', ...]
- `rewrite_functions`: SymPy ç®€åŒ–è§„åˆ™

#### 4. `Trainer` (trainer.py)

å®Œæ•´çš„è®­ç»ƒå¾ªç¯å®ç°ã€‚

**å…³é”®æ–¹æ³•**:
```python
class Trainer:
    def __init__(self, modules, env, params):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨

        å‚æ•°:
            modules: {'encoder', 'decoder', 'embedder'}
            env: Environment å®ä¾‹
            params: è®­ç»ƒå‚æ•°
        """

    def run(self):
        """
        æ‰§è¡Œå®Œæ•´è®­ç»ƒæµç¨‹

        - epoch å¾ªç¯
        - batch è¿­ä»£
        - éªŒè¯å’Œä¿å­˜
        """

    def step(self, batch):
        """
        å•ä¸ªè®­ç»ƒæ­¥éª¤

        è¿”å›:
            loss: å½“å‰ batch çš„æŸå¤±
        """
```

---

## å­æ¨¡å—è¯¦è§£

### 1. model/ - æ¨¡å‹æ¶æ„

#### transformer.py (1242 è¡Œ)

**æ ¸å¿ƒç±»**:
- `TransformerModel`: ä¸»æ¨¡å‹ç±»,ç»„åˆç¼–ç å™¨å’Œè§£ç å™¨
- `TransformerEncoder`: Transformer ç¼–ç å™¨
- `TransformerDecoder`: Transformer è§£ç å™¨
- `MultiHeadAttention`: è‡ªå®šä¹‰å¤šå¤´æ³¨æ„åŠ›
- `PositionwiseFeedForward`: å‰é¦ˆç½‘ç»œ

**å…³é”®ç‰¹æ€§**:
- ä½ç½®ç¼–ç  (Sinusoidal æˆ–å­¦ä¹ å‹)
- å±‚å½’ä¸€åŒ–
- Dropout æ­£åˆ™åŒ–
- æŸæœç´¢ç”Ÿæˆ

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from symbolicregression.model import build_modules

# æ„å»ºæ¨¡å‹
modules = build_modules(env, params)
encoder = modules['encoder']
decoder = modules['decoder']
embedder = modules['embedder']

# å‰å‘ä¼ æ’­
embedded = embedder(x_data)  # [B, L, D]
encoded = encoder(embedded)  # [B, L, D]
output = decoder(encoded, target_formula)  # [B, T, V]
```

---

### è¯¦ç»†ç½‘ç»œæ¶æ„

æœ¬èŠ‚æä¾›å„ç¥ç»ç½‘ç»œç»„ä»¶çš„è¯¦ç»†å±‚çº§ç»“æ„ã€ç»´åº¦æµè½¬å’Œå‚æ•°é…ç½®ã€‚

#### A. MultiHeadAttention è¯¦ç»†æ¶æ„

**ä½ç½®**: `transformer.py:58-157`

**å±‚çº§ç»“æ„**:
```
è¾“å…¥: x [seq_len, batch, dim]
  â†“
Q/K/V æŠ•å½±å±‚:
  - q_lin: Linear(dim â†’ dim)
  - k_lin: Linear(src_dim â†’ dim)
  - v_lin: Linear(src_dim â†’ dim)
  â†“
å¤šå¤´åˆ†è£‚:
  - é‡å¡‘ä¸º [seq_len, batch * n_heads, dim_per_head]
  - dim_per_head = dim / n_heads
  â†“
ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›:
  - scores = (Q @ K.T) / sqrt(dim_per_head)
  - å¯é€‰: å½’ä¸€åŒ–
  - weights = softmax(scores + mask)
  - weights = dropout(weights)
  - context = weights @ V
  â†“
å¤šå¤´åˆå¹¶:
  - é‡å¡‘ä¸º [seq_len, batch, dim]
  â†“
è¾“å‡ºæŠ•å½±:
  - out_lin: Linear(dim â†’ dim)
  â†“
è¾“å‡º: [seq_len, batch, dim]
```

**å‚æ•°é…ç½®è¡¨**:
| å±‚åç§° | ç±»å‹ | è¾“å…¥ç»´åº¦ | è¾“å‡ºç»´åº¦ | å‚æ•°æ•°é‡ |
|--------|------|---------|---------|---------|
| q_lin | Linear | dim | dim | dimÂ² + dim |
| k_lin | Linear | src_dim | dim | src_dimÃ—dim + dim |
| v_lin | Linear | src_dim | dim | src_dimÃ—dim + dim |
| out_lin | Linear | dim | dim | dimÂ² + dim |

**å…³é”®å‚æ•°**:
- `n_heads`: æ³¨æ„åŠ›å¤´æ•° (é»˜è®¤ 16)
- `dim`: æ¨¡å‹ç»´åº¦ (é»˜è®¤ 512)
- `dropout`: Dropout ç‡ (é»˜è®¤ 0.1)

---

#### B. TransformerFFN è¯¦ç»†æ¶æ„

**ä½ç½®**: `transformer.py:160-179`

**å±‚çº§ç»“æ„**:
```
è¾“å…¥: x [seq_len, batch, dim]
  â†“
ç¬¬ä¸€å±‚: lin1 â†’ Linear(dim â†’ hidden_dim) â†’ ReLU â†’ Dropout
  â†“
ä¸­é—´å±‚ (å¯é€‰): midlin[i] â†’ Linear â†’ ReLU â†’ Dropout
  â†“
æœ€åä¸€å±‚: lin2 â†’ Linear(hidden_dim â†’ dim) â†’ Dropout
  â†“
è¾“å‡º: [seq_len, batch, dim]
```

**å…³é”®å‚æ•°**:
- `dim`: è¾“å…¥/è¾“å‡ºç»´åº¦ (é»˜è®¤ 512)
- `hidden_dim`: éšå±‚ç»´åº¦ (é»˜è®¤ 2048 = 4Ã—dim)
- `hidden_layers`: æ€»å±‚æ•° (é»˜è®¤ 2)
- `dropout`: Dropout ç‡ (é»˜è®¤ 0.1)

---

#### C. TransformerModel å®Œæ•´æ¶æ„

**ä½ç½®**: `transformer.py:182-1028`

**æ•´ä½“ç»“æ„**:
```
è¯åµŒå…¥å±‚ (å¯é€‰):
  - embeddings: Embedding(n_words, dim)
  - position_embeddings: Sinusoidal/Learnable
  - layer_norm_emb: LayerNorm(dim)

Transformerå±‚ (Ã— n_layers):
  - attentions[i]: MultiHeadAttention (è‡ªæ³¨æ„åŠ›)
  - layer_norm1[i]: LayerNorm(dim)
  - [ä»…è§£ç å™¨] encoder_attn[i]: MultiHeadAttention (äº¤å‰æ³¨æ„åŠ›)
  - [ä»…è§£ç å™¨] layer_norm15[i]: LayerNorm(dim)
  - ffns[i]: TransformerFFN
  - layer_norm2[i]: LayerNorm(dim)

è¾“å‡ºå±‚ (with_output=True):
  - proj: Linear(dim, n_words)
```

**å…³é”®é…ç½®**:
- **ç¼–ç å™¨**: `is_encoder=True, is_decoder=False, with_output=False`
- **è§£ç å™¨**: `is_encoder=False, is_decoder=True, with_output=True`

**ç‰©ç†å•ä½ç¼–ç  (double-seq æ¨¡å¼)**:
```python
# 5ç»´ç‰©ç†å•ä½å‘é‡ [kg, m, s, T, V]
units_embedding = units_enc(units_vector)  # [5] â†’ [dim]
tensor = tensor + units_embedding
```

---

#### D. LinearPointEmbedder è¯¦ç»†æ¶æ„

**ä½ç½®**: `embedders.py:41-end`

**å±‚çº§ç»“æ„**:
```
è¾“å…¥: data_points [batch, n_points, n_vars+1]
  â†“
1. æµ®ç‚¹æ•°ç¼–ç  (num_encode):
   æ¯ä¸ªæµ®ç‚¹æ•° â†’ [sign, exp, mantissa_digits...]
  â†“
2. åˆå§‹åµŒå…¥:
   embed: Linear((n_vars+1)*num_encoding_dim â†’ emb_dim)
  â†“
3. å‹ç¼©ç½‘ç»œ (å¯é€‰):
   compress: Linear â†’ ReLU â†’ Linear â†’ ReLU...
  â†“
4. ç‰©ç†æç¤ºç¼–ç  (hint_encode):
   - units: Linear(5 â†’ emb_dim)
   - complexity: Linear(1 â†’ emb_dim)
   - unarys: Linear(n_unary â†’ emb_dim)
  â†“
è¾“å‡º: [batch, n_points + n_hints, emb_dim]
```

**æµ®ç‚¹æ•°ç¼–ç ç¤ºä¾‹**:
```python
# 3.14 â†’ [0(æ­£å·), 20(æŒ‡æ•°+åç§»), 1, 5, 7, 0, 0, ...]
```

**å‚æ•°é…ç½®**:
- `num_encoding_dim`: æ¯ä¸ªæµ®ç‚¹æ•°ç¼–ç é•¿åº¦ (é»˜è®¤ 10)
- `emb_dim`: åµŒå…¥ç»´åº¦ (é»˜è®¤ 512)
- `n_compress_layers`: å‹ç¼©ç½‘ç»œå±‚æ•° (é»˜è®¤ 2)

---

### å®Œæ•´æ¨ç†æµç¨‹çš„ç»´åº¦æµè½¬

```
ç¤ºä¾‹: batch=4, n_points=100, n_vars=2, emb_dim=512, vocab=1000

1. æ•°æ®åµŒå…¥:
   xy_data: (4, 100, 3) â†’ LinearPointEmbedder â†’ (4, 103, 512)

2. ç¼–ç å™¨:
   (103, 4, 512) â†’ TransformerModel(encoder) â†’ (103, 4, 512)

3. è§£ç å™¨ (è‡ªå›å½’ç”Ÿæˆ):
   åˆå§‹: [<BOS>] (1, 4) â†’ Embedding â†’ (1, 4, 512)
   â†’ TransformerModel(decoder) â†’ (1, 4, 1000)
   â†’ argmax â†’ next_token (1, 4)
   é‡å¤ç›´åˆ° <EOS>

4. è¾“å‡º:
   generated_formula: (4, max_len)
```

---

#### embedders.py

**ä½ç½®**: `symbolicregression/model/embedders.py`

**å½“å‰å®ç°çš„åµŒå…¥å™¨**:

1. **LinearPointEmbedder** (embedders.py:41-end): çº¿æ€§æ•°æ®ç‚¹åµŒå…¥
   ```python
   # è¾“å…¥: [batch, n_points, n_vars]
   # è¾“å‡º: [batch, n_points, emb_dim]
   ```

**åŠŸèƒ½**:
- å°† (x, y) æ•°æ®ç‚¹åºåˆ—åµŒå…¥åˆ°é«˜ç»´ç©ºé—´
- æ”¯æŒæµ®ç‚¹æ•°ç¼–ç  (ç¬¦å·ã€æŒ‡æ•°ã€å°¾æ•°åˆ†ç¦»)
- é›†æˆç‰©ç†æç¤ºç¼–ç  (å•ä½ã€å¤æ‚åº¦ã€ä¸€å…ƒè¿ç®—)
- å¯é€‰çš„å‹ç¼©ç½‘ç»œå±‚

**æ³¨æ„**: é¡¹ç›®æ—©æœŸè®¡åˆ’ä¸­åŒ…å« TNet å’Œ AttentionPoint åµŒå…¥å™¨ï¼Œä½†å½“å‰ç‰ˆæœ¬ä»…å®ç°äº† LinearPointEmbedder

### 2. envs/ - ç¯å¢ƒä¸æ•°æ®ç”Ÿæˆ

#### environment.py (946 è¡Œ)

**ä¸»è¦èŒè´£**:
1. **å…¬å¼ç”Ÿæˆ**: éšæœºç”Ÿæˆç¬¦åˆçº¦æŸçš„æ•°å­¦å…¬å¼
2. **æ•°æ®é‡‡æ ·**: ä»å…¬å¼ç”Ÿæˆ (x, y) æ•°æ®ç‚¹
3. **ç¼–ç **: å…¬å¼æ ‘è½¬å‰ç¼€åºåˆ—
4. **è§£ç **: å‰ç¼€åºåˆ—è½¬å…¬å¼æ ‘

**å…¬å¼ç”Ÿæˆæµç¨‹**:
```mermaid
graph TD
    A[å¼€å§‹] --> B[éšæœºé€‰æ‹©è¿ç®—ç¬¦]
    B --> C{éœ€è¦å­æ ‘?}
    C -->|æ˜¯| D[é€’å½’ç”Ÿæˆå­æ ‘]
    C -->|å¦| E[é€‰æ‹©å˜é‡æˆ–å¸¸æ•°]
    D --> F[ç»„åˆå­æ ‘]
    E --> F
    F --> G{æ»¡è¶³çº¦æŸ?}
    G -->|å¦| B
    G -->|æ˜¯| H[ç”Ÿæˆæ•°æ®ç‚¹]
    H --> I[è®¡ç®—ç›®æ ‡å€¼ y]
    I --> J[è¿”å›æ ·æœ¬]
```

**å…³é”®çº¦æŸ**:
- `max_ops`: æœ€å¤§è¿ç®—ç¬¦æ•°
- `units`: ç‰©ç†å•ä½ä¸€è‡´æ€§
- `complexity`: å¤æ‚åº¦è¦æ±‚
- `unarys`: å…è®¸çš„ä¸€å…ƒè¿ç®—

#### operators.py

**è¿ç®—ç¬¦å®šä¹‰**:

```python
# äºŒå…ƒè¿ç®—ç¬¦
BINARY_OPS = {
    'add': ('+', 1),      # (ç¬¦å·, ä¼˜å…ˆçº§)
    'sub': ('-', 1),
    'mul': ('*', 2),
    'div': ('/', 2),
    'pow': ('**', 3),
}

# ä¸€å…ƒè¿ç®—ç¬¦
UNARY_OPS = {
    'sin': 'sin',
    'cos': 'cos',
    'exp': 'exp',
    'log': 'log',
    'sqrt': 'sqrt',
    'abs': 'abs',
}
```

**å•ä½ä¼ æ’­**:
```python
# ä¾‹: y = x_0 * x_1
# x_0 å•ä½: kg1m0s0
# x_1 å•ä½: kg0m1s-1
# y å•ä½: kg1m1s-1 (ç›¸ä¹˜)

def propagate_units(op, units_left, units_right):
    """è®¡ç®—è¿ç®—åçš„å•ä½"""
    if op == 'mul':
        return [u1 + u2 for u1, u2 in zip(units_left, units_right)]
    # ... å…¶ä»–è§„åˆ™
```

### 3. MCTS/ - è’™ç‰¹å¡æ´›æ ‘æœç´¢

#### mcts.py

**MCTS ç²¾ç‚¼ç­–ç•¥**:

```mermaid
graph TD
    A[åˆå§‹å…¬å¼] --> B[é€‰æ‹©èŠ‚ç‚¹]
    B --> C[æ‰©å±•å€™é€‰]
    C --> D[æ¨¡æ‹Ÿè¯„ä¼°]
    D --> E[å›æº¯æ›´æ–°]
    E --> F{è¾¾åˆ°è¿­ä»£æ¬¡æ•°?}
    F -->|å¦| B
    F -->|æ˜¯| G[è¿”å›æœ€ä½³å…¬å¼]
```

**å…³é”®å‚æ•°**:
- `n_iterations`: æœç´¢è¿­ä»£æ¬¡æ•° (é»˜è®¤ 100)
- `exploration_weight`: æ¢ç´¢æƒé‡ (é»˜è®¤ 1.0)
- `beam_size`: æŸå¤§å°

**ä½¿ç”¨åœºæ™¯**:
- å…¬å¼é¢„æµ‹ç½®ä¿¡åº¦ä½æ—¶
- éœ€è¦å±€éƒ¨ä¼˜åŒ–æ—¶
- å¤æ‚å…¬å¼çš„ç²¾ç‚¼

### 4. GA/ - é—ä¼ ç®—æ³•ä¼˜åŒ–

#### ga.py

**é—ä¼ ç¼–ç¨‹æµç¨‹**:

```python
# 1. åˆå§‹åŒ–ç§ç¾¤
population = initialize_population(initial_formulas)

# 2. è¿­ä»£è¿›åŒ–
for generation in range(n_generations):
    # 3. è¯„ä¼°é€‚åº”åº¦
    fitness = evaluate_population(population, x, y)

    # 4. é€‰æ‹©
    parents = tournament_selection(population, fitness)

    # 5. äº¤å‰
    offspring = crossover(parents)

    # 6. å˜å¼‚
    offspring = mutate(offspring, mutation_rate)

    # 7. æ›¿æ¢
    population = elitism_replacement(population, offspring)

# 8. è¿”å›æœ€ä¼˜è§£
best = max(population, key=fitness)
```

**é—ä¼ ç®—å­**:
- **äº¤å‰**: å­æ ‘äº¤æ¢
- **å˜å¼‚**: èŠ‚ç‚¹æ›¿æ¢ã€å­æ ‘æ’å…¥
- **ç®€åŒ–**: SymPy è‡ªåŠ¨ç®€åŒ–

---

## ä¾èµ–å…³ç³»

### å†…éƒ¨ä¾èµ–

```mermaid
graph TD
    Trainer --> Model
    Trainer --> Env
    Model --> Embedders
    Env --> Operators
    Env --> Generators
    PhyReg[PhysicsRegression.py] --> Trainer
    PhyReg --> MCTS
    PhyReg --> GA
```

### å¤–éƒ¨ä¾èµ–

**æ ¸å¿ƒä¾èµ–**:
- `torch >= 2.0.1`: Transformer æ¨¡å‹
- `numpy >= 1.24.3`: æ•°å€¼è®¡ç®—
- `sympy >= 1.13.3`: ç¬¦å·æ•°å­¦
- `scipy >= 1.10.1`: ä¼˜åŒ–ç®—æ³•

**å¯é€‰ä¾èµ–**:
- `sympytorch`: SymPy ä¸ PyTorch é›†æˆ
- `pysr`: PySR åˆå§‹åŒ–æ”¯æŒ

---

## å¼€å‘æŒ‡å—

### æ·»åŠ æ–°è¿ç®—ç¬¦

**æ­¥éª¤**:
1. åœ¨ `envs/operators.py` ä¸­æ³¨å†Œ:
   ```python
   UNARY_OPS['tanh'] = 'tanh'
   ```

2. åœ¨ `envs/environment.py` ä¸­æ·»åŠ å•ä½è§„åˆ™:
   ```python
   def propagate_units_unary(op, units):
       if op == 'tanh':
           # tanh ä¿æŒå•ä½
           return units
   ```

3. æ›´æ–° SymPy é‡å†™è§„åˆ™ (å¦‚éœ€è¦):
   ```python
   rewrite_functions.append(
       (sp.tanh, lambda x: (sp.exp(x) - sp.exp(-x)) / (sp.exp(x) + sp.exp(-x)))
   )
   ```

### è‡ªå®šä¹‰åµŒå…¥å™¨

**å®ç°æ¥å£**:
```python
import torch.nn as nn

class CustomEmbedder(nn.Module):
    def __init__(self, params):
        super().__init__()
        self.input_dim = params.n_vars
        self.output_dim = params.enc_emb_dim
        # å®šä¹‰ç½‘ç»œå±‚

    def forward(self, x):
        """
        å‚æ•°:
            x: [batch, n_points, n_vars]
        è¿”å›:
            embedded: [batch, n_points, emb_dim]
        """
        # å®ç°åµŒå…¥é€»è¾‘
        return embedded
```

**æ³¨å†Œåˆ° `model_wrapper.py`**:
```python
from .embedders import CustomEmbedder

def build_modules(env, params):
    if params.embedder == 'custom':
        embedder = CustomEmbedder(params)
    # ...
```

### è®­ç»ƒæŠ€å·§

**1. è°ƒæ•´æ‰¹æ¬¡å¤§å°**:
```bash
# GPU å†…å­˜ä¸è¶³æ—¶
python train.py --tokens_per_batch 10000  # é»˜è®¤ 20000

# å¢åŠ è®­ç»ƒæ­¥æ•°è¡¥å¿
python train.py --n_steps_per_epoch 1000  # é»˜è®¤ 500
```

**2. ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹**:
```python
from PhysicsRegression import PhyReg

# åŠ è½½é¢„è®­ç»ƒ
model = PhyReg("model.pt")

# ç»§ç»­è®­ç»ƒ
model.fit(x, y, continue_training=True)
```

**3. è°ƒè¯•æ¨¡å¼**:
```python
# è®¾ç½®è¯¦ç»†æ—¥å¿—
params.log_level = "DEBUG"

# ä¿å­˜ä¸­é—´ç»“æœ
params.save_intermediate = True
```

### æµ‹è¯•

**è¿è¡Œæµ‹è¯•**:
```bash
# å•å…ƒæµ‹è¯•
pytest symbolicregression/tests/

# æ¨¡å‹æµ‹è¯•
python -m symbolicregression.model.test_transformer

# ç¯å¢ƒæµ‹è¯•
python -m symbolicregression.envs.test_environment
```

### æ€§èƒ½ä¼˜åŒ–

**1. æ•°æ®åŠ è½½**:
- ä½¿ç”¨ `num_workers > 0` å¹¶è¡ŒåŠ è½½
- é¢„ç”Ÿæˆæ•°æ®ç¼“å­˜

**2. æ¨¡å‹æ¨ç†**:
- ä½¿ç”¨ `torch.jit.script` ç¼–è¯‘
- å‡å°‘æŸæœç´¢å¤§å°

**3. å†…å­˜ç®¡ç†**:
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
- å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)

---

## PaddlePaddle è¿ç§»æ³¨æ„äº‹é¡¹

### å…³é”®æ›¿æ¢

| PyTorch | PaddlePaddle | æ–‡ä»¶ |
|---------|--------------|------|
| `torch.nn.TransformerEncoder` | `paddle.nn.TransformerEncoder` | transformer.py:245 |
| `torch.nn.MultiheadAttention` | `paddle.nn.MultiHeadAttention` | transformer.py:312 |
| `torch.optim.Adam` | `paddle.optimizer.Adam` | trainer.py:156 |
| `torch.utils.data.DataLoader` | `paddle.io.DataLoader` | trainer.py:89 |

### æ•°å€¼å·®å¼‚æ£€æŸ¥

**ä½ç½®**:
- `model/transformer.py:567` - ä½ç½®ç¼–ç 
- `envs/environment.py:234` - æ•°æ®é‡‡æ ·
- `trainer.py:423` - æŸå¤±è®¡ç®—

**éªŒè¯æ–¹æ³•**:
```python
# å¯¹æ¯” PyTorch å’Œ PaddlePaddle è¾“å‡º
torch_output = torch_model(x)
paddle_output = paddle_model(x)
diff = np.abs(torch_output - paddle_output).max()
assert diff < 1e-5, f"æ•°å€¼å·®å¼‚è¿‡å¤§: {diff}"
```

---

**æœ€åæ›´æ–°**: 2026-01-22
**ç»´æŠ¤è€…**: PhysicsRegression Team
**ç›¸å…³æ–‡æ¡£**: [æ ¹ç›®å½• CLAUDE.md](../CLAUDE.md) | [Oracle æ¨¡å—](../Oracle/CLAUDE.md) | [ç‰©ç†æ¡ˆä¾‹](../physical/CLAUDE.md)
