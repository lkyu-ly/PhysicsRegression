# PaddlePaddle è¿ç§»æŒ‡å—

> **é¡¹ç›®**: PhysicsRegression PyTorch â†’ PaddlePaddle æ¡†æ¶è¿ç§»
> **è¿ç§»å·¥å…·**: PaConvert (ç™¾åº¦è‡ªåŠ¨è½¬æ¢å·¥å…·)
> **è¿ç§»æ—¥æœŸ**: 2026å¹´
> **æ–‡æ¡£ç‰ˆæœ¬**: 1.0

---

## ğŸ“‹ ç›®å½•

- [è¿ç§»æ¦‚è§ˆ](#è¿ç§»æ¦‚è§ˆ)
- [æ ¸å¿ƒAPIå˜åŒ–](#æ ¸å¿ƒapiå˜åŒ–)
- [paddle_utils.py å…¼å®¹å±‚](#paddle_utilspy-å…¼å®¹å±‚)
- [å…³é”®ä»£ç å¯¹æ¯”](#å…³é”®ä»£ç å¯¹æ¯”)
- [è®¾å¤‡ç®¡ç†å˜åŒ–](#è®¾å¤‡ç®¡ç†å˜åŒ–)
- [æ¨¡å‹æ–‡ä»¶æ ¼å¼](#æ¨¡å‹æ–‡ä»¶æ ¼å¼)
- [ç‰¹æ®Šå¤„ç†è¯´æ˜](#ç‰¹æ®Šå¤„ç†è¯´æ˜)
- [è¿ç§»æ£€æŸ¥æ¸…å•](#è¿ç§»æ£€æŸ¥æ¸…å•)
- [å·²çŸ¥é—®é¢˜](#å·²çŸ¥é—®é¢˜)

---

## è¿ç§»æ¦‚è§ˆ

### è¿ç§»æ–¹æ³•

æœ¬é¡¹ç›®ä½¿ç”¨ **PaConvert** (ç™¾åº¦å®˜æ–¹å·¥å…·) è¿›è¡Œè‡ªåŠ¨ä»£ç è½¬æ¢:

```bash
# è¿ç§»å‘½ä»¤ (å·²å®Œæˆ)
paconvert --in_dir ./PhysicsRegression --out_dir ./PhysicsRegressionPaddle
```

### è¿ç§»çŠ¶æ€

| ç»„ä»¶ | è¿ç§»çŠ¶æ€ | è‡ªåŠ¨è½¬æ¢ç‡ | å¤‡æ³¨ |
|------|---------|-----------|------|
| **ç¬¦å·å›å½’æ¨¡å—** | âœ… å®Œæˆ | ~95% | Transformer, Embedders, Environment |
| **Oracleæ¨¡å—** | âœ… å®Œæˆ | ~98% | SimpleNetç½‘ç»œ, Oracleè®­ç»ƒ |
| **è®­ç»ƒè„šæœ¬** | âœ… å®Œæˆ | ~90% | train.py, trainer.py |
| **è¯„ä¼°è„šæœ¬** | âœ… å®Œæˆ | ~90% | evaluate.py |
| **å·¥å…·å‡½æ•°** | âœ… å®Œæˆ | ~95% | utils.py, metrics.py |
| **å…¼å®¹å±‚** | âœ… è‡ªåŠ¨ç”Ÿæˆ | 100% | paddle_utils.py |

### æ–‡ä»¶ç»“æ„å¯¹æ¯”

```
PhysicsRegression/              PhysicsRegressionPaddle/
â”œâ”€â”€ *.py (PyTorchä»£ç )         â”œâ”€â”€ *.py (PaddlePaddleä»£ç )
â”œâ”€â”€ symbolicregression/         â”œâ”€â”€ symbolicregression/
â”œâ”€â”€ Oracle/                     â”œâ”€â”€ Oracle/
â”œâ”€â”€ physical/                   â”œâ”€â”€ physical/
â”œâ”€â”€ model.pt (PyTorchæ¨¡å‹)     â”œâ”€â”€ model.pdparams (éœ€è½¬æ¢)
â””â”€â”€ CLAUDE.md                   â”œâ”€â”€ paddle_utils.py (æ–°å¢å…¼å®¹å±‚)
                                â””â”€â”€ CLAUDE.md (éœ€æ›´æ–°)
```

---

## æ ¸å¿ƒAPIå˜åŒ–

### æ¨¡å—å¯¼å…¥å˜åŒ–

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

# âœ… PaddlePaddle
import paddle
import paddle.nn as nn
from paddle.io import DataLoader
from paddle_utils import *  # å¯¼å…¥å…¼å®¹å±‚
```

### ç¥ç»ç½‘ç»œæ¨¡å—

#### åŸºç¡€ç±»ç»§æ‰¿

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
class MyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()

# âœ… PaddlePaddle
class MyModel(paddle.nn.Module):  # æˆ– paddle.nn.Layer
    def __init__(self):
        super().__init__()
```

#### çº¿æ€§å±‚

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
self.fc = torch.nn.Linear(128, 64)

# âœ… PaddlePaddle (å…¼å®¹å‘½åç©ºé—´)
self.fc = paddle.compat.nn.Linear(128, 64)

# âš ï¸ æ³¨æ„: ä½¿ç”¨ paddle.compat.nn.Linear è€Œé paddle.nn.Linear
# è¿™æ˜¯PaConvertå·¥å…·çš„å¤„ç†æ–¹å¼,ç¡®ä¿APIå…¼å®¹æ€§
```

#### æ¿€æ´»å‡½æ•°

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
x = torch.tanh(x)
x = torch.nn.functional.relu(x)

# âœ… PaddlePaddle
x = paddle.tanh(x)
x = paddle.nn.functional.relu(x)
```

#### åµŒå…¥å±‚

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
self.embed = torch.nn.Embedding(vocab_size, emb_dim, padding_idx=0)

# âœ… PaddlePaddle
self.embed = paddle.nn.Embedding(vocab_size, emb_dim, padding_idx=0)
```

#### å®¹å™¨æ¨¡å—

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
self.layers = torch.nn.ModuleList([...])

# âœ… PaddlePaddle
self.layers = paddle.nn.ModuleList([...])
```

### å¼ é‡æ“ä½œ

#### å¼ é‡åˆ›å»º

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
x = torch.tensor([1, 2, 3])
x = torch.zeros(3, 4)
x = torch.FloatTensor([1.0, 2.0])

# âœ… PaddlePaddle
x = paddle.to_tensor([1, 2, 3])  # æ³¨æ„: paddle.tensorä¹Ÿå¯ç”¨
x = paddle.zeros([3, 4])
x = paddle.FloatTensor([1.0, 2.0])
```

#### æ•°æ®ç±»å‹

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
x = x.long()
x = x.float()

# âœ… PaddlePaddle
x = x.astype(paddle.long)  # æˆ– paddle.int64
x = x.astype(paddle.float32)
```

#### å¼ é‡æ–¹æ³•å·®å¼‚

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch: ä½¿ç”¨ dim å‚æ•°
x.max(dim=1)
x.sum(dim=0)

# âœ… PaddlePaddle: ä½¿ç”¨ axis å‚æ•°
x.max(axis=1)  # æˆ–ä½¿ç”¨ paddle_utils ä¸­çš„å…¼å®¹æ–¹æ³•
x.sum(axis=0)
```

### ä¼˜åŒ–å™¨

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

# âœ… PaddlePaddle
optimizer = paddle.optimizer.Adam(parameters=model.parameters(), learning_rate=0.001)
optimizer = paddle.optimizer.SGD(parameters=model.parameters(), learning_rate=0.01, momentum=0.9)
```

### æŸå¤±å‡½æ•°

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
loss_fn = torch.nn.MSELoss()
loss = torch.nn.functional.cross_entropy(pred, target)

# âœ… PaddlePaddle
loss_fn = paddle.nn.MSELoss()
loss = paddle.nn.functional.cross_entropy(pred, target)
```

### æ•°æ®åŠ è½½

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
from torch.utils.data import Dataset, DataLoader

class MyDataset(Dataset):
    pass

loader = DataLoader(dataset, batch_size=32, shuffle=True)

# âœ… PaddlePaddle
from paddle.io import Dataset, DataLoader

class MyDataset(Dataset):
    pass

loader = DataLoader(dataset, batch_size=32, shuffle=True)
```

---

## paddle_utils.py å…¼å®¹å±‚

PaConvert è‡ªåŠ¨ç”Ÿæˆçš„å…¼å®¹å±‚æ–‡ä»¶,ç”¨äºå¤„ç†PyTorchå’ŒPaddlePaddleçš„APIå·®å¼‚ã€‚

### æ–‡ä»¶ä½ç½®

```
PhysicsRegressionPaddle/
â””â”€â”€ paddle_utils.py  # é¡¹ç›®æ ¹ç›®å½•
```

### æ ¸å¿ƒåŠŸèƒ½

#### 1. è®¾å¤‡å­—ç¬¦ä¸²è½¬æ¢

**åŠŸèƒ½**: å°†PyTorchçš„è®¾å¤‡å­—ç¬¦ä¸²è½¬æ¢ä¸ºPaddlePaddleæ ¼å¼

```python
def device2int(device):
    """
    è½¬æ¢è®¾å¤‡å­—ç¬¦ä¸²æ ¼å¼

    ç¤ºä¾‹:
        'cuda:0' â†’ 'gpu:0' â†’ 0
        'cuda:1' â†’ 'gpu:1' â†’ 1
    """
    if isinstance(device, str):
        print("Converting device string to int:", device)
        device = device.replace('cuda', 'gpu')
        device = device.replace('gpu:', '')
    return int(device)
```

**ä½¿ç”¨åœºæ™¯**:
```python
# PyTorchä»£ç : device = 'cuda:0'
# PaddlePaddleè½¬æ¢: device = device2int('cuda:0')  # è¿”å› 0
```

#### 2. Tensor.max() æ–¹æ³•é€‚é…

**åŠŸèƒ½**: å¤„ç† `dim`/`axis` å‚æ•°å·®å¼‚

```python
def _Tensor_max(self, *args, **kwargs):
    """
    é€‚é… Tensor.max() æ–¹æ³•

    å¤„ç†:
    1. PyTorch: tensor.max(dim=1)
    2. PaddlePaddle: tensor.max(axis=1)
    3. è¿”å›å€¼å·®å¼‚: (values, indices)
    """
    if "other" in kwargs:
        kwargs["y"] = kwargs.pop("other")
        ret = paddle.maximum(self, *args, **kwargs)
    elif len(args) == 1 and isinstance(args[0], paddle.Tensor):
        ret = paddle.maximum(self, *args, **kwargs)
    else:
        if "dim" in kwargs:
            kwargs["axis"] = kwargs.pop("dim")  # â† å…³é”®è½¬æ¢

        if "axis" in kwargs or len(args) >= 1:
            ret = paddle.max(self, *args, **kwargs), paddle.argmax(self, *args, **kwargs)
        else:
            ret = paddle.max(self, *args, **kwargs)

    return ret

# å°†æ–¹æ³•ç»‘å®šåˆ° Tensor ç±»
setattr(paddle.Tensor, "_max", _Tensor_max)
```

**ä½¿ç”¨ç¤ºä¾‹**:
```python
import paddle
from paddle_utils import *

x = paddle.randn([3, 4])

# PyTorché£æ ¼ (é€šè¿‡å…¼å®¹å±‚è‡ªåŠ¨å¤„ç†)
max_val, max_idx = x._max(dim=1)

# ç­‰ä»·äºPaddlePaddleåŸç”Ÿå†™æ³•:
max_val = x.max(axis=1)
max_idx = x.argmax(axis=1)
```

### ä½¿ç”¨æ–¹æ³•

åœ¨æ¯ä¸ªéœ€è¦å…¼å®¹å¤„ç†çš„æ¨¡å—é¡¶éƒ¨æ·»åŠ :

```python
import paddle
from paddle_utils import *
```

**æ³¨æ„äº‹é¡¹**:
- âš ï¸ `paddle_utils.py` å¿…é¡»ä½äºPythonå¯¼å…¥è·¯å¾„ä¸­
- âš ï¸ å¯¼å…¥é¡ºåº: å…ˆ `import paddle`,å† `from paddle_utils import *`
- âš ï¸ æŸäº›é¡¹ç›®æ–‡ä»¶é€šè¿‡ `sys.path.append` æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„

---

## å…³é”®ä»£ç å¯¹æ¯”

### ç¤ºä¾‹ 1: Transformer MultiHeadAttention

**æ–‡ä»¶**: `symbolicregression/model/transformer.py:54-74`

```python
# ===== PyTorchç‰ˆæœ¬ =====
import torch
import torch.nn as nn

class MultiHeadAttention(nn.Module):
    def __init__(self, n_heads, dim, src_dim, dropout, normalized_attention):
        super().__init__()
        self.q_lin = nn.Linear(dim, dim)
        self.k_lin = nn.Linear(src_dim, dim)
        self.v_lin = nn.Linear(src_dim, dim)
        self.out_lin = nn.Linear(dim, dim)
        if self.normalized_attention:
            self.attention_scale = nn.Parameter(
                torch.tensor(1.0 / math.sqrt(dim // n_heads))
            )

# ===== PaddlePaddleç‰ˆæœ¬ =====
import paddle
from paddle_utils import *

class MultiHeadAttention(paddle.nn.Module):
    def __init__(self, n_heads, dim, src_dim, dropout, normalized_attention):
        super().__init__()
        self.q_lin = paddle.compat.nn.Linear(dim, dim)           # â† ä½¿ç”¨ compat
        self.k_lin = paddle.compat.nn.Linear(src_dim, dim)       # â† ä½¿ç”¨ compat
        self.v_lin = paddle.compat.nn.Linear(src_dim, dim)       # â† ä½¿ç”¨ compat
        self.out_lin = paddle.compat.nn.Linear(dim, dim)         # â† ä½¿ç”¨ compat
        if self.normalized_attention:
            self.attention_scale = paddle.nn.Parameter(
                paddle.tensor(1.0 / math.sqrt(dim // n_heads))  # â† paddle.tensor
            )
```

**å˜åŒ–è¦ç‚¹**:
1. å¯¼å…¥: `torch` â†’ `paddle`
2. ç±»ç»§æ‰¿: `nn.Module` â†’ `paddle.nn.Module`
3. çº¿æ€§å±‚: `nn.Linear` â†’ `paddle.compat.nn.Linear`
4. å‚æ•°: `torch.tensor` â†’ `paddle.tensor`

---

### ç¤ºä¾‹ 2: Oracle SimpleNet

**æ–‡ä»¶**: `Oracle/oracle.py:20-35`

```python
# ===== PyTorchç‰ˆæœ¬ =====
import torch
import torch.nn as nn

class SimpleNet(nn.Module):
    def __init__(self, _in):
        super().__init__()
        self.linear1 = nn.Linear(_in, 128)
        self.linear2 = nn.Linear(128, 128)
        self.linear3 = nn.Linear(128, 64)
        self.linear4 = nn.Linear(64, 64)
        self.linear5 = nn.Linear(64, 1)

    def forward(self, x):
        x = torch.tanh(self.linear1(x))
        x = torch.tanh(self.linear2(x))
        x = torch.tanh(self.linear3(x))
        x = torch.tanh(self.linear4(x))
        x = self.linear5(x)
        return x

# ===== PaddlePaddleç‰ˆæœ¬ =====
import paddle

class SimpleNet(paddle.nn.Module):
    def __init__(self, _in):
        super().__init__()
        self.linear1 = paddle.compat.nn.Linear(_in, 128)
        self.linear2 = paddle.compat.nn.Linear(128, 128)
        self.linear3 = paddle.compat.nn.Linear(128, 64)
        self.linear4 = paddle.compat.nn.Linear(64, 64)
        self.linear5 = paddle.compat.nn.Linear(64, 1)

    def forward(self, x):
        x = paddle.tanh(self.linear1(x))      # â† paddle.tanh
        x = paddle.tanh(self.linear2(x))
        x = paddle.tanh(self.linear3(x))
        x = paddle.tanh(self.linear4(x))
        x = self.linear5(x)
        return x
```

---

### ç¤ºä¾‹ 3: LinearPointEmbedder

**æ–‡ä»¶**: `symbolicregression/model/embedders.py:45-73`

```python
# ===== PyTorchç‰ˆæœ¬ =====
import torch
import torch.nn as nn

class LinearPointEmbedder(Embedder):
    def __init__(self, params, env):
        super().__init__()
        self.embeddings = Embedding(
            len(self.env.float_id2word),
            self.input_dim,
            padding_idx=env.float_word2id["<PAD>"],
        )
        self.activation_fn = torch.nn.functional.relu
        self.hidden_layers = nn.ModuleList()
        self.hidden_layers.append(nn.Linear(size, hidden_size))
        for i in range(self.params.n_emb_layers - 1):
            self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))
        self.fc = nn.Linear(hidden_size, self.output_dim)

# ===== PaddlePaddleç‰ˆæœ¬ =====
import paddle
from paddle_utils import *

class LinearPointEmbedder(Embedder):
    def __init__(self, params, env):
        super().__init__()
        self.embeddings = Embedding(
            len(self.env.float_id2word),
            self.input_dim,
            padding_idx=env.float_word2id["<PAD>"],
        )
        self.activation_fn = paddle.nn.functional.relu  # â† paddle
        self.hidden_layers = paddle.nn.ModuleList()     # â† paddle
        self.hidden_layers.append(paddle.compat.nn.Linear(size, hidden_size))
        for i in range(self.params.n_emb_layers - 1):
            self.hidden_layers.append(paddle.compat.nn.Linear(hidden_size, hidden_size))
        self.fc = paddle.compat.nn.Linear(hidden_size, self.output_dim)
```

---

### ç¤ºä¾‹ 4: è®­ç»ƒå¾ªç¯

```python
# ===== PyTorchç‰ˆæœ¬ =====
import torch

def train_step(model, optimizer, x, y):
    model.train()

    # å‰å‘ä¼ æ’­
    pred = model(x)
    loss = torch.nn.functional.mse_loss(pred, y)

    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss.item()

# ===== PaddlePaddleç‰ˆæœ¬ =====
import paddle

def train_step(model, optimizer, x, y):
    model.train()

    # å‰å‘ä¼ æ’­
    pred = model(x)
    loss = paddle.nn.functional.mse_loss(pred, y)

    # åå‘ä¼ æ’­
    optimizer.clear_grad()  # â† clear_grad è€Œé zero_grad
    loss.backward()
    optimizer.step()

    return loss.item()
```

**å…³é”®å·®å¼‚**:
- `optimizer.zero_grad()` â†’ `optimizer.clear_grad()`

---

## è®¾å¤‡ç®¡ç†å˜åŒ–

### è®¾å¤‡å­—ç¬¦ä¸²æ ¼å¼

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
device = 'cuda:0'
device = 'cuda:1'
device = 'cpu'

# âœ… PaddlePaddle
device = 'gpu:0'   # æˆ–ä½¿ç”¨ device2int() è½¬æ¢ä¸ºæ•´æ•° 0
device = 'gpu:1'   # æˆ–æ•´æ•° 1
device = 'cpu'
```

### æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
model = model.to('cuda:0')
x = x.to('cuda:0')

# âœ… PaddlePaddle (æ–¹å¼1: å­—ç¬¦ä¸²)
model = model.to('gpu:0')
x = x.to('gpu:0')

# âœ… PaddlePaddle (æ–¹å¼2: è®¾å¤‡å¯¹è±¡)
device = paddle.CUDAPlace(0)
model = model.to(device)
x = x.to(device)
```

### æ£€æŸ¥GPUå¯ç”¨æ€§

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
if torch.cuda.is_available():
    device = 'cuda:0'
else:
    device = 'cpu'

# âœ… PaddlePaddle
if paddle.is_compiled_with_cuda():
    device = 'gpu:0'
else:
    device = 'cpu'
```

---

## æ¨¡å‹æ–‡ä»¶æ ¼å¼

### æ¨¡å‹ä¿å­˜

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch: ä¿å­˜ä¸º .pt æˆ– .pth
torch.save(model.state_dict(), 'model.pt')

# âœ… PaddlePaddle: ä¿å­˜ä¸º .pdparams
paddle.save(model.state_dict(), 'model.pdparams')
```

### æ¨¡å‹åŠ è½½

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
state_dict = torch.load('model.pt', map_location='cpu')
model.load_state_dict(state_dict)

# âœ… PaddlePaddle
state_dict = paddle.load('model.pdparams')
model.set_state_dict(state_dict)
```

### æ¨¡å‹è½¬æ¢

**ä»PyTorchè¿ç§»åˆ°PaddlePaddleæ—¶,æ¨¡å‹æ–‡ä»¶éœ€è¦é‡æ–°è®­ç»ƒæˆ–ä½¿ç”¨è½¬æ¢å·¥å…·**:

```python
# æ–¹æ³•1: é‡æ–°è®­ç»ƒ (æ¨è)
# ä½¿ç”¨ train.py é‡æ–°è®­ç»ƒæ¨¡å‹

# æ–¹æ³•2: æ‰‹åŠ¨è½¬æ¢æƒé‡ (å¤æ‚,ä»…å½“å¿…è¦æ—¶)
# éœ€è¦ç¼–å†™è‡ªå®šä¹‰è½¬æ¢è„šæœ¬åŒ¹é…ç½‘ç»œç»“æ„
```

**æ³¨æ„**: ç”±äºæ¶æ„å·®å¼‚,ç›´æ¥è½¬æ¢`.pt`åˆ°`.pdparams`å¯èƒ½ä¸å¯è¡Œ,å»ºè®®é‡æ–°è®­ç»ƒã€‚

---

## ç‰¹æ®Šå¤„ç†è¯´æ˜

### paddle.compat.nn.Linear

**ä¸ºä»€ä¹ˆä½¿ç”¨ `paddle.compat.nn.Linear`?**

PaConvertå·¥å…·ä½¿ç”¨ `paddle.compat.nn.Linear` ç¡®ä¿APIå…¼å®¹æ€§:

```python
# PyTorchåŸå§‹ä»£ç 
fc = torch.nn.Linear(128, 64)

# PaConvertè½¬æ¢å
fc = paddle.compat.nn.Linear(128, 64)

# è€Œéç›´æ¥ä½¿ç”¨
fc = paddle.nn.Linear(128, 64)  # â† å¯èƒ½å­˜åœ¨ç»†å¾®å·®å¼‚
```

**å…¼å®¹å‘½åç©ºé—´ä½ç½®**:
- æ–‡ä»¶: `symbolicregression/model/transformer.py`
- æ–‡ä»¶: `symbolicregression/model/embedders.py`
- æ–‡ä»¶: `Oracle/oracle.py`

**æ˜¯å¦å¯ä»¥æ”¹ä¸º `paddle.nn.Linear`?**

ç†è®ºä¸Šå¯ä»¥,ä½†éœ€è¦éªŒè¯ä»¥ä¸‹å†…å®¹:
1. æƒé‡åˆå§‹åŒ–æ–¹æ³•æ˜¯å¦ä¸€è‡´
2. biaså¤„ç†æ˜¯å¦ç›¸åŒ
3. å‰å‘ä¼ æ’­æ•°å€¼ç²¾åº¦

### sys.path.append

å¤šä¸ªæ–‡ä»¶åŒ…å«ä»¥ä¸‹ä»£ç ä»¥ç¡®ä¿å¯¼å…¥è·¯å¾„æ­£ç¡®:

```python
import sys
sys.path.append("/home/lkyu/baidu/PhysicsRegressionPaddle")
```

**ä½ç½®**:
- `symbolicregression/model/transformer.py:1-2`
- `symbolicregression/model/embedders.py:1-2`

**ä½œç”¨**: ç¡®ä¿ `paddle_utils.py` å¯ä»¥è¢«æ­£ç¡®å¯¼å…¥

**æ³¨æ„**: å¦‚æœé¡¹ç›®è·¯å¾„æ”¹å˜,éœ€è¦æ›´æ–°è¿™äº›è·¯å¾„

---

## è¿ç§»æ£€æŸ¥æ¸…å•

### ä»£ç å±‚é¢

- [x] æ‰€æœ‰ `import torch` å·²æ›¿æ¢ä¸º `import paddle`
- [x] æ‰€æœ‰ `torch.nn.Module` å·²æ›¿æ¢ä¸º `paddle.nn.Module`
- [x] æ‰€æœ‰ `torch.nn.Linear` å·²æ›¿æ¢ä¸º `paddle.compat.nn.Linear`
- [x] æ‰€æœ‰ `torch.optim.Adam` å·²æ›¿æ¢ä¸º `paddle.optimizer.Adam`
- [x] æ‰€æœ‰æ¿€æ´»å‡½æ•°å·²æ›´æ–° (`torch.tanh` â†’ `paddle.tanh`)
- [x] ä¼˜åŒ–å™¨è°ƒç”¨å·²æ›´æ–° (`zero_grad()` â†’ `clear_grad()`)
- [x] è®¾å¤‡å­—ç¬¦ä¸²å·²æ›´æ–° (`cuda:0` â†’ `gpu:0`)
- [x] å¼ é‡æ–¹æ³•å·²æ›´æ–° (`dim` â†’ `axis`)
- [x] `paddle_utils.py` å·²æ­£ç¡®å¯¼å…¥

### åŠŸèƒ½éªŒè¯

- [ ] **æµ‹è¯•è®­ç»ƒæµç¨‹**: è¿è¡Œ `train.py` ç¡®è®¤æ— é”™è¯¯
- [ ] **æµ‹è¯•è¯„ä¼°æµç¨‹**: è¿è¡Œ `evaluate.py` éªŒè¯æ¨¡å‹æ¨ç†
- [ ] **æµ‹è¯•Oracleæ¨¡å—**: éªŒè¯åˆ†æ²»ç­–ç•¥æ­£å¸¸å·¥ä½œ
- [ ] **æµ‹è¯•MCTS/GP**: ç¡®è®¤ä¼˜åŒ–ç®—æ³•å¯ç”¨
- [ ] **å¯¹æ¯”æ•°å€¼ç²¾åº¦**: PyTorch vs PaddlePaddle è¾“å‡ºå·®å¼‚ < 1e-5
- [ ] **GPUå†…å­˜æµ‹è¯•**: ç¡®è®¤æ˜¾å­˜ä½¿ç”¨åˆç†
- [ ] **å¤šå¡è®­ç»ƒ**: æµ‹è¯•åˆ†å¸ƒå¼è®­ç»ƒåŠŸèƒ½

### æ–‡æ¡£æ›´æ–°

- [ ] æ›´æ–°æ ¹ç›®å½• `CLAUDE.md`
- [ ] æ›´æ–° `symbolicregression/CLAUDE.md`
- [ ] æ›´æ–° `Oracle/CLAUDE.md`
- [ ] æ›´æ–° `physical/CLAUDE.md`
- [x] åˆ›å»º `PADDLE_MIGRATION.md` (æœ¬æ–‡æ¡£)

### ç¯å¢ƒé…ç½®

- [ ] åˆ›å»ºPaddlePaddleç‰ˆæœ¬çš„ `environment.yml`
- [ ] æ›´æ–° `README.md` å®‰è£…è¯´æ˜
- [ ] å‡†å¤‡PaddlePaddleç‰ˆæœ¬çš„é¢„è®­ç»ƒæ¨¡å‹

---

## å·²çŸ¥é—®é¢˜

### é—®é¢˜ 1: æ¨¡å‹æ ¼å¼ä¸å…¼å®¹

**æè¿°**: PyTorchçš„ `.pt` æ¨¡å‹æ–‡ä»¶æ— æ³•ç›´æ¥ç”¨äºPaddlePaddle

**è§£å†³æ–¹æ¡ˆ**:
1. ä½¿ç”¨ç›¸åŒæ•°æ®é‡æ–°è®­ç»ƒæ¨¡å‹
2. æˆ–ç¼–å†™è‡ªå®šä¹‰è½¬æ¢è„šæœ¬ (éœ€è¦æ·±å…¥ç†è§£ç½‘ç»œç»“æ„)

### é—®é¢˜ 2: paddle.compat å‘½åç©ºé—´

**æè¿°**: ä»£ç ä¸­ä½¿ç”¨ `paddle.compat.nn.Linear` å¯èƒ½è®©äººå›°æƒ‘

**è¯´æ˜**:
- è¿™æ˜¯PaConvertå·¥å…·çš„æ ‡å‡†åšæ³•
- ç¡®ä¿APIå…¼å®¹æ€§
- ä¸å½±å“åŠŸèƒ½

### é—®é¢˜ 3: æ•°å€¼ç²¾åº¦å·®å¼‚

**æè¿°**: PaddlePaddleå’ŒPyTorchåœ¨æŸäº›æ“ä½œä¸Šå¯èƒ½æœ‰ç»†å¾®æ•°å€¼å·®å¼‚

**éªŒè¯æ–¹æ³•**:
```python
import paddle
import torch
import numpy as np

# ç›¸åŒè¾“å…¥
x_np = np.random.randn(4, 128).astype('float32')

# PyTorch
x_torch = torch.from_numpy(x_np)
out_torch = torch_model(x_torch).detach().numpy()

# PaddlePaddle
x_paddle = paddle.to_tensor(x_np)
out_paddle = paddle_model(x_paddle).numpy()

# å¯¹æ¯”
diff = np.abs(out_torch - out_paddle).max()
print(f"æœ€å¤§å·®å¼‚: {diff}")  # åº”è¯¥ < 1e-5
```

### é—®é¢˜ 4: ç¡¬ç¼–ç è·¯å¾„

**æè¿°**: éƒ¨åˆ†æ–‡ä»¶åŒ…å«ç¡¬ç¼–ç çš„ç»å¯¹è·¯å¾„

**ä½ç½®**:
```python
sys.path.append("/home/lkyu/baidu/PhysicsRegressionPaddle")
```

**è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨ç›¸å¯¹è·¯å¾„æˆ–ç¯å¢ƒå˜é‡

### é—®é¢˜ 5: ä¼˜åŒ–å™¨åŸºç±»åˆå§‹åŒ–ç­¾åä¸å…¼å®¹ âš ï¸

**æè¿°**: PaConvert **æ— æ³•è‡ªåŠ¨å¤„ç†** PyTorch å’Œ PaddlePaddle ä¼˜åŒ–å™¨åŸºç±»çš„æ„é€ å‡½æ•°ç­¾åå·®å¼‚

**å½±å“æ–‡ä»¶**: `symbolicregression/optim.py`

**é—®é¢˜æ ¹æº**:

| æ¡†æ¶ | ä¼˜åŒ–å™¨åŸºç±»ç­¾å |
|------|--------------|
| **PyTorch** | `__init__(self, params, defaults)` |
| **PaddlePaddle** | `__init__(self, learning_rate, parameters, weight_decay, ...)` |

**é”™è¯¯ä»£ç ç¤ºä¾‹**:
```python
# âŒ é”™è¯¯: PaConvertè‡ªåŠ¨è½¬æ¢åçš„ä»£ç 
class Adam(paddle.optimizer.Optimizer):
    def __init__(self, params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        super().__init__(params, defaults)  # â† é”™è¯¯: params è¢«ä¼ ç»™äº† learning_rate
```

**é”™è¯¯ä¿¡æ¯**:
```
TypeError: `parameters` argument should not get dict type, if parameter groups is needed,
please set `parameters` as list of dict
```

**æ‰‹åŠ¨ä¿®å¤** (å·²å®Œæˆ):
```python
# âœ… æ­£ç¡®: ä½¿ç”¨å‘½åå‚æ•°è°ƒç”¨çˆ¶ç±»
class Adam(paddle.optimizer.Optimizer):
    def __init__(self, params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0):
        # å‚æ•°éªŒè¯...

        super().__init__(
            learning_rate=lr,      # æ˜ç¡®æŒ‡å®šå­¦ä¹ ç‡
            parameters=params,     # æ˜ç¡®æŒ‡å®šå‚æ•°åˆ—è¡¨
            weight_decay=weight_decay if weight_decay != 0 else None
        )

        # çŠ¶æ€åˆå§‹åŒ–...
```

**ä¿®å¤ä½ç½®**:
- `Adam` (ç¬¬25è¡Œ)
- `AdamWithWarmup` (ç¬¬94-101è¡Œ)
- `AdamInverseSqrtWithWarmup` (ç¬¬149-156è¡Œ)
- `AdamCosineWithWarmup` (ç¬¬211-218è¡Œ)

**ä¸ºä»€ä¹ˆ PaConvert æ— æ³•è‡ªåŠ¨å¤„ç†**:
1. å‚æ•°ä½ç½®å®Œå…¨ä¸åŒ (ç¬¬1ä¸ªå‚æ•°: `params` vs `learning_rate`)
2. å‚æ•°åç§°ä¸åŒ (`params` vs `parameters`)
3. PaddlePaddle ä¸ä½¿ç”¨ `defaults` å­—å…¸æ¨¡å¼
4. éœ€è¦æ ¹æ®è¯­ä¹‰é‡æ–°æ˜ å°„ï¼Œè¶…å‡ºå·¥å…·èƒ½åŠ›

**æœ€ä½³å®è·µ**:
- è¿ç§»ååŠ¡å¿…æµ‹è¯•ä¼˜åŒ–å™¨åˆå§‹åŒ–
- ä¿æŒ PyTorch ç‰ˆæœ¬ä¸å˜ï¼ˆæ ‡å‡†å®ç°ï¼‰
- åœ¨ PaddlePaddle ç‰ˆæœ¬ä¸­æ‰‹åŠ¨ä¿®å¤

---

### é—®é¢˜ 6: tensor.cuda(device=) å‚æ•°ä¸å…¼å®¹ âš ï¸

**æè¿°**: PaddlePaddle çš„ `tensor.cuda()` ä¸æ¥å— `device` å‚æ•°ï¼Œè¿™æ˜¯ä¸PyTorchçš„å…³é”®å·®å¼‚

**å½±å“æ–‡ä»¶**:
- `symbolicregression/utils.py` (to_cuda å‡½æ•°ï¼Œç¬¬140-152è¡Œ)

**é”™è¯¯ä¿¡æ¯**:
```
TypeError: monkey_patch_tensor.<locals>.cuda() got an unexpected keyword argument 'device'
```

**æ ¹æœ¬åŸå› **:

| APIç±»å‹ | PyTorch | PaddlePaddle |
|---------|---------|--------------|
| **Tensor.cuda()** | `tensor.cuda(device=0)` âœ… æ¥å—deviceå‚æ•° | `tensor.cuda()` âŒ ä¸æ¥å—ä»»ä½•å‚æ•° |
| **Module.cuda()** | `module.cuda(device=0)` âœ… æ¥å—deviceå‚æ•° | `module.cuda(device=device_id)` âœ… æ¥å—deviceå‚æ•° |

**å…³é”®å‘ç°**:
- Moduleå’ŒTensorçš„cuda()æ–¹æ³•è¡Œä¸ºä¸åŒ
- PaddlePaddleçš„Tensor.cuda()å®Œå…¨ä¸æ¥å—å‚æ•°
- å®˜æ–¹æ–‡æ¡£è¯´æ˜ä¸å‡†ç¡®ï¼ˆæ–‡æ¡£è¯´æœ‰device_idå‚æ•°ï¼Œå®é™…ä¸å­˜åœ¨ï¼‰

**æ‰‹åŠ¨ä¿®å¤** (å·²å®Œæˆ):

```python
# âŒ ä¿®å¤å‰ (PyTorché£æ ¼)
def to_cuda(*args, use_cpu=False, device=None):
    if not CUDA or use_cpu:
        return args
    if device is None:
        device = 0
    return [(None if x is None else x.cuda(device=device)) for x in args]
    #                                       ^^^^^^^^^^^^^ é”™è¯¯ï¼

# âœ… ä¿®å¤å (æ–¹æ¡ˆB: å…¨å±€è®¾å¤‡ + æ— å‚æ•°.cuda())
def to_cuda(*args, use_cpu=False, device=None):
    """
    Move tensors to CUDA (PaddlePaddle version).

    Note: PaddlePaddle's Tensor.cuda() does not accept any parameters.
    We set global device first, then call parameter-less .cuda()
    """
    if not CUDA or use_cpu:
        return args

    # è®¾ç½®å…¨å±€é»˜è®¤è®¾å¤‡ (å¦‚æœæŒ‡å®šäº†device)
    if device is not None:
        import paddle
        from paddle_utils import device2int

        if isinstance(device, str):
            device = device2int(device)

        # è®¾ç½®å…¨å±€é»˜è®¤GPUè®¾å¤‡
        paddle.device.set_device(f'gpu:{device}')

    # è°ƒç”¨æ— å‚æ•°çš„ .cuda() æ–¹æ³•
    return [
        (None if x is None else x.cuda())
        for x in args
    ]
```

**ä¿®å¤ç­–ç•¥é€‰æ‹©**:
- æ–¹æ¡ˆA: ä½¿ç”¨`paddle.to_device() + CUDAPlace()`
- **æ–¹æ¡ˆB**: ä½¿ç”¨`paddle.device.set_device() + æ— å‚æ•°.cuda()` â† å·²é‡‡ç”¨
- æ–¹æ¡ˆC: æ£€æŸ¥å¼ é‡è®¾å¤‡ + æ¡ä»¶ç§»åŠ¨

é€‰æ‹©æ–¹æ¡ˆBçš„åŸå› ï¼š
1. ä¸æºä»£ç æœ€ç›¸ä¼¼
2. å®ç°ç®€å•ï¼Œæ˜“äºç»´æŠ¤
3. ä¸Module.cuda()çš„ä½¿ç”¨æ–¹å¼ä¸€è‡´
4. é€‚ç”¨äºå•GPUåœºæ™¯ï¼ˆé¡¹ç›®ä¸»è¦åœºæ™¯ï¼‰

**ä¸ºä»€ä¹ˆ PaConvert æ— æ³•è‡ªåŠ¨å¤„ç†**:
1. éœ€è¦åŒºåˆ†Module.cuda()å’ŒTensor.cuda()çš„ä¸åŒè¡Œä¸º
2. éœ€è¦æ’å…¥å…¨å±€è®¾å¤‡è®¾ç½®é€»è¾‘
3. éœ€è¦ç†è§£deviceå‚æ•°çš„è¯­ä¹‰è½¬æ¢
4. è¶…å‡ºç®€å•APIæ˜ å°„èŒƒå›´

**è°ƒç”¨ä½ç½®** (æ— éœ€ä¿®æ”¹):
- `symbolicregression/model/embedders.py:101-106`
- `symbolicregression/trainer.py:666, 669`

è¿™äº›è°ƒç”¨ä½ç½®æ— éœ€ä¿®æ”¹ï¼Œå› ä¸ºto_cudaçš„æ¥å£ä¿æŒä¸å˜ã€‚

**æœ€ä½³å®è·µ**:
- å¯¹äºModule: å¯ä»¥ä½¿ç”¨`.cuda(device=device_id)`
- å¯¹äºTensor: å¿…é¡»å…ˆ`set_device()`å†è°ƒç”¨æ— å‚æ•°`.cuda()`
- å»ºè®®ç»Ÿä¸€ä½¿ç”¨`paddle.to_device(tensor, place)`æ˜¾å¼æŒ‡å®šè®¾å¤‡

---

### é—®é¢˜ 7: tensor.new() æ–¹æ³•ä¸å­˜åœ¨ âš ï¸

**æè¿°**: PaddlePaddle çš„ Tensor æ²¡æœ‰ `.new()` æ–¹æ³•ï¼Œè¿™æ˜¯PyTorchç‹¬æœ‰çš„ä¾¿æ·åˆ›å»ºå¼ é‡çš„æ–¹æ³•

**å½±å“æ–‡ä»¶**:
- `symbolicregression/model/transformer.py` (15å¤„è°ƒç”¨)

**é”™è¯¯ä¿¡æ¯**:
```
AttributeError: 'Tensor' object has no attribute 'new'. Did you mean: 'ne'?
```

**æ ¹æœ¬åŸå› **:

| åŠŸèƒ½ | PyTorch | PaddlePaddle |
|------|---------|--------------|
| **åˆ›å»ºåŒè®¾å¤‡å¼ é‡** | `tensor.new(size)` | ä¸å­˜åœ¨æ­¤æ–¹æ³• |
| **åˆ›å»ºåŒç±»å‹å¼ é‡** | `tensor.new([1,2,3])` | ä¸å­˜åœ¨æ­¤æ–¹æ³• |
| **ä¾¿æ·æ–¹æ³•** | `tensor.new(5).long()` | éœ€è¦æ˜¾å¼ä½¿ç”¨paddle API |

**æ‰‹åŠ¨ä¿®å¤** (å·²å®Œæˆ):

ä¿®å¤äº†transformer.pyä¸­æ‰€æœ‰15å¤„`.new()`è°ƒç”¨ï¼š

```python
# âŒ ä¿®å¤å‰ (PyTorché£æ ¼)
positions = x.new(slen).long()
positions = paddle.arange(slen, out=positions).unsqueeze(0)

# âœ… ä¿®å¤å (PaddlePaddleé£æ ¼)
positions = paddle.arange(slen, dtype='int64').unsqueeze(0)
```

**ä¿®å¤æ¨¡å¼æ€»ç»“**:

| PyTorchæ¨¡å¼ | PaddlePaddleæ›¿ä»£ | è¯´æ˜ |
|-------------|-----------------|------|
| `x.new(size).fill_(val)` | `paddle.full([size], val, dtype=x.dtype)` | åˆ›å»ºå¡«å……å¼ é‡ |
| `x.new(size).long()` | `paddle.arange(size, dtype='int64')` | åˆ›å»ºæ•´æ•°åºåˆ— |
| `x.new([list])` | `paddle.to_tensor([list], dtype=x.dtype)` | ä»åˆ—è¡¨åˆ›å»º |
| `x.new(size).float().fill_(0)` | `paddle.zeros([size], dtype='float32')` | åˆ›å»ºé›¶å¼ é‡ |

**è¯¦ç»†ä¿®å¤ä½ç½®** (å…±15å¤„):

1. **ç¬¬399è¡Œ** - `fwd()`æ–¹æ³•ä¸­çš„ä½ç½®å¼ é‡:
```python
# ä¿®å¤å‰:
positions = x.new(slen).long()
positions = paddle.arange(slen, out=positions).unsqueeze(0)

# ä¿®å¤å:
positions = paddle.arange(slen, dtype='int64').unsqueeze(0)
```

2. **ç¬¬516-520è¡Œ** - `generate()`æ–¹æ³•ä¸­çš„ç”Ÿæˆå¼ é‡:
```python
# ä¿®å¤å‰:
generated = src_len.new(max_len, bs)
generated.fill_(self.pad_index)
positions = src_len.new(max_len).long()

# ä¿®å¤å:
generated = paddle.full([max_len, bs], self.pad_index, dtype=src_len.dtype)
generated[0].fill_(self.eos_index)
positions = paddle.arange(max_len, dtype='int64').unsqueeze(1).expand([max_len, bs])
```

3. **ç¬¬578-584è¡Œ** - `generate_double_seq()`æ–¹æ³•:
```python
# ä¿®å¤å‰:
generated1 = src_len.new(max_len, bs)
generated2 = src_len.new(max_len, bs, 5)

# ä¿®å¤å:
generated1 = paddle.full([max_len, bs], self.pad_index, dtype=src_len.dtype)
generated2 = paddle.full([max_len, bs, 5], self.pad_index, dtype=src_len.dtype)
```

4. **ç¬¬758-769è¡Œ** - `generate_beam()`æ–¹æ³•çš„æŸæœç´¢åˆå§‹åŒ–:
```python
# ä¿®å¤å‰:
generated = src_len.new(max_len, bs * beam_size)
beam_scores = src_enc.new(bs, beam_size).float().fill_(0)

# ä¿®å¤å:
generated = paddle.full([max_len, bs * beam_size], self.pad_index, dtype=src_len.dtype)
beam_scores = paddle.full([bs, beam_size], 0.0, dtype='float32')
beam_scores[:, 1:] = -1000000000.0
```

5. **ç¬¬778è¡Œ** - æŸæœç´¢å¾ªç¯ä¸­çš„é•¿åº¦å¼ é‡:
```python
# ä¿®å¤å‰:
lengths=src_len.new(bs * beam_size).fill_(cur_len)

# ä¿®å¤å:
lengths=paddle.full([bs * beam_size], cur_len, dtype=src_len.dtype)
```

6. **ç¬¬830-833è¡Œ** - ä»åˆ—è¡¨åˆ›å»ºæŸæœç´¢è·Ÿè¸ªå¼ é‡:
```python
# ä¿®å¤å‰:
beam_scores = beam_scores.new([x[0] for x in next_batch_beam])
beam_words = generated.new([x[1] for x in next_batch_beam])
beam_idx = src_len.new([x[2] for x in next_batch_beam])

# ä¿®å¤å:
beam_scores = paddle.to_tensor([x[0] for x in next_batch_beam], dtype='float32')
beam_words = paddle.to_tensor([x[1] for x in next_batch_beam], dtype=generated.dtype)
beam_idx = paddle.to_tensor([x[2] for x in next_batch_beam], dtype=src_len.dtype)
```

7. **ç¬¬845-853è¡Œ** - æœ€ç»ˆè§£ç ç»“æœ:
```python
# ä¿®å¤å‰:
tgt_len = src_len.new(bs)
decoded = src_len.new(tgt_len._max().item(), bs).fill_(self.pad_index)

# ä¿®å¤å:
tgt_len = paddle.zeros([bs], dtype=src_len.dtype)
# ... å¡«å……tgt_len ...
decoded = paddle.full([int(tgt_len._max().item()), bs], self.pad_index, dtype=src_len.dtype)
```

**ä¸ºä»€ä¹ˆ PaConvert æ— æ³•è‡ªåŠ¨å¤„ç†**:
1. `.new()`æ˜¯PyTorchçš„ä¾¿æ·æ–¹æ³•ï¼Œæ²¡æœ‰ç›´æ¥å¯¹åº”çš„PaddlePaddle API
2. éœ€è¦æ ¹æ®ä½¿ç”¨åœºæ™¯é€‰æ‹©ä¸åŒçš„æ›¿ä»£æ–¹æ³•ï¼ˆfull/zeros/arange/to_tensorï¼‰
3. éœ€è¦ä¿æŒdtypeä¸€è‡´æ€§ï¼Œè¦ä»åŸå¼ é‡æ¨æ–­ç±»å‹
4. æ¶‰åŠå¤æ‚çš„æ–¹æ³•é“¾ï¼ˆå¦‚`.new().long().fill_()`ï¼‰éœ€è¦è¯­ä¹‰ç†è§£
5. è¶…å‡ºç®€å•APIæ˜ å°„çš„èƒ½åŠ›èŒƒå›´

**æœ€ä½³å®è·µ**:
- ä½¿ç”¨`paddle.full()`åˆ›å»ºå¡«å……å¼ é‡
- ä½¿ç”¨`paddle.zeros()`/`paddle.ones()`åˆ›å»ºé›¶/ä¸€å¼ é‡
- ä½¿ç”¨`paddle.arange()`åˆ›å»ºåºåˆ—
- ä½¿ç”¨`paddle.to_tensor()`ä»Pythonåˆ—è¡¨åˆ›å»º
- å§‹ç»ˆæ˜¾å¼æŒ‡å®š`dtype`ç¡®ä¿ç±»å‹ä¸€è‡´

**éªŒè¯ç»“æœ**: âœ… è®­ç»ƒæˆåŠŸè¿è¡Œ120+æ­¥ï¼Œæ‰€æœ‰.new()è°ƒç”¨å·²æ­£ç¡®æ›¿æ¢

---

### é—®é¢˜ 8: model.parameters() è¿”å›ç±»å‹å·®å¼‚ âš ï¸

**æè¿°**: PaddlePaddle çš„ `model.parameters()` è¿”å› list è€Œé generatorï¼Œä»¥åŠç›¸å…³çš„ç±»å‹æå‡é—®é¢˜

**å½±å“æ–‡ä»¶**:
- `symbolicregression/model/model_wrapper.py` (ç¬¬40è¡Œ)
- `symbolicregression/model/__init__.py` (ç¬¬66è¡Œ)
- `Oracle/oracle.py` (ç¬¬179è¡Œ)
- `symbolicregression/model/transformer.py` (å¤šå¤„ç±»å‹æå‡)

**é”™è¯¯ä¿¡æ¯**:
```
TypeError: 'list' object is not an iterator
```

**æ ¹æœ¬åŸå› **:

| APIç±»å‹ | PyTorch | PaddlePaddle |
|---------|---------|--------------|
| **model.parameters()** | è¿”å› **generator** | è¿”å› **list** |
| **named_parameters()** | è¿”å› **generator** | è¿”å› **list** |
| **next(model.parameters())** | âœ… å¯è¡Œ | âŒ TypeError |
| **iter(model.parameters())** | âœ… è¿”å›generatoræœ¬èº« | âœ… åˆ›å»ºlist_iterator |

#### å­é—®é¢˜ 8.1: parameters() è¿­ä»£å™¨é—®é¢˜

**é”™è¯¯ä½ç½®**: `model_wrapper.py:40`

**æ‰‹åŠ¨ä¿®å¤** (å·²å®Œæˆ):
```python
# âŒ ä¿®å¤å‰
class ModelWrapper:
    def __init__(self, ...):
        self.device = next(self.embedder.parameters()).device  # â† é”™è¯¯ï¼

# âœ… ä¿®å¤å
class ModelWrapper:
    def __init__(self, ...):
        # PaddlePaddle: parameters() è¿”å›listï¼Œéœ€è¦ç”¨iter()åŒ…è£…
        self.device = next(iter(self.embedder.parameters())).device
```

**ä¸ºä»€ä¹ˆè¿™æ ·ä¿®å¤**:
- `iter(list)` åˆ›å»º list_iteratorï¼Œå¼€é”€æå°
- åœ¨ PyTorch ä¸­ï¼Œ`iter(generator)` è¿”å› generator æœ¬èº«ï¼Œæ— é¢å¤–å¼€é”€
- ä»£ç å…¼å®¹ä¸¤ä¸ªæ¡†æ¶

#### å­é—®é¢˜ 8.2: å‚æ•°ç»Ÿè®¡æ–¹æ³•å·®å¼‚

**é”™è¯¯ä½ç½®**: `model/__init__.py:66`

**æ‰‹åŠ¨ä¿®å¤** (å·²å®Œæˆ):
```python
# âŒ ä¿®å¤å‰
f"Number of parameters ({k}): {sum([p.size for p in v.parameters() if p.requires_grad])}"

# âœ… ä¿®å¤å
f"Number of parameters ({k}): {sum([p.numel() for p in v.parameters() if p.requires_grad])}"
```

**è¯´æ˜**: `.numel()` (number of elements) åœ¨ä¸¤ä¸ªæ¡†æ¶ä¸­éƒ½å­˜åœ¨ä¸”è¯­ä¹‰ä¸€è‡´

#### å­é—®é¢˜ 8.3: ä¼˜åŒ–å™¨å‚æ•°ä¼ é€’

**é”™è¯¯ä½ç½®**: `Oracle/oracle.py:179`

**æ‰‹åŠ¨ä¿®å¤** (å·²å®Œæˆ):
```python
# âŒ ä¿®å¤å‰
optimizer = paddle.optimizer.Adam(
    parameters=model.parameters(), ...
)

# âœ… ä¿®å¤å
optimizer = paddle.optimizer.Adam(
    parameters=list(model.parameters()), ...
)
```

#### å­é—®é¢˜ 8.4: ç±»å‹æå‡é—®é¢˜ - float Ã— int

**æ ¹æœ¬åŸå› **: PaddlePaddle ä¸å…è®¸ float32 å’Œ int64 ä¹‹é—´çš„éšå¼ç±»å‹æå‡

**é”™è¯¯ä¿¡æ¯**:
```
TypeError: (InvalidType) Type promotion only support calculations between floating-point numbers
and between complex and real numbers. But got different data type x: float32, y: int64.
```

**å½±å“ä½ç½®**:
- `transformer.py:561, 705, 708` - `paddle.log(perplexity) * unfinished_sents`

**æ‰‹åŠ¨ä¿®å¤** (å·²å®Œæˆ):
```python
# âŒ ä¿®å¤å‰
word_perplexity.add_(
    paddle.log(next_words_perplexity.detach()) * unfinished_sents  # int64
)

# âœ… ä¿®å¤å
word_perplexity.add_(
    # PaddlePaddle: æ˜¾å¼ç±»å‹è½¬æ¢ float32 * int64 -> float32
    paddle.log(next_words_perplexity.detach()) * unfinished_sents.astype('float32')
)
```

#### å­é—®é¢˜ 8.5: .ne() æ–¹æ³•å‚æ•°ç±»å‹

**æ ¹æœ¬åŸå› **: PaddlePaddle çš„ `.ne()` æ–¹æ³•è¦æ±‚å‚æ•°å¿…é¡»æ˜¯ Tensor

**é”™è¯¯ä¿¡æ¯**:
```
ValueError: not_equal(): argument 'y' (position 1) must be Tensor, but got int
```

**å½±å“ä½ç½®**:
- `transformer.py:565, 714` - `next_words.ne(self.eos_index)`

**æ‰‹åŠ¨ä¿®å¤** (å·²å®Œæˆ):
```python
# âŒ ä¿®å¤å‰
unfinished_sents.mul_(next_words.ne(self.eos_index).long())

# âœ… ä¿®å¤å
# PaddlePaddle: .ne() éœ€è¦tensorå‚æ•°ï¼Œæ”¹ç”¨ != è¿ç®—ç¬¦
unfinished_sents.mul_((next_words != self.eos_index).astype('int64'))
```

**ä¸ºä»€ä¹ˆä½¿ç”¨ `!=`**:
- `!=` è¿ç®—ç¬¦åœ¨ PaddlePaddle ä¸­å¯ä»¥å¤„ç†æ ‡é‡
- æ›´ç®€æ´ï¼Œé¿å…åˆ›å»ºä¸å¿…è¦çš„ tensor

#### å­é—®é¢˜ 8.6: é™¤æ³•ç±»å‹æå‡

**å½±å“ä½ç½®**:
- `transformer.py:575, 726, 727` - `word_perplexity / rows`

**æ‰‹åŠ¨ä¿®å¤** (å·²å®Œæˆ):
```python
# âŒ ä¿®å¤å‰
rows, cols = paddle.nonzero(generated[1:] == self.eos_index, as_tuple=True)
word_perplexity = paddle.exp(word_perplexity / rows)  # rows æ˜¯ int64

# âœ… ä¿®å¤å
rows, cols = paddle.nonzero(generated[1:] == self.eos_index, as_tuple=True)
# PaddlePaddle: æ˜¾å¼è½¬æ¢ int64 -> float32
word_perplexity = paddle.exp(word_perplexity / rows.astype('float32'))
```

**ä¿®å¤æ€»ç»“**:

| æ–‡ä»¶ | ä¿®å¤ç‚¹ | ç±»å‹ | æ•°é‡ |
|------|--------|------|------|
| `model_wrapper.py` | parameters() è¿­ä»£ | è¿­ä»£å™¨ | 1 |
| `model/__init__.py` | å‚æ•°ç»Ÿè®¡æ–¹æ³• | APIå·®å¼‚ | 1 |
| `Oracle/oracle.py` | ä¼˜åŒ–å™¨å‚æ•° | æ˜¾å¼list | 1 |
| `transformer.py` | float Ã— int ä¹˜æ³• | ç±»å‹è½¬æ¢ | 3 |
| `transformer.py` | .ne() æ–¹æ³•è°ƒç”¨ | APIå·®å¼‚ | 2 |
| `transformer.py` | float / int é™¤æ³• | ç±»å‹è½¬æ¢ | 3 |
| **æ€»è®¡** | | | **11å¤„** |

**ä¸ºä»€ä¹ˆ PaConvert æ— æ³•è‡ªåŠ¨å¤„ç†**:
1. éœ€è¦è¯†åˆ« `next(model.parameters())` æ¨¡å¼å¹¶è‡ªåŠ¨æ’å…¥ `iter()`
2. éœ€è¦ç†è§£è¿”å›å€¼ç±»å‹å·®å¼‚ï¼ˆgenerator vs listï¼‰
3. éœ€è¦æ£€æµ‹æ‰€æœ‰æ½œåœ¨çš„ç±»å‹æå‡ä½ç½®
4. éœ€è¦ç†è§£æ–¹æ³•è°ƒç”¨è¯­ä¹‰ï¼ˆ`.ne()` å‚æ•°è¦æ±‚ï¼‰
5. è¶…å‡ºç®€å•APIæ˜ å°„çš„èƒ½åŠ›èŒƒå›´

**æœ€ä½³å®è·µ**:
- ä½¿ç”¨ `next(iter(model.parameters()))` å…¼å®¹ä¸¤ä¸ªæ¡†æ¶
- å‚æ•°ç»Ÿè®¡ä½¿ç”¨ `.numel()` æ ‡å‡†æ–¹æ³•
- ä¼˜åŒ–å™¨åˆå§‹åŒ–æ˜¾å¼ä½¿ç”¨ `list(model.parameters())`
- **å…³é”®**: PaddlePaddle ä¸­æ‰€æœ‰ float å’Œ int çš„æ··åˆè¿ç®—éƒ½éœ€è¦æ˜¾å¼ç±»å‹è½¬æ¢
- ä½¿ç”¨ `!=` è¿ç®—ç¬¦ä»£æ›¿ `.ne()` æ–¹æ³•æ›´ç®€æ´
- é™¤æ³•è¿ç®—å‰ç¡®ä¿ä¸¤è¾¹ç±»å‹ä¸€è‡´

**éªŒè¯ç»“æœ**: âœ… å®Œæ•´è®­ç»ƒ-éªŒè¯å¾ªç¯æˆåŠŸè¿è¡Œï¼ˆ500æ­¥è®­ç»ƒ + 5æ ·æœ¬éªŒè¯ï¼‰

---

## å‚è€ƒèµ„æº

### PaddlePaddle å®˜æ–¹æ–‡æ¡£

- **APIæ˜ å°„è¡¨**: https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/model_convert/pytorch_api_mapping_cn.html
- **è¿ç§»æŒ‡å—**: https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/model_convert/convert_from_pytorch/pytorch_migration_cn.html
- **PaConvertå·¥å…·**: https://github.com/PaddlePaddle/PaConvert

### é¡¹ç›®ç›¸å…³

- **åŸé¡¹ç›®è®ºæ–‡**: Ying et al., Nature Machine Intelligence (2025)
- **GitHub**: PhysicsRegression (åŸPyTorchç‰ˆæœ¬)
- **Google Drive**: é¢„è®­ç»ƒæ¨¡å‹å’Œæ•°æ®é›†

---

**æœ€åæ›´æ–°**: 2026-01-28
**ç»´æŠ¤è€…**: è¿ç§»é¡¹ç›®å›¢é˜Ÿ
**é—®é¢˜åé¦ˆ**: è¯·åœ¨é¡¹ç›®Issueä¸­æŠ¥å‘Šè¿ç§»ç›¸å…³é—®é¢˜

---

## é™„å½•: å®Œæ•´APIå¯¹ç…§è¡¨

| åŠŸèƒ½ç±»åˆ« | PyTorch | PaddlePaddle | å¤‡æ³¨ |
|---------|---------|--------------|------|
| **æ¨¡å—å¯¼å…¥** | `import torch` | `import paddle` | |
| **ç¥ç»ç½‘ç»œåŸºç±»** | `torch.nn.Module` | `paddle.nn.Module` | æˆ– `paddle.nn.Layer` |
| **çº¿æ€§å±‚** | `torch.nn.Linear` | `paddle.compat.nn.Linear` | âš ï¸ ä½¿ç”¨compat |
| **åµŒå…¥å±‚** | `torch.nn.Embedding` | `paddle.nn.Embedding` | |
| **æ¿€æ´»å‡½æ•°** | `torch.tanh` | `paddle.tanh` | |
| | `torch.nn.functional.relu` | `paddle.nn.functional.relu` | |
| **å‚æ•°** | `torch.nn.Parameter` | `paddle.nn.Parameter` | |
| **å®¹å™¨** | `torch.nn.ModuleList` | `paddle.nn.ModuleList` | |
| **å¼ é‡åˆ›å»º** | `torch.tensor` | `paddle.to_tensor` | æ¨èç”¨æ³• |
| | `torch.zeros` | `paddle.zeros` | |
| | `torch.FloatTensor` | `paddle.FloatTensor` | |
| **æ•°æ®ç±»å‹** | `.long()` | `.astype(paddle.int64)` | |
| **å¼ é‡æ“ä½œ** | `.max(dim=1)` | `.max(axis=1)` | âš ï¸ dimâ†’axis |
| **ä¼˜åŒ–å™¨** | `torch.optim.Adam` | `paddle.optimizer.Adam` | å‚æ•°åä¸åŒ |
| | `.zero_grad()` | `.clear_grad()` | âš ï¸ æ–¹æ³•åä¸åŒ |
| **æŸå¤±å‡½æ•°** | `torch.nn.functional.mse_loss` | `paddle.nn.functional.mse_loss` | |
| **æ•°æ®åŠ è½½** | `torch.utils.data.DataLoader` | `paddle.io.DataLoader` | |
| **è®¾å¤‡ç®¡ç†** | `cuda:0` | `gpu:0` | âš ï¸ å­—ç¬¦ä¸²æ ¼å¼ |
| | `torch.cuda.is_available()` | `paddle.is_compiled_with_cuda()` | |
| **æ¨¡å‹ä¿å­˜** | `torch.save` | `paddle.save` | |
| **æ¨¡å‹åŠ è½½** | `torch.load` | `paddle.load` | |
| | `.load_state_dict` | `.set_state_dict` | âš ï¸ æ–¹æ³•åä¸åŒ |
