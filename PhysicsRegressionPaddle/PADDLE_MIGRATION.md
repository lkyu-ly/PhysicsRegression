# PaddlePaddle è¿ç§»æŒ‡å—

> **é¡¹ç›®**: PhysicsRegression PyTorch â†’ PaddlePaddle æ¡†æ¶è¿ç§»
> **è¿ç§»å·¥å…·**: PaConvert (ç™¾åº¦è‡ªåŠ¨è½¬æ¢å·¥å…·)
> **è¿ç§»æ—¥æœŸ**: 2026å¹´
> **æ–‡æ¡£ç‰ˆæœ¬**: 1.0

---

## ğŸ“‹ ç›®å½•

- [è¿ç§»æ¦‚è§ˆ](#è¿ç§»æ¦‚è§ˆ)
- [æ ¸å¿ƒAPIå˜åŒ–](#æ ¸å¿ƒapiå˜åŒ–)
- [paddle_utils.py å…¼å®¹å±‚](#paddle_utilspy-å…¼å®¹å±‚)
- [å…³é”®ä»£ç å¯¹æ¯”](#å…³é”®ä»£ç å¯¹æ¯”)
- [è®¾å¤‡ç®¡ç†å˜åŒ–](#è®¾å¤‡ç®¡ç†å˜åŒ–)
- [æ¨¡å‹æ–‡ä»¶æ ¼å¼](#æ¨¡å‹æ–‡ä»¶æ ¼å¼)
- [ç‰¹æ®Šå¤„ç†è¯´æ˜](#ç‰¹æ®Šå¤„ç†è¯´æ˜)
- [è¿ç§»æ£€æŸ¥æ¸…å•](#è¿ç§»æ£€æŸ¥æ¸…å•)
- [å·²çŸ¥é—®é¢˜](#å·²çŸ¥é—®é¢˜)

---

## è¿ç§»æ¦‚è§ˆ

### è¿ç§»æ–¹æ³•

æœ¬é¡¹ç›®ä½¿ç”¨ **PaConvert** (ç™¾åº¦å®˜æ–¹å·¥å…·) è¿›è¡Œè‡ªåŠ¨ä»£ç è½¬æ¢:

```bash
# è¿ç§»å‘½ä»¤ (å·²å®Œæˆ)
paconvert --in_dir ./PhysicsRegression --out_dir ./PhysicsRegressionPaddle
```

### è¿ç§»çŠ¶æ€

| ç»„ä»¶ | è¿ç§»çŠ¶æ€ | è‡ªåŠ¨è½¬æ¢ç‡ | å¤‡æ³¨ |
|------|---------|-----------|------|
| **ç¬¦å·å›å½’æ¨¡å—** | âœ… å®Œæˆ | ~95% | Transformer, Embedders, Environment |
| **Oracleæ¨¡å—** | âœ… å®Œæˆ | ~98% | SimpleNetç½‘ç»œ, Oracleè®­ç»ƒ |
| **è®­ç»ƒè„šæœ¬** | âœ… å®Œæˆ | ~90% | train.py, trainer.py |
| **è¯„ä¼°è„šæœ¬** | âœ… å®Œæˆ | ~90% | evaluate.py |
| **å·¥å…·å‡½æ•°** | âœ… å®Œæˆ | ~95% | utils.py, metrics.py |
| **å…¼å®¹å±‚** | âœ… è‡ªåŠ¨ç”Ÿæˆ | 100% | paddle_utils.py |

### æ–‡ä»¶ç»“æ„å¯¹æ¯”

```
PhysicsRegression/              PhysicsRegressionPaddle/
â”œâ”€â”€ *.py (PyTorchä»£ç )         â”œâ”€â”€ *.py (PaddlePaddleä»£ç )
â”œâ”€â”€ symbolicregression/         â”œâ”€â”€ symbolicregression/
â”œâ”€â”€ Oracle/                     â”œâ”€â”€ Oracle/
â”œâ”€â”€ physical/                   â”œâ”€â”€ physical/
â”œâ”€â”€ model.pt (PyTorchæ¨¡å‹)     â”œâ”€â”€ model.pdparams (éœ€è½¬æ¢)
â””â”€â”€ CLAUDE.md                   â”œâ”€â”€ paddle_utils.py (æ–°å¢å…¼å®¹å±‚)
                                â””â”€â”€ CLAUDE.md (éœ€æ›´æ–°)
```

---

## æ ¸å¿ƒAPIå˜åŒ–

### æ¨¡å—å¯¼å…¥å˜åŒ–

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

# âœ… PaddlePaddle
import paddle
import paddle.nn as nn
from paddle.io import DataLoader
from paddle_utils import *  # å¯¼å…¥å…¼å®¹å±‚
```

### ç¥ç»ç½‘ç»œæ¨¡å—

#### åŸºç¡€ç±»ç»§æ‰¿

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
class MyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()

# âœ… PaddlePaddle
class MyModel(paddle.nn.Module):  # æˆ– paddle.nn.Layer
    def __init__(self):
        super().__init__()
```

#### çº¿æ€§å±‚

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
self.fc = torch.nn.Linear(128, 64)

# âœ… PaddlePaddle (å…¼å®¹å‘½åç©ºé—´)
self.fc = paddle.compat.nn.Linear(128, 64)

# âš ï¸ æ³¨æ„: ä½¿ç”¨ paddle.compat.nn.Linear è€Œé paddle.nn.Linear
# è¿™æ˜¯PaConvertå·¥å…·çš„å¤„ç†æ–¹å¼,ç¡®ä¿APIå…¼å®¹æ€§
```

#### æ¿€æ´»å‡½æ•°

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
x = torch.tanh(x)
x = torch.nn.functional.relu(x)

# âœ… PaddlePaddle
x = paddle.tanh(x)
x = paddle.nn.functional.relu(x)
```

#### åµŒå…¥å±‚

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
self.embed = torch.nn.Embedding(vocab_size, emb_dim, padding_idx=0)

# âœ… PaddlePaddle
self.embed = paddle.nn.Embedding(vocab_size, emb_dim, padding_idx=0)
```

#### å®¹å™¨æ¨¡å—

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
self.layers = torch.nn.ModuleList([...])

# âœ… PaddlePaddle
self.layers = paddle.nn.ModuleList([...])
```

### å¼ é‡æ“ä½œ

#### å¼ é‡åˆ›å»º

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
x = torch.tensor([1, 2, 3])
x = torch.zeros(3, 4)
x = torch.FloatTensor([1.0, 2.0])

# âœ… PaddlePaddle
x = paddle.to_tensor([1, 2, 3])  # æ³¨æ„: paddle.tensorä¹Ÿå¯ç”¨
x = paddle.zeros([3, 4])
x = paddle.FloatTensor([1.0, 2.0])
```

#### æ•°æ®ç±»å‹

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
x = x.long()
x = x.float()

# âœ… PaddlePaddle
x = x.astype(paddle.long)  # æˆ– paddle.int64
x = x.astype(paddle.float32)
```

#### å¼ é‡æ–¹æ³•å·®å¼‚

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch: ä½¿ç”¨ dim å‚æ•°
x.max(dim=1)
x.sum(dim=0)

# âœ… PaddlePaddle: ä½¿ç”¨ axis å‚æ•°
x.max(axis=1)  # æˆ–ä½¿ç”¨ paddle_utils ä¸­çš„å…¼å®¹æ–¹æ³•
x.sum(axis=0)
```

### ä¼˜åŒ–å™¨

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

# âœ… PaddlePaddle
optimizer = paddle.optimizer.Adam(parameters=model.parameters(), learning_rate=0.001)
optimizer = paddle.optimizer.SGD(parameters=model.parameters(), learning_rate=0.01, momentum=0.9)
```

### æŸå¤±å‡½æ•°

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
loss_fn = torch.nn.MSELoss()
loss = torch.nn.functional.cross_entropy(pred, target)

# âœ… PaddlePaddle
loss_fn = paddle.nn.MSELoss()
loss = paddle.nn.functional.cross_entropy(pred, target)
```

### æ•°æ®åŠ è½½

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
from torch.utils.data import Dataset, DataLoader

class MyDataset(Dataset):
    pass

loader = DataLoader(dataset, batch_size=32, shuffle=True)

# âœ… PaddlePaddle
from paddle.io import Dataset, DataLoader

class MyDataset(Dataset):
    pass

loader = DataLoader(dataset, batch_size=32, shuffle=True)
```

---

## paddle_utils.py å…¼å®¹å±‚

PaConvert è‡ªåŠ¨ç”Ÿæˆçš„å…¼å®¹å±‚æ–‡ä»¶,ç”¨äºå¤„ç†PyTorchå’ŒPaddlePaddleçš„APIå·®å¼‚ã€‚

### æ–‡ä»¶ä½ç½®

```
PhysicsRegressionPaddle/
â””â”€â”€ paddle_utils.py  # é¡¹ç›®æ ¹ç›®å½•
```

### æ ¸å¿ƒåŠŸèƒ½

#### 1. è®¾å¤‡å­—ç¬¦ä¸²è½¬æ¢

**åŠŸèƒ½**: å°†PyTorchçš„è®¾å¤‡å­—ç¬¦ä¸²è½¬æ¢ä¸ºPaddlePaddleæ ¼å¼

```python
def device2int(device):
    """
    è½¬æ¢è®¾å¤‡å­—ç¬¦ä¸²æ ¼å¼

    ç¤ºä¾‹:
        'cuda:0' â†’ 'gpu:0' â†’ 0
        'cuda:1' â†’ 'gpu:1' â†’ 1
    """
    if isinstance(device, str):
        print("Converting device string to int:", device)
        device = device.replace('cuda', 'gpu')
        device = device.replace('gpu:', '')
    return int(device)
```

**ä½¿ç”¨åœºæ™¯**:
```python
# PyTorchä»£ç : device = 'cuda:0'
# PaddlePaddleè½¬æ¢: device = device2int('cuda:0')  # è¿”å› 0
```

#### 2. Tensor.max() æ–¹æ³•é€‚é…

**åŠŸèƒ½**: å¤„ç† `dim`/`axis` å‚æ•°å·®å¼‚

```python
def _Tensor_max(self, *args, **kwargs):
    """
    é€‚é… Tensor.max() æ–¹æ³•

    å¤„ç†:
    1. PyTorch: tensor.max(dim=1)
    2. PaddlePaddle: tensor.max(axis=1)
    3. è¿”å›å€¼å·®å¼‚: (values, indices)
    """
    if "other" in kwargs:
        kwargs["y"] = kwargs.pop("other")
        ret = paddle.maximum(self, *args, **kwargs)
    elif len(args) == 1 and isinstance(args[0], paddle.Tensor):
        ret = paddle.maximum(self, *args, **kwargs)
    else:
        if "dim" in kwargs:
            kwargs["axis"] = kwargs.pop("dim")  # â† å…³é”®è½¬æ¢

        if "axis" in kwargs or len(args) >= 1:
            ret = paddle.max(self, *args, **kwargs), paddle.argmax(self, *args, **kwargs)
        else:
            ret = paddle.max(self, *args, **kwargs)

    return ret

# å°†æ–¹æ³•ç»‘å®šåˆ° Tensor ç±»
setattr(paddle.Tensor, "_max", _Tensor_max)
```

**ä½¿ç”¨ç¤ºä¾‹**:
```python
import paddle
from paddle_utils import *

x = paddle.randn([3, 4])

# PyTorché£æ ¼ (é€šè¿‡å…¼å®¹å±‚è‡ªåŠ¨å¤„ç†)
max_val, max_idx = x._max(dim=1)

# ç­‰ä»·äºPaddlePaddleåŸç”Ÿå†™æ³•:
max_val = x.max(axis=1)
max_idx = x.argmax(axis=1)
```

### ä½¿ç”¨æ–¹æ³•

åœ¨æ¯ä¸ªéœ€è¦å…¼å®¹å¤„ç†çš„æ¨¡å—é¡¶éƒ¨æ·»åŠ :

```python
import paddle
from paddle_utils import *
```

**æ³¨æ„äº‹é¡¹**:
- âš ï¸ `paddle_utils.py` å¿…é¡»ä½äºPythonå¯¼å…¥è·¯å¾„ä¸­
- âš ï¸ å¯¼å…¥é¡ºåº: å…ˆ `import paddle`,å† `from paddle_utils import *`
- âš ï¸ æŸäº›é¡¹ç›®æ–‡ä»¶é€šè¿‡ `sys.path.append` æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„

---

## å…³é”®ä»£ç å¯¹æ¯”

### ç¤ºä¾‹ 1: Transformer MultiHeadAttention

**æ–‡ä»¶**: `symbolicregression/model/transformer.py:54-74`

```python
# ===== PyTorchç‰ˆæœ¬ =====
import torch
import torch.nn as nn

class MultiHeadAttention(nn.Module):
    def __init__(self, n_heads, dim, src_dim, dropout, normalized_attention):
        super().__init__()
        self.q_lin = nn.Linear(dim, dim)
        self.k_lin = nn.Linear(src_dim, dim)
        self.v_lin = nn.Linear(src_dim, dim)
        self.out_lin = nn.Linear(dim, dim)
        if self.normalized_attention:
            self.attention_scale = nn.Parameter(
                torch.tensor(1.0 / math.sqrt(dim // n_heads))
            )

# ===== PaddlePaddleç‰ˆæœ¬ =====
import paddle
from paddle_utils import *

class MultiHeadAttention(paddle.nn.Module):
    def __init__(self, n_heads, dim, src_dim, dropout, normalized_attention):
        super().__init__()
        self.q_lin = paddle.compat.nn.Linear(dim, dim)           # â† ä½¿ç”¨ compat
        self.k_lin = paddle.compat.nn.Linear(src_dim, dim)       # â† ä½¿ç”¨ compat
        self.v_lin = paddle.compat.nn.Linear(src_dim, dim)       # â† ä½¿ç”¨ compat
        self.out_lin = paddle.compat.nn.Linear(dim, dim)         # â† ä½¿ç”¨ compat
        if self.normalized_attention:
            self.attention_scale = paddle.nn.Parameter(
                paddle.tensor(1.0 / math.sqrt(dim // n_heads))  # â† paddle.tensor
            )
```

**å˜åŒ–è¦ç‚¹**:
1. å¯¼å…¥: `torch` â†’ `paddle`
2. ç±»ç»§æ‰¿: `nn.Module` â†’ `paddle.nn.Module`
3. çº¿æ€§å±‚: `nn.Linear` â†’ `paddle.compat.nn.Linear`
4. å‚æ•°: `torch.tensor` â†’ `paddle.tensor`

---

### ç¤ºä¾‹ 2: Oracle SimpleNet

**æ–‡ä»¶**: `Oracle/oracle.py:20-35`

```python
# ===== PyTorchç‰ˆæœ¬ =====
import torch
import torch.nn as nn

class SimpleNet(nn.Module):
    def __init__(self, _in):
        super().__init__()
        self.linear1 = nn.Linear(_in, 128)
        self.linear2 = nn.Linear(128, 128)
        self.linear3 = nn.Linear(128, 64)
        self.linear4 = nn.Linear(64, 64)
        self.linear5 = nn.Linear(64, 1)

    def forward(self, x):
        x = torch.tanh(self.linear1(x))
        x = torch.tanh(self.linear2(x))
        x = torch.tanh(self.linear3(x))
        x = torch.tanh(self.linear4(x))
        x = self.linear5(x)
        return x

# ===== PaddlePaddleç‰ˆæœ¬ =====
import paddle

class SimpleNet(paddle.nn.Module):
    def __init__(self, _in):
        super().__init__()
        self.linear1 = paddle.compat.nn.Linear(_in, 128)
        self.linear2 = paddle.compat.nn.Linear(128, 128)
        self.linear3 = paddle.compat.nn.Linear(128, 64)
        self.linear4 = paddle.compat.nn.Linear(64, 64)
        self.linear5 = paddle.compat.nn.Linear(64, 1)

    def forward(self, x):
        x = paddle.tanh(self.linear1(x))      # â† paddle.tanh
        x = paddle.tanh(self.linear2(x))
        x = paddle.tanh(self.linear3(x))
        x = paddle.tanh(self.linear4(x))
        x = self.linear5(x)
        return x
```

---

### ç¤ºä¾‹ 3: LinearPointEmbedder

**æ–‡ä»¶**: `symbolicregression/model/embedders.py:45-73`

```python
# ===== PyTorchç‰ˆæœ¬ =====
import torch
import torch.nn as nn

class LinearPointEmbedder(Embedder):
    def __init__(self, params, env):
        super().__init__()
        self.embeddings = Embedding(
            len(self.env.float_id2word),
            self.input_dim,
            padding_idx=env.float_word2id["<PAD>"],
        )
        self.activation_fn = torch.nn.functional.relu
        self.hidden_layers = nn.ModuleList()
        self.hidden_layers.append(nn.Linear(size, hidden_size))
        for i in range(self.params.n_emb_layers - 1):
            self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))
        self.fc = nn.Linear(hidden_size, self.output_dim)

# ===== PaddlePaddleç‰ˆæœ¬ =====
import paddle
from paddle_utils import *

class LinearPointEmbedder(Embedder):
    def __init__(self, params, env):
        super().__init__()
        self.embeddings = Embedding(
            len(self.env.float_id2word),
            self.input_dim,
            padding_idx=env.float_word2id["<PAD>"],
        )
        self.activation_fn = paddle.nn.functional.relu  # â† paddle
        self.hidden_layers = paddle.nn.ModuleList()     # â† paddle
        self.hidden_layers.append(paddle.compat.nn.Linear(size, hidden_size))
        for i in range(self.params.n_emb_layers - 1):
            self.hidden_layers.append(paddle.compat.nn.Linear(hidden_size, hidden_size))
        self.fc = paddle.compat.nn.Linear(hidden_size, self.output_dim)
```

---

### ç¤ºä¾‹ 4: è®­ç»ƒå¾ªç¯

```python
# ===== PyTorchç‰ˆæœ¬ =====
import torch

def train_step(model, optimizer, x, y):
    model.train()

    # å‰å‘ä¼ æ’­
    pred = model(x)
    loss = torch.nn.functional.mse_loss(pred, y)

    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss.item()

# ===== PaddlePaddleç‰ˆæœ¬ =====
import paddle

def train_step(model, optimizer, x, y):
    model.train()

    # å‰å‘ä¼ æ’­
    pred = model(x)
    loss = paddle.nn.functional.mse_loss(pred, y)

    # åå‘ä¼ æ’­
    optimizer.clear_grad()  # â† clear_grad è€Œé zero_grad
    loss.backward()
    optimizer.step()

    return loss.item()
```

**å…³é”®å·®å¼‚**:
- `optimizer.zero_grad()` â†’ `optimizer.clear_grad()`

---

## è®¾å¤‡ç®¡ç†å˜åŒ–

### è®¾å¤‡å­—ç¬¦ä¸²æ ¼å¼

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
device = 'cuda:0'
device = 'cuda:1'
device = 'cpu'

# âœ… PaddlePaddle
device = 'gpu:0'   # æˆ–ä½¿ç”¨ device2int() è½¬æ¢ä¸ºæ•´æ•° 0
device = 'gpu:1'   # æˆ–æ•´æ•° 1
device = 'cpu'
```

### æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
model = model.to('cuda:0')
x = x.to('cuda:0')

# âœ… PaddlePaddle (æ–¹å¼1: å­—ç¬¦ä¸²)
model = model.to('gpu:0')
x = x.to('gpu:0')

# âœ… PaddlePaddle (æ–¹å¼2: è®¾å¤‡å¯¹è±¡)
device = paddle.CUDAPlace(0)
model = model.to(device)
x = x.to(device)
```

### æ£€æŸ¥GPUå¯ç”¨æ€§

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
if torch.cuda.is_available():
    device = 'cuda:0'
else:
    device = 'cpu'

# âœ… PaddlePaddle
if paddle.is_compiled_with_cuda():
    device = 'gpu:0'
else:
    device = 'cpu'
```

---

## æ¨¡å‹æ–‡ä»¶æ ¼å¼

### æ¨¡å‹ä¿å­˜

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch: ä¿å­˜ä¸º .pt æˆ– .pth
torch.save(model.state_dict(), 'model.pt')

# âœ… PaddlePaddle: ä¿å­˜ä¸º .pdparams
paddle.save(model.state_dict(), 'model.pdparams')
```

### æ¨¡å‹åŠ è½½

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
state_dict = torch.load('model.pt', map_location='cpu')
model.load_state_dict(state_dict)

# âœ… PaddlePaddle
state_dict = paddle.load('model.pdparams')
model.set_state_dict(state_dict)
```

### æ¨¡å‹è½¬æ¢

**ä»PyTorchè¿ç§»åˆ°PaddlePaddleæ—¶,æ¨¡å‹æ–‡ä»¶éœ€è¦é‡æ–°è®­ç»ƒæˆ–ä½¿ç”¨è½¬æ¢å·¥å…·**:

```python
# æ–¹æ³•1: é‡æ–°è®­ç»ƒ (æ¨è)
# ä½¿ç”¨ train.py é‡æ–°è®­ç»ƒæ¨¡å‹

# æ–¹æ³•2: æ‰‹åŠ¨è½¬æ¢æƒé‡ (å¤æ‚,ä»…å½“å¿…è¦æ—¶)
# éœ€è¦ç¼–å†™è‡ªå®šä¹‰è½¬æ¢è„šæœ¬åŒ¹é…ç½‘ç»œç»“æ„
```

**æ³¨æ„**: ç”±äºæ¶æ„å·®å¼‚,ç›´æ¥è½¬æ¢`.pt`åˆ°`.pdparams`å¯èƒ½ä¸å¯è¡Œ,å»ºè®®é‡æ–°è®­ç»ƒã€‚

---

## ç‰¹æ®Šå¤„ç†è¯´æ˜

### paddle.compat.nn.Linear

**ä¸ºä»€ä¹ˆä½¿ç”¨ `paddle.compat.nn.Linear`?**

PaConvertå·¥å…·ä½¿ç”¨ `paddle.compat.nn.Linear` ç¡®ä¿APIå…¼å®¹æ€§:

```python
# PyTorchåŸå§‹ä»£ç 
fc = torch.nn.Linear(128, 64)

# PaConvertè½¬æ¢å
fc = paddle.compat.nn.Linear(128, 64)

# è€Œéç›´æ¥ä½¿ç”¨
fc = paddle.nn.Linear(128, 64)  # â† å¯èƒ½å­˜åœ¨ç»†å¾®å·®å¼‚
```

**å…¼å®¹å‘½åç©ºé—´ä½ç½®**:
- æ–‡ä»¶: `symbolicregression/model/transformer.py`
- æ–‡ä»¶: `symbolicregression/model/embedders.py`
- æ–‡ä»¶: `Oracle/oracle.py`

**æ˜¯å¦å¯ä»¥æ”¹ä¸º `paddle.nn.Linear`?**

ç†è®ºä¸Šå¯ä»¥,ä½†éœ€è¦éªŒè¯ä»¥ä¸‹å†…å®¹:
1. æƒé‡åˆå§‹åŒ–æ–¹æ³•æ˜¯å¦ä¸€è‡´
2. biaså¤„ç†æ˜¯å¦ç›¸åŒ
3. å‰å‘ä¼ æ’­æ•°å€¼ç²¾åº¦

### sys.path.append

å¤šä¸ªæ–‡ä»¶åŒ…å«ä»¥ä¸‹ä»£ç ä»¥ç¡®ä¿å¯¼å…¥è·¯å¾„æ­£ç¡®:

```python
import sys
sys.path.append("/home/lkyu/baidu/PhysicsRegressionPaddle")
```

**ä½ç½®**:
- `symbolicregression/model/transformer.py:1-2`
- `symbolicregression/model/embedders.py:1-2`

**ä½œç”¨**: ç¡®ä¿ `paddle_utils.py` å¯ä»¥è¢«æ­£ç¡®å¯¼å…¥

**æ³¨æ„**: å¦‚æœé¡¹ç›®è·¯å¾„æ”¹å˜,éœ€è¦æ›´æ–°è¿™äº›è·¯å¾„

---

## è¿ç§»æ£€æŸ¥æ¸…å•

### ä»£ç å±‚é¢

- [x] æ‰€æœ‰ `import torch` å·²æ›¿æ¢ä¸º `import paddle`
- [x] æ‰€æœ‰ `torch.nn.Module` å·²æ›¿æ¢ä¸º `paddle.nn.Module`
- [x] æ‰€æœ‰ `torch.nn.Linear` å·²æ›¿æ¢ä¸º `paddle.compat.nn.Linear`
- [x] æ‰€æœ‰ `torch.optim.Adam` å·²æ›¿æ¢ä¸º `paddle.optimizer.Adam`
- [x] æ‰€æœ‰æ¿€æ´»å‡½æ•°å·²æ›´æ–° (`torch.tanh` â†’ `paddle.tanh`)
- [x] ä¼˜åŒ–å™¨è°ƒç”¨å·²æ›´æ–° (`zero_grad()` â†’ `clear_grad()`)
- [x] è®¾å¤‡å­—ç¬¦ä¸²å·²æ›´æ–° (`cuda:0` â†’ `gpu:0`)
- [x] å¼ é‡æ–¹æ³•å·²æ›´æ–° (`dim` â†’ `axis`)
- [x] `paddle_utils.py` å·²æ­£ç¡®å¯¼å…¥

### åŠŸèƒ½éªŒè¯

- [ ] **æµ‹è¯•è®­ç»ƒæµç¨‹**: è¿è¡Œ `train.py` ç¡®è®¤æ— é”™è¯¯
- [ ] **æµ‹è¯•è¯„ä¼°æµç¨‹**: è¿è¡Œ `evaluate.py` éªŒè¯æ¨¡å‹æ¨ç†
- [ ] **æµ‹è¯•Oracleæ¨¡å—**: éªŒè¯åˆ†æ²»ç­–ç•¥æ­£å¸¸å·¥ä½œ
- [ ] **æµ‹è¯•MCTS/GP**: ç¡®è®¤ä¼˜åŒ–ç®—æ³•å¯ç”¨
- [ ] **å¯¹æ¯”æ•°å€¼ç²¾åº¦**: PyTorch vs PaddlePaddle è¾“å‡ºå·®å¼‚ < 1e-5
- [ ] **GPUå†…å­˜æµ‹è¯•**: ç¡®è®¤æ˜¾å­˜ä½¿ç”¨åˆç†
- [ ] **å¤šå¡è®­ç»ƒ**: æµ‹è¯•åˆ†å¸ƒå¼è®­ç»ƒåŠŸèƒ½

### æ–‡æ¡£æ›´æ–°

- [ ] æ›´æ–°æ ¹ç›®å½• `CLAUDE.md`
- [ ] æ›´æ–° `symbolicregression/CLAUDE.md`
- [ ] æ›´æ–° `Oracle/CLAUDE.md`
- [ ] æ›´æ–° `physical/CLAUDE.md`
- [x] åˆ›å»º `PADDLE_MIGRATION.md` (æœ¬æ–‡æ¡£)

### ç¯å¢ƒé…ç½®

- [ ] åˆ›å»ºPaddlePaddleç‰ˆæœ¬çš„ `environment.yml`
- [ ] æ›´æ–° `README.md` å®‰è£…è¯´æ˜
- [ ] å‡†å¤‡PaddlePaddleç‰ˆæœ¬çš„é¢„è®­ç»ƒæ¨¡å‹

---

## å·²çŸ¥é—®é¢˜

### é—®é¢˜ 1: æ¨¡å‹æ ¼å¼ä¸å…¼å®¹

**æè¿°**: PyTorchçš„ `.pt` æ¨¡å‹æ–‡ä»¶æ— æ³•ç›´æ¥ç”¨äºPaddlePaddle

**è§£å†³æ–¹æ¡ˆ**:
1. ä½¿ç”¨ç›¸åŒæ•°æ®é‡æ–°è®­ç»ƒæ¨¡å‹
2. æˆ–ç¼–å†™è‡ªå®šä¹‰è½¬æ¢è„šæœ¬ (éœ€è¦æ·±å…¥ç†è§£ç½‘ç»œç»“æ„)

### é—®é¢˜ 2: paddle.compat å‘½åç©ºé—´

**æè¿°**: ä»£ç ä¸­ä½¿ç”¨ `paddle.compat.nn.Linear` å¯èƒ½è®©äººå›°æƒ‘

**è¯´æ˜**:
- è¿™æ˜¯PaConvertå·¥å…·çš„æ ‡å‡†åšæ³•
- ç¡®ä¿APIå…¼å®¹æ€§
- ä¸å½±å“åŠŸèƒ½

### é—®é¢˜ 3: æ•°å€¼ç²¾åº¦å·®å¼‚

**æè¿°**: PaddlePaddleå’ŒPyTorchåœ¨æŸäº›æ“ä½œä¸Šå¯èƒ½æœ‰ç»†å¾®æ•°å€¼å·®å¼‚

**éªŒè¯æ–¹æ³•**:
```python
import paddle
import torch
import numpy as np

# ç›¸åŒè¾“å…¥
x_np = np.random.randn(4, 128).astype('float32')

# PyTorch
x_torch = torch.from_numpy(x_np)
out_torch = torch_model(x_torch).detach().numpy()

# PaddlePaddle
x_paddle = paddle.to_tensor(x_np)
out_paddle = paddle_model(x_paddle).numpy()

# å¯¹æ¯”
diff = np.abs(out_torch - out_paddle).max()
print(f"æœ€å¤§å·®å¼‚: {diff}")  # åº”è¯¥ < 1e-5
```

### é—®é¢˜ 4: ç¡¬ç¼–ç è·¯å¾„

**æè¿°**: éƒ¨åˆ†æ–‡ä»¶åŒ…å«ç¡¬ç¼–ç çš„ç»å¯¹è·¯å¾„

**ä½ç½®**:
```python
sys.path.append("/home/lkyu/baidu/PhysicsRegressionPaddle")
```

**è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨ç›¸å¯¹è·¯å¾„æˆ–ç¯å¢ƒå˜é‡

### é—®é¢˜ 5: ä¼˜åŒ–å™¨åŸºç±»åˆå§‹åŒ–ç­¾åä¸å…¼å®¹ âš ï¸

**æè¿°**: PaConvert **æ— æ³•è‡ªåŠ¨å¤„ç†** PyTorch å’Œ PaddlePaddle ä¼˜åŒ–å™¨åŸºç±»çš„æ„é€ å‡½æ•°ç­¾åå·®å¼‚

**å½±å“æ–‡ä»¶**: `symbolicregression/optim.py`

**é—®é¢˜æ ¹æº**:

| æ¡†æ¶ | ä¼˜åŒ–å™¨åŸºç±»ç­¾å |
|------|--------------|
| **PyTorch** | `__init__(self, params, defaults)` |
| **PaddlePaddle** | `__init__(self, learning_rate, parameters, weight_decay, ...)` |

**é”™è¯¯ä»£ç ç¤ºä¾‹**:
```python
# âŒ é”™è¯¯: PaConvertè‡ªåŠ¨è½¬æ¢åçš„ä»£ç 
class Adam(paddle.optimizer.Optimizer):
    def __init__(self, params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        super().__init__(params, defaults)  # â† é”™è¯¯: params è¢«ä¼ ç»™äº† learning_rate
```

**é”™è¯¯ä¿¡æ¯**:
```
TypeError: `parameters` argument should not get dict type, if parameter groups is needed,
please set `parameters` as list of dict
```

**æ‰‹åŠ¨ä¿®å¤** (å·²å®Œæˆ):
```python
# âœ… æ­£ç¡®: ä½¿ç”¨å‘½åå‚æ•°è°ƒç”¨çˆ¶ç±»
class Adam(paddle.optimizer.Optimizer):
    def __init__(self, params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0):
        # å‚æ•°éªŒè¯...

        super().__init__(
            learning_rate=lr,      # æ˜ç¡®æŒ‡å®šå­¦ä¹ ç‡
            parameters=params,     # æ˜ç¡®æŒ‡å®šå‚æ•°åˆ—è¡¨
            weight_decay=weight_decay if weight_decay != 0 else None
        )

        # çŠ¶æ€åˆå§‹åŒ–...
```

**ä¿®å¤ä½ç½®**:
- `Adam` (ç¬¬25è¡Œ)
- `AdamWithWarmup` (ç¬¬94-101è¡Œ)
- `AdamInverseSqrtWithWarmup` (ç¬¬149-156è¡Œ)
- `AdamCosineWithWarmup` (ç¬¬211-218è¡Œ)

**ä¸ºä»€ä¹ˆ PaConvert æ— æ³•è‡ªåŠ¨å¤„ç†**:
1. å‚æ•°ä½ç½®å®Œå…¨ä¸åŒ (ç¬¬1ä¸ªå‚æ•°: `params` vs `learning_rate`)
2. å‚æ•°åç§°ä¸åŒ (`params` vs `parameters`)
3. PaddlePaddle ä¸ä½¿ç”¨ `defaults` å­—å…¸æ¨¡å¼
4. éœ€è¦æ ¹æ®è¯­ä¹‰é‡æ–°æ˜ å°„ï¼Œè¶…å‡ºå·¥å…·èƒ½åŠ›

**æœ€ä½³å®è·µ**:
- è¿ç§»ååŠ¡å¿…æµ‹è¯•ä¼˜åŒ–å™¨åˆå§‹åŒ–
- ä¿æŒ PyTorch ç‰ˆæœ¬ä¸å˜ï¼ˆæ ‡å‡†å®ç°ï¼‰
- åœ¨ PaddlePaddle ç‰ˆæœ¬ä¸­æ‰‹åŠ¨ä¿®å¤

---

### é—®é¢˜ 6: tensor.cuda(device=) å‚æ•°ä¸å…¼å®¹ âš ï¸

**æè¿°**: PaddlePaddle çš„ `tensor.cuda()` ä¸æ¥å— `device` å‚æ•°ï¼Œè¿™æ˜¯ä¸PyTorchçš„å…³é”®å·®å¼‚

**å½±å“æ–‡ä»¶**:
- `symbolicregression/utils.py` (to_cuda å‡½æ•°ï¼Œç¬¬140-152è¡Œ)

**é”™è¯¯ä¿¡æ¯**:
```
TypeError: monkey_patch_tensor.<locals>.cuda() got an unexpected keyword argument 'device'
```

**æ ¹æœ¬åŸå› **:

| APIç±»å‹ | PyTorch | PaddlePaddle |
|---------|---------|--------------|
| **Tensor.cuda()** | `tensor.cuda(device=0)` âœ… æ¥å—deviceå‚æ•° | `tensor.cuda()` âŒ ä¸æ¥å—ä»»ä½•å‚æ•° |
| **Module.cuda()** | `module.cuda(device=0)` âœ… æ¥å—deviceå‚æ•° | `module.cuda(device=device_id)` âœ… æ¥å—deviceå‚æ•° |

**å…³é”®å‘ç°**:
- Moduleå’ŒTensorçš„cuda()æ–¹æ³•è¡Œä¸ºä¸åŒ
- PaddlePaddleçš„Tensor.cuda()å®Œå…¨ä¸æ¥å—å‚æ•°
- å®˜æ–¹æ–‡æ¡£è¯´æ˜ä¸å‡†ç¡®ï¼ˆæ–‡æ¡£è¯´æœ‰device_idå‚æ•°ï¼Œå®é™…ä¸å­˜åœ¨ï¼‰

**æ‰‹åŠ¨ä¿®å¤** (å·²å®Œæˆ):

```python
# âŒ ä¿®å¤å‰ (PyTorché£æ ¼)
def to_cuda(*args, use_cpu=False, device=None):
    if not CUDA or use_cpu:
        return args
    if device is None:
        device = 0
    return [(None if x is None else x.cuda(device=device)) for x in args]
    #                                       ^^^^^^^^^^^^^ é”™è¯¯ï¼

# âœ… ä¿®å¤å (æ–¹æ¡ˆB: å…¨å±€è®¾å¤‡ + æ— å‚æ•°.cuda())
def to_cuda(*args, use_cpu=False, device=None):
    """
    Move tensors to CUDA (PaddlePaddle version).

    Note: PaddlePaddle's Tensor.cuda() does not accept any parameters.
    We set global device first, then call parameter-less .cuda()
    """
    if not CUDA or use_cpu:
        return args

    # è®¾ç½®å…¨å±€é»˜è®¤è®¾å¤‡ (å¦‚æœæŒ‡å®šäº†device)
    if device is not None:
        import paddle
        from paddle_utils import device2int

        if isinstance(device, str):
            device = device2int(device)

        # è®¾ç½®å…¨å±€é»˜è®¤GPUè®¾å¤‡
        paddle.device.set_device(f'gpu:{device}')

    # è°ƒç”¨æ— å‚æ•°çš„ .cuda() æ–¹æ³•
    return [
        (None if x is None else x.cuda())
        for x in args
    ]
```

**ä¿®å¤ç­–ç•¥é€‰æ‹©**:
- æ–¹æ¡ˆA: ä½¿ç”¨`paddle.to_device() + CUDAPlace()`
- **æ–¹æ¡ˆB**: ä½¿ç”¨`paddle.device.set_device() + æ— å‚æ•°.cuda()` â† å·²é‡‡ç”¨
- æ–¹æ¡ˆC: æ£€æŸ¥å¼ é‡è®¾å¤‡ + æ¡ä»¶ç§»åŠ¨

é€‰æ‹©æ–¹æ¡ˆBçš„åŸå› ï¼š
1. ä¸æºä»£ç æœ€ç›¸ä¼¼
2. å®ç°ç®€å•ï¼Œæ˜“äºç»´æŠ¤
3. ä¸Module.cuda()çš„ä½¿ç”¨æ–¹å¼ä¸€è‡´
4. é€‚ç”¨äºå•GPUåœºæ™¯ï¼ˆé¡¹ç›®ä¸»è¦åœºæ™¯ï¼‰

**ä¸ºä»€ä¹ˆ PaConvert æ— æ³•è‡ªåŠ¨å¤„ç†**:
1. éœ€è¦åŒºåˆ†Module.cuda()å’ŒTensor.cuda()çš„ä¸åŒè¡Œä¸º
2. éœ€è¦æ’å…¥å…¨å±€è®¾å¤‡è®¾ç½®é€»è¾‘
3. éœ€è¦ç†è§£deviceå‚æ•°çš„è¯­ä¹‰è½¬æ¢
4. è¶…å‡ºç®€å•APIæ˜ å°„èŒƒå›´

**è°ƒç”¨ä½ç½®** (æ— éœ€ä¿®æ”¹):
- `symbolicregression/model/embedders.py:101-106`
- `symbolicregression/trainer.py:666, 669`

è¿™äº›è°ƒç”¨ä½ç½®æ— éœ€ä¿®æ”¹ï¼Œå› ä¸ºto_cudaçš„æ¥å£ä¿æŒä¸å˜ã€‚

**æœ€ä½³å®è·µ**:
- å¯¹äºModule: å¯ä»¥ä½¿ç”¨`.cuda(device=device_id)`
- å¯¹äºTensor: å¿…é¡»å…ˆ`set_device()`å†è°ƒç”¨æ— å‚æ•°`.cuda()`
- å»ºè®®ç»Ÿä¸€ä½¿ç”¨`paddle.to_device(tensor, place)`æ˜¾å¼æŒ‡å®šè®¾å¤‡

---

### é—®é¢˜ 7: tensor.new() æ–¹æ³•ä¸å­˜åœ¨ âš ï¸

**æè¿°**: PaddlePaddle çš„ Tensor æ²¡æœ‰ `.new()` æ–¹æ³•ï¼Œè¿™æ˜¯PyTorchç‹¬æœ‰çš„ä¾¿æ·åˆ›å»ºå¼ é‡çš„æ–¹æ³•

**å½±å“æ–‡ä»¶**:
- `symbolicregression/model/transformer.py` (15å¤„è°ƒç”¨)

**é”™è¯¯ä¿¡æ¯**:
```
AttributeError: 'Tensor' object has no attribute 'new'. Did you mean: 'ne'?
```

**æ ¹æœ¬åŸå› **:

| åŠŸèƒ½ | PyTorch | PaddlePaddle |
|------|---------|--------------|
| **åˆ›å»ºåŒè®¾å¤‡å¼ é‡** | `tensor.new(size)` | ä¸å­˜åœ¨æ­¤æ–¹æ³• |
| **åˆ›å»ºåŒç±»å‹å¼ é‡** | `tensor.new([1,2,3])` | ä¸å­˜åœ¨æ­¤æ–¹æ³• |
| **ä¾¿æ·æ–¹æ³•** | `tensor.new(5).long()` | éœ€è¦æ˜¾å¼ä½¿ç”¨paddle API |

**æ‰‹åŠ¨ä¿®å¤** (å·²å®Œæˆ):

ä¿®å¤äº†transformer.pyä¸­æ‰€æœ‰15å¤„`.new()`è°ƒç”¨ï¼š

```python
# âŒ ä¿®å¤å‰ (PyTorché£æ ¼)
positions = x.new(slen).long()
positions = paddle.arange(slen, out=positions).unsqueeze(0)

# âœ… ä¿®å¤å (PaddlePaddleé£æ ¼)
positions = paddle.arange(slen, dtype='int64').unsqueeze(0)
```

**ä¿®å¤æ¨¡å¼æ€»ç»“**:

| PyTorchæ¨¡å¼ | PaddlePaddleæ›¿ä»£ | è¯´æ˜ |
|-------------|-----------------|------|
| `x.new(size).fill_(val)` | `paddle.full([size], val, dtype=x.dtype)` | åˆ›å»ºå¡«å……å¼ é‡ |
| `x.new(size).long()` | `paddle.arange(size, dtype='int64')` | åˆ›å»ºæ•´æ•°åºåˆ— |
| `x.new([list])` | `paddle.to_tensor([list], dtype=x.dtype)` | ä»åˆ—è¡¨åˆ›å»º |
| `x.new(size).float().fill_(0)` | `paddle.zeros([size], dtype='float32')` | åˆ›å»ºé›¶å¼ é‡ |

**è¯¦ç»†ä¿®å¤ä½ç½®** (å…±15å¤„):

1. **ç¬¬399è¡Œ** - `fwd()`æ–¹æ³•ä¸­çš„ä½ç½®å¼ é‡:
```python
# ä¿®å¤å‰:
positions = x.new(slen).long()
positions = paddle.arange(slen, out=positions).unsqueeze(0)

# ä¿®å¤å:
positions = paddle.arange(slen, dtype='int64').unsqueeze(0)
```

2. **ç¬¬516-520è¡Œ** - `generate()`æ–¹æ³•ä¸­çš„ç”Ÿæˆå¼ é‡:
```python
# ä¿®å¤å‰:
generated = src_len.new(max_len, bs)
generated.fill_(self.pad_index)
positions = src_len.new(max_len).long()

# ä¿®å¤å:
generated = paddle.full([max_len, bs], self.pad_index, dtype=src_len.dtype)
generated[0].fill_(self.eos_index)
positions = paddle.arange(max_len, dtype='int64').unsqueeze(1).expand([max_len, bs])
```

3. **ç¬¬578-584è¡Œ** - `generate_double_seq()`æ–¹æ³•:
```python
# ä¿®å¤å‰:
generated1 = src_len.new(max_len, bs)
generated2 = src_len.new(max_len, bs, 5)

# ä¿®å¤å:
generated1 = paddle.full([max_len, bs], self.pad_index, dtype=src_len.dtype)
generated2 = paddle.full([max_len, bs, 5], self.pad_index, dtype=src_len.dtype)
```

4. **ç¬¬758-769è¡Œ** - `generate_beam()`æ–¹æ³•çš„æŸæœç´¢åˆå§‹åŒ–:
```python
# ä¿®å¤å‰:
generated = src_len.new(max_len, bs * beam_size)
beam_scores = src_enc.new(bs, beam_size).float().fill_(0)

# ä¿®å¤å:
generated = paddle.full([max_len, bs * beam_size], self.pad_index, dtype=src_len.dtype)
beam_scores = paddle.full([bs, beam_size], 0.0, dtype='float32')
beam_scores[:, 1:] = -1000000000.0
```

5. **ç¬¬778è¡Œ** - æŸæœç´¢å¾ªç¯ä¸­çš„é•¿åº¦å¼ é‡:
```python
# ä¿®å¤å‰:
lengths=src_len.new(bs * beam_size).fill_(cur_len)

# ä¿®å¤å:
lengths=paddle.full([bs * beam_size], cur_len, dtype=src_len.dtype)
```

6. **ç¬¬830-833è¡Œ** - ä»åˆ—è¡¨åˆ›å»ºæŸæœç´¢è·Ÿè¸ªå¼ é‡:
```python
# ä¿®å¤å‰:
beam_scores = beam_scores.new([x[0] for x in next_batch_beam])
beam_words = generated.new([x[1] for x in next_batch_beam])
beam_idx = src_len.new([x[2] for x in next_batch_beam])

# ä¿®å¤å:
beam_scores = paddle.to_tensor([x[0] for x in next_batch_beam], dtype='float32')
beam_words = paddle.to_tensor([x[1] for x in next_batch_beam], dtype=generated.dtype)
beam_idx = paddle.to_tensor([x[2] for x in next_batch_beam], dtype=src_len.dtype)
```

7. **ç¬¬845-853è¡Œ** - æœ€ç»ˆè§£ç ç»“æœ:
```python
# ä¿®å¤å‰:
tgt_len = src_len.new(bs)
decoded = src_len.new(tgt_len._max().item(), bs).fill_(self.pad_index)

# ä¿®å¤å:
tgt_len = paddle.zeros([bs], dtype=src_len.dtype)
# ... å¡«å……tgt_len ...
decoded = paddle.full([int(tgt_len._max().item()), bs], self.pad_index, dtype=src_len.dtype)
```

**ä¸ºä»€ä¹ˆ PaConvert æ— æ³•è‡ªåŠ¨å¤„ç†**:
1. `.new()`æ˜¯PyTorchçš„ä¾¿æ·æ–¹æ³•ï¼Œæ²¡æœ‰ç›´æ¥å¯¹åº”çš„PaddlePaddle API
2. éœ€è¦æ ¹æ®ä½¿ç”¨åœºæ™¯é€‰æ‹©ä¸åŒçš„æ›¿ä»£æ–¹æ³•ï¼ˆfull/zeros/arange/to_tensorï¼‰
3. éœ€è¦ä¿æŒdtypeä¸€è‡´æ€§ï¼Œè¦ä»åŸå¼ é‡æ¨æ–­ç±»å‹
4. æ¶‰åŠå¤æ‚çš„æ–¹æ³•é“¾ï¼ˆå¦‚`.new().long().fill_()`ï¼‰éœ€è¦è¯­ä¹‰ç†è§£
5. è¶…å‡ºç®€å•APIæ˜ å°„çš„èƒ½åŠ›èŒƒå›´

**æœ€ä½³å®è·µ**:
- ä½¿ç”¨`paddle.full()`åˆ›å»ºå¡«å……å¼ é‡
- ä½¿ç”¨`paddle.zeros()`/`paddle.ones()`åˆ›å»ºé›¶/ä¸€å¼ é‡
- ä½¿ç”¨`paddle.arange()`åˆ›å»ºåºåˆ—
- ä½¿ç”¨`paddle.to_tensor()`ä»Pythonåˆ—è¡¨åˆ›å»º
- å§‹ç»ˆæ˜¾å¼æŒ‡å®š`dtype`ç¡®ä¿ç±»å‹ä¸€è‡´

**éªŒè¯ç»“æœ**: âœ… è®­ç»ƒæˆåŠŸè¿è¡Œ120+æ­¥ï¼Œæ‰€æœ‰.new()è°ƒç”¨å·²æ­£ç¡®æ›¿æ¢

---

### é—®é¢˜ 8: model.parameters() è¿”å›ç±»å‹å·®å¼‚ âš ï¸

**æè¿°**: PaddlePaddle çš„ `model.parameters()` è¿”å› list è€Œé generatorï¼Œä»¥åŠç›¸å…³çš„ç±»å‹æå‡é—®é¢˜

**å½±å“æ–‡ä»¶**:
- `symbolicregression/model/model_wrapper.py` (ç¬¬40è¡Œ)
- `symbolicregression/model/__init__.py` (ç¬¬66è¡Œ)
- `Oracle/oracle.py` (ç¬¬179è¡Œ)
- `symbolicregression/model/transformer.py` (å¤šå¤„ç±»å‹æå‡)

**é”™è¯¯ä¿¡æ¯**:
```
TypeError: 'list' object is not an iterator
```

**æ ¹æœ¬åŸå› **:

| APIç±»å‹ | PyTorch | PaddlePaddle |
|---------|---------|--------------|
| **model.parameters()** | è¿”å› **generator** | è¿”å› **list** |
| **named_parameters()** | è¿”å› **generator** | è¿”å› **list** |
| **next(model.parameters())** | âœ… å¯è¡Œ | âŒ TypeError |
| **iter(model.parameters())** | âœ… è¿”å›generatoræœ¬èº« | âœ… åˆ›å»ºlist_iterator |

#### å­é—®é¢˜ 8.1: parameters() è¿­ä»£å™¨é—®é¢˜

**é”™è¯¯ä½ç½®**: `model_wrapper.py:40`

**æ‰‹åŠ¨ä¿®å¤** (å·²å®Œæˆ):
```python
# âŒ ä¿®å¤å‰
class ModelWrapper:
    def __init__(self, ...):
        self.device = next(self.embedder.parameters()).device  # â† é”™è¯¯ï¼

# âœ… ä¿®å¤å
class ModelWrapper:
    def __init__(self, ...):
        # PaddlePaddle: parameters() è¿”å›listï¼Œéœ€è¦ç”¨iter()åŒ…è£…
        self.device = next(iter(self.embedder.parameters())).device
```

**ä¸ºä»€ä¹ˆè¿™æ ·ä¿®å¤**:
- `iter(list)` åˆ›å»º list_iteratorï¼Œå¼€é”€æå°
- åœ¨ PyTorch ä¸­ï¼Œ`iter(generator)` è¿”å› generator æœ¬èº«ï¼Œæ— é¢å¤–å¼€é”€
- ä»£ç å…¼å®¹ä¸¤ä¸ªæ¡†æ¶

#### å­é—®é¢˜ 8.2: å‚æ•°ç»Ÿè®¡æ–¹æ³•å·®å¼‚

**é”™è¯¯ä½ç½®**: `model/__init__.py:66`

**æ‰‹åŠ¨ä¿®å¤** (å·²å®Œæˆ):
```python
# âŒ ä¿®å¤å‰
f"Number of parameters ({k}): {sum([p.size for p in v.parameters() if p.requires_grad])}"

# âœ… ä¿®å¤å
f"Number of parameters ({k}): {sum([p.numel() for p in v.parameters() if p.requires_grad])}"
```

**è¯´æ˜**: `.numel()` (number of elements) åœ¨ä¸¤ä¸ªæ¡†æ¶ä¸­éƒ½å­˜åœ¨ä¸”è¯­ä¹‰ä¸€è‡´

#### å­é—®é¢˜ 8.3: ä¼˜åŒ–å™¨å‚æ•°ä¼ é€’

**é”™è¯¯ä½ç½®**: `Oracle/oracle.py:179`

**æ‰‹åŠ¨ä¿®å¤** (å·²å®Œæˆ):
```python
# âŒ ä¿®å¤å‰
optimizer = paddle.optimizer.Adam(
    parameters=model.parameters(), ...
)

# âœ… ä¿®å¤å
optimizer = paddle.optimizer.Adam(
    parameters=list(model.parameters()), ...
)
```

#### å­é—®é¢˜ 8.4: ç±»å‹æå‡é—®é¢˜ - float Ã— int

**æ ¹æœ¬åŸå› **: PaddlePaddle ä¸å…è®¸ float32 å’Œ int64 ä¹‹é—´çš„éšå¼ç±»å‹æå‡

**é”™è¯¯ä¿¡æ¯**:
```
TypeError: (InvalidType) Type promotion only support calculations between floating-point numbers
and between complex and real numbers. But got different data type x: float32, y: int64.
```

**å½±å“ä½ç½®**:
- `transformer.py:561, 705, 708` - `paddle.log(perplexity) * unfinished_sents`

**æ‰‹åŠ¨ä¿®å¤** (å·²å®Œæˆ):
```python
# âŒ ä¿®å¤å‰
word_perplexity.add_(
    paddle.log(next_words_perplexity.detach()) * unfinished_sents  # int64
)

# âœ… ä¿®å¤å
word_perplexity.add_(
    # PaddlePaddle: æ˜¾å¼ç±»å‹è½¬æ¢ float32 * int64 -> float32
    paddle.log(next_words_perplexity.detach()) * unfinished_sents.astype('float32')
)
```

#### å­é—®é¢˜ 8.5: .ne() æ–¹æ³•å‚æ•°ç±»å‹

**æ ¹æœ¬åŸå› **: PaddlePaddle çš„ `.ne()` æ–¹æ³•è¦æ±‚å‚æ•°å¿…é¡»æ˜¯ Tensor

**é”™è¯¯ä¿¡æ¯**:
```
ValueError: not_equal(): argument 'y' (position 1) must be Tensor, but got int
```

**å½±å“ä½ç½®**:
- `transformer.py:565, 714` - `next_words.ne(self.eos_index)`

**æ‰‹åŠ¨ä¿®å¤** (å·²å®Œæˆ):
```python
# âŒ ä¿®å¤å‰
unfinished_sents.mul_(next_words.ne(self.eos_index).long())

# âœ… ä¿®å¤å
# PaddlePaddle: .ne() éœ€è¦tensorå‚æ•°ï¼Œæ”¹ç”¨ != è¿ç®—ç¬¦
unfinished_sents.mul_((next_words != self.eos_index).astype('int64'))
```

**ä¸ºä»€ä¹ˆä½¿ç”¨ `!=`**:
- `!=` è¿ç®—ç¬¦åœ¨ PaddlePaddle ä¸­å¯ä»¥å¤„ç†æ ‡é‡
- æ›´ç®€æ´ï¼Œé¿å…åˆ›å»ºä¸å¿…è¦çš„ tensor

#### å­é—®é¢˜ 8.6: é™¤æ³•ç±»å‹æå‡

**å½±å“ä½ç½®**:
- `transformer.py:575, 726, 727` - `word_perplexity / rows`

**æ‰‹åŠ¨ä¿®å¤** (å·²å®Œæˆ):
```python
# âŒ ä¿®å¤å‰
rows, cols = paddle.nonzero(generated[1:] == self.eos_index, as_tuple=True)
word_perplexity = paddle.exp(word_perplexity / rows)  # rows æ˜¯ int64

# âœ… ä¿®å¤å
rows, cols = paddle.nonzero(generated[1:] == self.eos_index, as_tuple=True)
# PaddlePaddle: æ˜¾å¼è½¬æ¢ int64 -> float32
word_perplexity = paddle.exp(word_perplexity / rows.astype('float32'))
```

**ä¿®å¤æ€»ç»“**:

| æ–‡ä»¶ | ä¿®å¤ç‚¹ | ç±»å‹ | æ•°é‡ |
|------|--------|------|------|
| `model_wrapper.py` | parameters() è¿­ä»£ | è¿­ä»£å™¨ | 1 |
| `model/__init__.py` | å‚æ•°ç»Ÿè®¡æ–¹æ³• | APIå·®å¼‚ | 1 |
| `Oracle/oracle.py` | ä¼˜åŒ–å™¨å‚æ•° | æ˜¾å¼list | 1 |
| `transformer.py` | float Ã— int ä¹˜æ³• | ç±»å‹è½¬æ¢ | 3 |
| `transformer.py` | .ne() æ–¹æ³•è°ƒç”¨ | APIå·®å¼‚ | 2 |
| `transformer.py` | float / int é™¤æ³• | ç±»å‹è½¬æ¢ | 3 |
| **æ€»è®¡** | | | **11å¤„** |

**ä¸ºä»€ä¹ˆ PaConvert æ— æ³•è‡ªåŠ¨å¤„ç†**:
1. éœ€è¦è¯†åˆ« `next(model.parameters())` æ¨¡å¼å¹¶è‡ªåŠ¨æ’å…¥ `iter()`
2. éœ€è¦ç†è§£è¿”å›å€¼ç±»å‹å·®å¼‚ï¼ˆgenerator vs listï¼‰
3. éœ€è¦æ£€æµ‹æ‰€æœ‰æ½œåœ¨çš„ç±»å‹æå‡ä½ç½®
4. éœ€è¦ç†è§£æ–¹æ³•è°ƒç”¨è¯­ä¹‰ï¼ˆ`.ne()` å‚æ•°è¦æ±‚ï¼‰
5. è¶…å‡ºç®€å•APIæ˜ å°„çš„èƒ½åŠ›èŒƒå›´

**æœ€ä½³å®è·µ**:
- ä½¿ç”¨ `next(iter(model.parameters()))` å…¼å®¹ä¸¤ä¸ªæ¡†æ¶
- å‚æ•°ç»Ÿè®¡ä½¿ç”¨ `.numel()` æ ‡å‡†æ–¹æ³•
- ä¼˜åŒ–å™¨åˆå§‹åŒ–æ˜¾å¼ä½¿ç”¨ `list(model.parameters())`
- **å…³é”®**: PaddlePaddle ä¸­æ‰€æœ‰ float å’Œ int çš„æ··åˆè¿ç®—éƒ½éœ€è¦æ˜¾å¼ç±»å‹è½¬æ¢
- ä½¿ç”¨ `!=` è¿ç®—ç¬¦ä»£æ›¿ `.ne()` æ–¹æ³•æ›´ç®€æ´
- é™¤æ³•è¿ç®—å‰ç¡®ä¿ä¸¤è¾¹ç±»å‹ä¸€è‡´

**éªŒè¯ç»“æœ**: âœ… å®Œæ•´è®­ç»ƒ-éªŒè¯å¾ªç¯æˆåŠŸè¿è¡Œï¼ˆ500æ­¥è®­ç»ƒ + 5æ ·æœ¬éªŒè¯ï¼‰

---

## å‚è€ƒèµ„æº

### PaddlePaddle å®˜æ–¹æ–‡æ¡£

- **APIæ˜ å°„è¡¨**: https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/model_convert/pytorch_api_mapping_cn.html
- **è¿ç§»æŒ‡å—**: https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/model_convert/convert_from_pytorch/pytorch_migration_cn.html
- **PaConvertå·¥å…·**: https://github.com/PaddlePaddle/PaConvert

### é¡¹ç›®ç›¸å…³

- **åŸé¡¹ç›®è®ºæ–‡**: Ying et al., Nature Machine Intelligence (2025)
- **GitHub**: PhysicsRegression (åŸPyTorchç‰ˆæœ¬)
- **Google Drive**: é¢„è®­ç»ƒæ¨¡å‹å’Œæ•°æ®é›†

---

**æœ€åæ›´æ–°**: 2026-01-28
**ç»´æŠ¤è€…**: è¿ç§»é¡¹ç›®å›¢é˜Ÿ
**é—®é¢˜åé¦ˆ**: è¯·åœ¨é¡¹ç›®Issueä¸­æŠ¥å‘Šè¿ç§»ç›¸å…³é—®é¢˜

---

## é™„å½•: å®Œæ•´APIå¯¹ç…§è¡¨

| åŠŸèƒ½ç±»åˆ« | PyTorch | PaddlePaddle | å¤‡æ³¨ |
|---------|---------|--------------|------|
| **æ¨¡å—å¯¼å…¥** | `import torch` | `import paddle` | |
| **ç¥ç»ç½‘ç»œåŸºç±»** | `torch.nn.Module` | `paddle.nn.Module` | æˆ– `paddle.nn.Layer` |
| **çº¿æ€§å±‚** | `torch.nn.Linear` | `paddle.compat.nn.Linear` | âš ï¸ ä½¿ç”¨compat |
| **åµŒå…¥å±‚** | `torch.nn.Embedding` | `paddle.nn.Embedding` | |
| **æ¿€æ´»å‡½æ•°** | `torch.tanh` | `paddle.tanh` | |
| | `torch.nn.functional.relu` | `paddle.nn.functional.relu` | |
| **å‚æ•°** | `torch.nn.Parameter` | `paddle.nn.Parameter` | |
| **å®¹å™¨** | `torch.nn.ModuleList` | `paddle.nn.ModuleList` | |
| **å¼ é‡åˆ›å»º** | `torch.tensor` | `paddle.to_tensor` | æ¨èç”¨æ³• |
| | `torch.zeros` | `paddle.zeros` | |
| | `torch.FloatTensor` | `paddle.FloatTensor` | |
| **æ•°æ®ç±»å‹** | `.long()` | `.astype(paddle.int64)` | |
| **å¼ é‡æ“ä½œ** | `.max(dim=1)` | `.max(axis=1)` | âš ï¸ dimâ†’axis |
| **ä¼˜åŒ–å™¨** | `torch.optim.Adam` | `paddle.optimizer.Adam` | å‚æ•°åä¸åŒ |
| | `.zero_grad()` | `.clear_grad()` | âš ï¸ æ–¹æ³•åä¸åŒ |
| **æŸå¤±å‡½æ•°** | `torch.nn.functional.mse_loss` | `paddle.nn.functional.mse_loss` | |
| **æ•°æ®åŠ è½½** | `torch.utils.data.DataLoader` | `paddle.io.DataLoader` | |
| **è®¾å¤‡ç®¡ç†** | `cuda:0` | `gpu:0` | âš ï¸ å­—ç¬¦ä¸²æ ¼å¼ |
| | `torch.cuda.is_available()` | `paddle.is_compiled_with_cuda()` | |
| **æ¨¡å‹ä¿å­˜** | `torch.save` | `paddle.save` | |
| **æ¨¡å‹åŠ è½½** | `torch.load` | `paddle.load` | |
| | `.load_state_dict` | `.set_state_dict` | âš ï¸ æ–¹æ³•åä¸åŒ |

---

## é—®é¢˜ 9: æ¨¡å‹åŠ è½½æ—¶ params å±æ€§è®¿é—®é”™è¯¯ âš ï¸

### é—®é¢˜æè¿°

ä»è®­ç»ƒ checkpoint åŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç†æ—¶ï¼Œå‡ºç°å±æ€§è®¿é—®é”™è¯¯ã€‚

### é”™è¯¯ä¿¡æ¯

```python
AttributeError: 'dict' object has no attribute 'rescale'
```

### å½±å“æ–‡ä»¶

- `PhysicsRegressionPaddle/PhysicsRegression.py` (ç¬¬ 33 è¡Œ)
- `PhysicsRegression/PhysicsRegression.py` (ç¬¬ 38 è¡Œ) - **PyTorch ç‰ˆæœ¬ä¹Ÿæœ‰æ­¤é—®é¢˜**

### æ ¹æœ¬åŸå› 

é¡¹ç›®ä¸­æœ‰**ä¸¤ç§ä¸åŒçš„æ¨¡å‹ä¿å­˜æ–¹å¼**ï¼š

#### 1ï¸âƒ£ æ¨ç†æ¨¡å‹ä¿å­˜ï¼ˆPhyReg.save()ï¼‰
```python
def save(self, path):
    save_dict = {
        'embedder': self.mw.embedder.state_dict(),
        'encoder': self.mw.encoder.state_dict(),
        'decoder': self.mw.decoder.state_dict(),
        'params': self.params,  # â† ç›´æ¥ä¿å­˜ Namespace å¯¹è±¡
    }
    paddle.save(obj=save_dict, path=path)
```
- **ä¿å­˜å†…å®¹**: `params` ä¿æŒä¸º `argparse.Namespace` å¯¹è±¡
- **åŠ è½½å**: params ä»ç„¶æ˜¯ Namespaceï¼Œå¯ä»¥å±æ€§è®¿é—® âœ…
- **ç¤ºä¾‹æ–‡ä»¶**: `model.pt`ï¼ˆé¢„è®­ç»ƒæ¨¡å‹ï¼‰

#### 2ï¸âƒ£ è®­ç»ƒ Checkpoint ä¿å­˜ï¼ˆtrainer.pyï¼‰
```python
def save_checkpoint(self, name, include_optimizer=True):
    data = {
        "epoch": self.epoch,
        "n_total_iter": self.n_total_iter,
        "best_metrics": self.best_metrics,
        "best_stopping_criterion": self.best_stopping_criterion,
        "params": {k: v for k, v in self.params.__dict__.items()},  # â† è½¬ä¸º dict
    }
    paddle.save(obj=data, path=path)
```
- **ä¿å­˜å†…å®¹**: `params` è¢«è½¬æ¢ä¸ºæ™®é€š Python **dict**
- **åŠ è½½å**: params æ˜¯ dictï¼Œä¸æ”¯æŒå±æ€§è®¿é—® âŒ
- **ç¤ºä¾‹æ–‡ä»¶**: `checkpoint.pth`ï¼ˆè®­ç»ƒ checkpointï¼‰

### æ‰‹åŠ¨ä¿®å¤ (å·²å®Œæˆ)

**PaddlePaddle ç‰ˆæœ¬** (`PhysicsRegressionPaddle/PhysicsRegression.py`):
```python
# ä¿®å¤å‰ âŒ
model = paddle.load(path=str(path))
params = model["params"]
params.rescale = False  # â† AttributeError

# ä¿®å¤å âœ…
from argparse import Namespace

model = paddle.load(path=str(path))
# å…¼å®¹ä¸¤ç§ä¿å­˜æ–¹å¼ï¼šæ¨ç†æ¨¡å‹(Namespace)å’Œè®­ç»ƒcheckpoint(dict)
params = model["params"] if isinstance(model["params"], Namespace) else Namespace(**model["params"])
params.rescale = False  # â† æ­£å¸¸å·¥ä½œ
```

**PyTorch ç‰ˆæœ¬** (`PhysicsRegression/PhysicsRegression.py`):
```python
# ä¿®å¤å‰ âŒ
model = torch.load(path)
params = model['params']
params.rescale = False  # â† AttributeError

# ä¿®å¤å âœ…
from argparse import Namespace

model = torch.load(path)
# å…¼å®¹ä¸¤ç§ä¿å­˜æ–¹å¼
params = model['params'] if isinstance(model['params'], Namespace) else Namespace(**model['params'])
params.rescale = False  # â† æ­£å¸¸å·¥ä½œ
```

### ä¸ºä»€ä¹ˆæ˜¯é€šç”¨é—®é¢˜

1. **ä¸æ˜¯æ¡†æ¶å·®å¼‚**: PyTorch å’Œ PaddlePaddle ç‰ˆæœ¬éƒ½æœ‰æ­¤ bug
2. **è®¾è®¡ä¸ä¸€è‡´**: ä¿å­˜æ—¶è½¬ä¸º dictï¼ŒåŠ è½½æ—¶å‡è®¾æ˜¯ Namespace
3. **ä¸¤ç§ä¿å­˜æ–¹å¼æ··ç”¨**: `PhyReg.save()` ä¿å­˜ Namespaceï¼Œ`trainer.py` ä¿å­˜ dict

### ä¿®å¤æ•ˆæœ

```python
# æµ‹è¯•ç»“æœ
from PhysicsRegression import PhyReg

phyreg = PhyReg(path='./checkpoint.pth')
print(f'âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼')
print(f'params ç±»å‹: {type(phyreg.params)}')  # <class 'argparse.Namespace'>
print(f'params.rescale: {phyreg.params.rescale}')  # False
```

### æœ€ä½³å®è·µ

1. **åŠ è½½æ—¶ç»Ÿä¸€å¤„ç†**: ä½¿ç”¨ `isinstance` æ£€æŸ¥ç±»å‹ï¼Œè‡ªåŠ¨è½¬æ¢
2. **æˆ–è€…ç»Ÿä¸€ä¿å­˜æ–¹å¼**:
   - æ–¹æ¡ˆ A: æ‰€æœ‰åœ°æ–¹éƒ½ä¿å­˜ä¸º dictï¼ŒåŠ è½½æ—¶ç»Ÿä¸€è½¬æ¢
   - æ–¹æ¡ˆ B: æ‰€æœ‰åœ°æ–¹éƒ½ä¿å­˜ä¸º Namespaceï¼ˆä½†å¯èƒ½æœ‰åºåˆ—åŒ–é™åˆ¶ï¼‰
3. **ä¸è¦æ··ç”¨è®¿é—®æ–¹å¼**: ç»Ÿä¸€ä½¿ç”¨å±æ€§è®¿é—®ï¼ˆæ¨èï¼‰æˆ–å­—å…¸è®¿é—®

### ç›¸å…³é—®é¢˜

- æ— ï¼ˆç‹¬ç«‹é—®é¢˜ï¼‰

---

## Problem 10: PaddlePaddleç±»å‹æå‡ä¸¥æ ¼æ€§ - å•ä½æ¯”è¾ƒæ—¶çš„ç±»å‹ä¸åŒ¹é…

### é—®é¢˜æè¿°

**é”™è¯¯ä¿¡æ¯**:
```
TypeError: (InvalidType) Type promotion only support calculations between floating-point numbers
and between complex and real numbers. But got different data type x: float64, y: int64.
(at /paddle/paddle/phi/common/type_promotion.h:220)
```

**é”™è¯¯ä½ç½®**: `symbolicregression/envs/encoders.py:417`

**è§¦å‘æ¡ä»¶**:
- è®­ç»ƒå‰å‡ ä¸ªepochæ­£å¸¸
- æŸä¸ªepochçš„éªŒè¯é˜¶æ®µ,å½“æ¨¡å‹ç”ŸæˆåŒ…å« `x_` å˜é‡çš„å…¬å¼æ—¶è§¦å‘
- å•ä½æ£€æŸ¥æ—¶ `temp.unit != dim` æ¯”è¾ƒå¤±è´¥

### æ ¹æœ¬åŸå› 

**æ ¸å¿ƒé—®é¢˜**: PaddlePaddleå¯¹ç±»å‹æå‡çš„æ£€æŸ¥æ¯”PyTorchæ›´ä¸¥æ ¼

```python
# encoders.py:417
if any(temp.unit != dim):  # â† è¿™é‡Œè§¦å‘é”™è¯¯
    return False
```

**ç±»å‹ä¸åŒ¹é…**:
- `temp.unit`: `np.ndarray(dtype=float64)` - 5ç»´ç‰©ç†å•ä½å‘é‡ `[kg, m, s, T, V]`
- `dim` (æ¥è‡ª `xy_units[idx]`): Python `int` æˆ– `list` (å…ƒç´ ä¸º `int64`)

**ä¸ºä»€ä¹ˆPyTorchæ²¡é—®é¢˜**:
- PyTorchçš„å¼ é‡æ¯”è¾ƒä¼šéšå¼è½¬æ¢ç±»å‹
- PaddlePaddleæ˜ç¡®æ‹’ç» `float64 != int64` çš„æ¯”è¾ƒ,æŠ›å‡ºç±»å‹æå‡é”™è¯¯

**ä¸ºä»€ä¹ˆå‰å‡ ä¸ªepochæ­£å¸¸**:
- å‰å‡ ä¸ªepochç”Ÿæˆçš„å…¬å¼æ°å¥½ä¸åŒ…å« `x_` å˜é‡(æˆ–å•ä½æ£€æŸ¥æ€»æ˜¯é€šè¿‡)
- åç»­epochç”Ÿæˆçš„å…¬å¼è§¦å‘äº†è¿™ä¸ªä»£ç åˆ†æ”¯

### è§£å†³æ–¹æ¡ˆ

**ä¿®å¤ä½ç½®**: `PhysicsRegressionPaddle/symbolicregression/envs/encoders.py:383-431`

**ä¿®å¤å‰** âŒ:
```python
def check_units(self, tree, xy_units):
    stack = [tree]
    while stack:
        temp = stack.pop(0)
        value = str(temp.value)
        # ... çœç•¥ä¸­é—´é€»è¾‘ ...
        elif value.startswith("x_"):
            idx = int(temp.value[2:])
            if not idx < len(xy_units) - 1:
                return False
            dim = xy_units[idx]
            if any(temp.unit != dim):  # â† float64 != int æŠ¥é”™
                return False
        stack += temp.children
    # ...
    if any(tree.unit != xy_units[-1]):  # â† åŒæ ·çš„é—®é¢˜
        return False
    return True
```

**ä¿®å¤å** âœ…:
```python
def check_units(self, tree, xy_units):
    stack = [tree]
    while stack:
        temp = stack.pop(0)
        value = str(temp.value)
        # ... çœç•¥ä¸­é—´é€»è¾‘ ...
        elif value.startswith("x_"):
            idx = int(temp.value[2:])
            if not idx < len(xy_units) - 1:
                return False
            dim = xy_units[idx]
            # âœ… ä¿®å¤ï¼šç»Ÿä¸€è½¬æ¢ä¸º np.ndarray(float64) ç±»å‹
            if not isinstance(dim, np.ndarray):
                dim = np.array(dim, dtype=np.float64)
            if any(temp.unit != dim):
                return False
        stack += temp.children
    if isinstance(xy_units[-1], str) and xy_units[-1] == "<UNKNOWN_PHYSICAL_UNITS>":
        return True
    # âœ… ä¿®å¤ï¼šç»Ÿä¸€è½¬æ¢ä¸º np.ndarray(float64) ç±»å‹
    last_unit = xy_units[-1]
    if not isinstance(last_unit, np.ndarray):
        last_unit = np.array(last_unit, dtype=np.float64)
    if any(tree.unit != last_unit):
        return False
    return True
```

### å…³é”®å˜åŒ–

1. **ç±»å‹ç»Ÿä¸€åŒ–**:
   ```python
   # åœ¨æ¯”è¾ƒå‰ç¡®ä¿ç±»å‹ä¸€è‡´
   if not isinstance(dim, np.ndarray):
       dim = np.array(dim, dtype=np.float64)
   ```

2. **ä¸¤å¤„ä¿®å¤**:
   - ç¬¬417è¡Œ: `x_` å˜é‡å•ä½æ£€æŸ¥
   - ç¬¬429è¡Œ: è¾“å‡ºå•ä½æ£€æŸ¥

3. **å…¼å®¹æ€§**:
   - å¦‚æœ `dim` å·²ç»æ˜¯ `np.ndarray`,ä¸åšä»»ä½•è½¬æ¢
   - å¦‚æœæ˜¯ `int` æˆ– `list`,è½¬æ¢ä¸º `np.ndarray(float64)`

### ä¸ºä»€ä¹ˆè¿™æ˜¯PaddlePaddleç‰¹æœ‰é—®é¢˜

**PyTorch**:
```python
import torch
x = torch.tensor([1.0, 2.0])  # float64
y = 1  # int
result = x != y  # âœ… æ­£å¸¸å·¥ä½œ,éšå¼ç±»å‹è½¬æ¢
```

**PaddlePaddle**:
```python
import paddle
x = paddle.to_tensor([1.0, 2.0])  # float64
y = 1  # int
result = x != y  # âŒ TypeError: Type promotion error
```

**è®¾è®¡ç†å¿µ**:
- PyTorch: å®½æ¾çš„ç±»å‹ç³»ç»Ÿ,è‡ªåŠ¨ç±»å‹æå‡
- PaddlePaddle: ä¸¥æ ¼çš„ç±»å‹æ£€æŸ¥,æ˜ç¡®ç±»å‹è½¬æ¢

### ä¿®å¤æ•ˆæœ

```python
# æµ‹è¯•ç»“æœ
# ä¿®å¤å‰: ç¬¬6ä¸ªepochéªŒè¯æ—¶å´©æºƒ
# ä¿®å¤å: æ‰€æœ‰epochæ­£å¸¸è®­ç»ƒå’ŒéªŒè¯

# è®­ç»ƒæ—¥å¿—ç¤ºä¾‹:
INFO - 01/29/26 09:33:07 - 0:35:48 - ============ End of epoch 6 ============
INFO - 01/29/26 09:33:07 - 0:35:48 - ====== STARTING EVALUATION E2E:VALID =======
INFO - 01/29/26 09:33:07 - 0:35:48 - Creating valid1 iterator for functions ...
âœ… å•ä½æ£€æŸ¥æ­£å¸¸é€šè¿‡
âœ… éªŒè¯å®Œæˆ
```

### æœ€ä½³å®è·µ

1. **ç±»å‹æ¯”è¾ƒå‰ç»Ÿä¸€è½¬æ¢**:
   ```python
   # æ¨èæ¨¡å¼
   if not isinstance(value, np.ndarray):
       value = np.array(value, dtype=np.float64)
   ```

2. **é¿å…æ··åˆç±»å‹æ¯”è¾ƒ**:
   ```python
   # âŒ ä¸æ¨è
   np_array != python_int

   # âœ… æ¨è
   np_array != np.array([python_int], dtype=np.float64)
   ```

3. **å•å…ƒæµ‹è¯•è¦†ç›–**:
   - æµ‹è¯•ä¸åŒç±»å‹çš„å•ä½è¾“å…¥
   - æµ‹è¯•æ··åˆç±»å‹åœºæ™¯

### ç›¸å…³é—®é¢˜

- æ— ï¼ˆç‹¬ç«‹çš„ç±»å‹ç³»ç»Ÿé—®é¢˜ï¼‰

### å‚è€ƒé“¾æ¥

- PaddlePaddleç±»å‹æå‡æ–‡æ¡£: [paddle/phi/common/type_promotion.h](https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/phi/common/type_promotion.h)
- ç±»å‹æå‡è§„åˆ™: ä»…æ”¯æŒæµ®ç‚¹æ•°ä¹‹é—´ã€å¤æ•°å’Œå®æ•°ä¹‹é—´çš„æå‡

---

## Problem 11: paddle.incubate.autograd.Hessian API å·®å¼‚ âš ï¸

### é—®é¢˜æè¿°

**é”™è¯¯ä¿¡æ¯**:
```
AttributeError: 'Hessian' object has no attribute 'detach'
```

**é”™è¯¯ä½ç½®**: `Oracle/oracle.py:300`

**è§¦å‘åœºæ™¯**: è¿è¡Œ example.ipynb çš„ "Divide-and-Conquer Strategy" ç¤ºä¾‹ï¼ŒOracle è®­ç»ƒå®Œæˆåè®¡ç®— Hessian çŸ©é˜µæ—¶æŠ¥é”™

### æ ¹æœ¬åŸå› 

PyTorch å’Œ PaddlePaddle çš„ Hessian API **è®¾è®¡èŒƒå¼å®Œå…¨ä¸åŒ**ï¼š

| åŠŸèƒ½ | PyTorch | PaddlePaddle |
|------|---------|--------------|
| **APIç±»å‹** | å‡½æ•° `hessian()` | ç±» `Hessian()` |
| **è¿”å›ç±»å‹** | `torch.Tensor` (ç›´æ¥å¯ç”¨) | `Hessian` å¯¹è±¡ (éœ€æå–) |
| **è®¿é—®æ–¹å¼** | ç›´æ¥ä½¿ç”¨è¿”å›å€¼ | éœ€è¦åˆ‡ç‰‡ `[:]` æå–å¼ é‡ |
| **å¼ é‡æ–¹æ³•** | ç›´æ¥è°ƒç”¨ `.detach()` | å…ˆæå–ï¼Œå†è°ƒç”¨ |

**PyTorch ç‰ˆæœ¬** (`PhysicsRegression/Oracle/oracle.py:328-329`):
```python
from torch.autograd.functional import hessian

h_pred = hessian(model, xx)  # âœ… è¿”å› torch.Tensor
h_pred = h_pred.detach().cpu().clone().unsqueeze(0)  # âœ… ç›´æ¥è°ƒç”¨å¼ é‡æ–¹æ³•
```

**PaddlePaddle ç‰ˆæœ¬** (`PhysicsRegressionPaddle/Oracle/oracle.py:297-300`):
```python
h_pred = paddle.incubate.autograd.Hessian(
    func=model, xs=xx, is_batched=False
)
h_pred = h_pred.detach().cpu().clone().unsqueeze(0)  # âŒ AttributeError
```

### ä¸ºä»€ä¹ˆè®¾è®¡ä¸åŒ

**PaddlePaddle å»¶è¿Ÿè®¡ç®—è®¾è®¡**:
- `Hessian` ç±»å°è£…è®¡ç®—é€»è¾‘ï¼Œæ”¯æŒæŒ‰éœ€è®¡ç®—å­çŸ©é˜µ
- ä¾‹å¦‚: `h_obj[0:2, 0:2]` åªè®¡ç®—å·¦ä¸Šè§’ 2Ã—2 åŒºåŸŸï¼Œæé«˜æ•ˆç‡
- ä½¿ç”¨ `h_obj[:]` å¯ä»¥ä¸€æ¬¡æ€§è®¡ç®—å®Œæ•´çŸ©é˜µ

**PyTorch ç«‹å³è®¡ç®—**:
- `hessian()` å‡½æ•°ç›´æ¥è®¡ç®—å¹¶è¿”å›å®Œæ•´ Hessian çŸ©é˜µ
- ç®€æ´ç›´è§‚ï¼Œä½†å¤§çŸ©é˜µè®¡ç®—å¼€é”€å¤§

### æ‰‹åŠ¨ä¿®å¤ (å·²å®Œæˆ)

**ä¿®å¤ä½ç½®**: `PhysicsRegressionPaddle/Oracle/oracle.py:294-309`

**ä¿®å¤å‰** âŒ:
```python
hs_pred = paddle.zeros((0, num_variables, num_variables))
for x in test_set:
    xx = paddle.from_numpy(x).float().to(device=self.params.device)
    h_pred = paddle.incubate.autograd.Hessian(
        func=model, xs=xx, is_batched=False
    )
    h_pred = h_pred.detach().cpu().clone().unsqueeze(0)  # âŒ é”™è¯¯
    hs_pred = paddle.cat((hs_pred, h_pred), dim=0)
```

**ä¿®å¤å** âœ…:
```python
hs_pred = paddle.zeros((0, num_variables, num_variables))
for x in test_set:
    # ç¡®ä¿è¾“å…¥å¼ é‡å¯æ±‚å¯¼ï¼ˆPaddlePaddle: å¿…é¡»è®¾ç½®ä»¥è®¡ç®—äºŒé˜¶å¯¼æ•°ï¼‰
    xx = paddle.from_numpy(x).float().to(device=self.params.device)
    xx.stop_gradient = False

    # åˆ›å»º Hessian å¯¹è±¡
    h_pred_obj = paddle.incubate.autograd.Hessian(
        func=model, xs=xx, is_batched=False
    )
    # PaddlePaddle: ä½¿ç”¨åˆ‡ç‰‡æ“ä½œæå–å®é™…çš„å¼ é‡çŸ©é˜µ
    h_pred = h_pred_obj[:]

    # æ ‡å‡†å¼ é‡æ“ä½œï¼ˆä¸ PyTorch ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰
    h_pred = h_pred.detach().cpu().clone().unsqueeze(0)
    hs_pred = paddle.cat((hs_pred, h_pred), dim=0)
```

### å…³é”®å˜åŒ–

1. **ç¬¬298è¡Œæ–°å¢**: `xx.stop_gradient = False`
   - **åŸå› **: PaddlePaddle é»˜è®¤ `from_numpy()` åˆ›å»ºçš„å¼ é‡ `stop_gradient=True`
   - **å¿…è¦æ€§**: ä¸è®¾ç½®ä¼šå¯¼è‡´ Hessian è®¡ç®—å¤±è´¥æˆ–è¿”å›é›¶çŸ©é˜µ

2. **ç¬¬301-303è¡Œ**: ä½¿ç”¨æ›´æ¸…æ™°çš„å˜é‡å
   ```python
   h_pred_obj = paddle.incubate.autograd.Hessian(...)  # Hessian å¯¹è±¡
   ```

3. **ç¬¬305è¡Œæ ¸å¿ƒä¿®å¤**: ä½¿ç”¨åˆ‡ç‰‡æ“ä½œæå–å¼ é‡
   ```python
   h_pred = h_pred_obj[:]  # â† è¿”å› paddle.Tensor
   ```

4. **ç¬¬308è¡Œ**: æ­£å¸¸è°ƒç”¨å¼ é‡æ–¹æ³•
   ```python
   h_pred = h_pred.detach().cpu().clone().unsqueeze(0)  # âœ… æ­£å¸¸å·¥ä½œ
   ```

### ä¸ºä»€ä¹ˆ PaConvert æ— æ³•è‡ªåŠ¨å¤„ç†

1. **API è®¾è®¡èŒƒå¼å®Œå…¨ä¸åŒ** (å‡½æ•° vs ç±»)
2. **éœ€è¦ç†è§£å»¶è¿Ÿè®¡ç®—æœºåˆ¶**
3. **éœ€è¦æ’å…¥åˆ‡ç‰‡æ“ä½œ** `[:]`
4. **éœ€è¦æ·»åŠ ** `stop_gradient = False`
5. **è¶…å‡ºç®€å• API æ˜ å°„èŒƒå›´**

### é€šç”¨ä¿®å¤æ¨¡å¼

```python
# é€‚ç”¨äºæ‰€æœ‰ PaddlePaddle Hessian è®¡ç®—
# Step 1: è®¾ç½®æ¢¯åº¦æ ‡å¿—ï¼ˆå¿…é¡»ï¼‰
xx.stop_gradient = False

# Step 2: åˆ›å»º Hessian å¯¹è±¡
h_obj = paddle.incubate.autograd.Hessian(
    func=model, xs=xx, is_batched=False
)

# Step 3: ä½¿ç”¨åˆ‡ç‰‡æå–å¼ é‡
h_matrix = h_obj[:]  # è¿”å› paddle.Tensor

# Step 4: è°ƒç”¨å¼ é‡æ–¹æ³•
h_matrix = h_matrix.detach().cpu()
```

### æµ‹è¯•éªŒè¯

**åˆ›å»ºæµ‹è¯•è„šæœ¬** (`test_hessian_fix.py`):
```python
import paddle

class SimpleModel(paddle.nn.Layer):
    def forward(self, x):
        return paddle.sum(x**2)

model = SimpleModel()
x = paddle.to_tensor([1.0, 2.0])
x.stop_gradient = False  # â† å¿…é¡»è®¾ç½®

# åˆ›å»º Hessian å¯¹è±¡
h_obj = paddle.incubate.autograd.Hessian(func=model, xs=x, is_batched=False)

# ä½¿ç”¨åˆ‡ç‰‡æå–å¼ é‡
h_matrix = h_obj[:]

# éªŒè¯å¯ä»¥è°ƒç”¨å¼ é‡æ–¹æ³•
h_matrix.detach()  # âœ… æ­£å¸¸å·¥ä½œ
h_matrix.cpu()     # âœ… æ­£å¸¸å·¥ä½œ

# éªŒè¯æ•°å€¼æ­£ç¡®æ€§
# ç†è®º Hessian: [[2, 0], [0, 2]]
print(h_matrix.numpy())  # [[2. 0.] [0. 2.]]
```

**æµ‹è¯•ç»“æœ**:
```
============================================================
æµ‹è¯• Hessian æå–ä¿®å¤
============================================================

1. Hessian å¯¹è±¡ç±»å‹: <class 'paddle.incubate.autograd.functional.Hessian'>
   æ˜¯å¦æ˜¯ Tensor: False

2. æå–åå¼ é‡ç±»å‹: <class 'paddle.Tensor'>
   æ˜¯å¦æ˜¯ Tensor: True
   å½¢çŠ¶: paddle.Size([2, 2])

3. âœ… å¼ é‡æ–¹æ³•æµ‹è¯•é€šè¿‡
   .detach() æˆåŠŸ
   .cpu() æˆåŠŸ

4. æ•°å€¼éªŒè¯:
   è®¡ç®—ç»“æœ:
[[2. 0.]
 [0. 2.]]
   ç†è®ºå€¼:
[[2. 0.]
 [0. 2.]]
   æœ€å¤§è¯¯å·®: 0.00e+00
   âœ… æ•°å€¼æ­£ç¡®

============================================================
âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼
============================================================
```

### ä¿®å¤æ•ˆæœ

- âœ… ä¸å†æŠ›å‡º `AttributeError: 'Hessian' object has no attribute 'detach'`
- âœ… Oracle è®­ç»ƒæ­£å¸¸å®Œæˆ
- âœ… Hessian è®¡ç®—æ­£å¸¸
- âœ… åˆ†æ²»ç­–ç•¥å®Œæ•´è¿è¡Œ
- âœ… æ•°å€¼ç²¾åº¦æ­£ç¡® (è¯¯å·® < 1e-10)

### æœ€ä½³å®è·µ

1. **å§‹ç»ˆè®¾ç½®æ¢¯åº¦æ ‡å¿—**:
   ```python
   xx.stop_gradient = False  # è®¡ç®—é«˜é˜¶å¯¼æ•°å‰å¿…é¡»è®¾ç½®
   ```

2. **ä½¿ç”¨åˆ‡ç‰‡æå–å¼ é‡**:
   ```python
   h_matrix = h_obj[:]  # å®Œæ•´çŸ©é˜µ
   h_sub = h_obj[0:2, 0:2]  # å­çŸ©é˜µï¼ˆæŒ‰éœ€è®¡ç®—ï¼‰
   ```

3. **ç†è§£è®¾è®¡å·®å¼‚**:
   - PyTorch: å‡½æ•°å¼ APIï¼Œç®€æ´ç›´è§‚
   - PaddlePaddle: é¢å‘å¯¹è±¡ APIï¼Œæ”¯æŒå»¶è¿Ÿè®¡ç®—

### ç›¸å…³é—®é¢˜

- æ— ï¼ˆç‹¬ç«‹çš„ API è®¾è®¡å·®å¼‚ï¼‰

### å‚è€ƒé“¾æ¥

- [PaddlePaddle Hessian æ–‡æ¡£](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/incubate/autograd/Hessian_cn.html)
- [PyTorch Hessian æ–‡æ¡£](https://pytorch.org/docs/stable/generated/torch.autograd.functional.hessian.html)

---

**æœ€åæ›´æ–°**: 2026-01-29
**ä¿®å¤çŠ¶æ€**: âœ… å·²å®Œæˆ
