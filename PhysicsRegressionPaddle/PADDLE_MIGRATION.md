# PaddlePaddle è¿ç§»æŒ‡å—

> **é¡¹ç›®**: PhysicsRegression PyTorch â†’ PaddlePaddle æ¡†æ¶è¿ç§»
> **è¿ç§»å·¥å…·**: PaConvert (ç™¾åº¦è‡ªåŠ¨è½¬æ¢å·¥å…·)
> **è¿ç§»æ—¥æœŸ**: 2026å¹´
> **æ–‡æ¡£ç‰ˆæœ¬**: 1.0

---

## ğŸ“‹ ç›®å½•

- [è¿ç§»æ¦‚è§ˆ](#è¿ç§»æ¦‚è§ˆ)
- [æ ¸å¿ƒAPIå˜åŒ–](#æ ¸å¿ƒapiå˜åŒ–)
- [paddle_utils.py å…¼å®¹å±‚](#paddle_utilspy-å…¼å®¹å±‚)
- [å…³é”®ä»£ç å¯¹æ¯”](#å…³é”®ä»£ç å¯¹æ¯”)
- [è®¾å¤‡ç®¡ç†å˜åŒ–](#è®¾å¤‡ç®¡ç†å˜åŒ–)
- [æ¨¡å‹æ–‡ä»¶æ ¼å¼](#æ¨¡å‹æ–‡ä»¶æ ¼å¼)
- [ç‰¹æ®Šå¤„ç†è¯´æ˜](#ç‰¹æ®Šå¤„ç†è¯´æ˜)
- [è¿ç§»æ£€æŸ¥æ¸…å•](#è¿ç§»æ£€æŸ¥æ¸…å•)
- [å·²çŸ¥é—®é¢˜](#å·²çŸ¥é—®é¢˜)

---

## è¿ç§»æ¦‚è§ˆ

### è¿ç§»æ–¹æ³•

æœ¬é¡¹ç›®ä½¿ç”¨ **PaConvert** (ç™¾åº¦å®˜æ–¹å·¥å…·) è¿›è¡Œè‡ªåŠ¨ä»£ç è½¬æ¢:

```bash
# è¿ç§»å‘½ä»¤ (å·²å®Œæˆ)
paconvert --in_dir ./PhysicsRegression --out_dir ./PhysicsRegressionPaddle
```

### è¿ç§»çŠ¶æ€

| ç»„ä»¶ | è¿ç§»çŠ¶æ€ | è‡ªåŠ¨è½¬æ¢ç‡ | å¤‡æ³¨ |
|------|---------|-----------|------|
| **ç¬¦å·å›å½’æ¨¡å—** | âœ… å®Œæˆ | ~95% | Transformer, Embedders, Environment |
| **Oracleæ¨¡å—** | âœ… å®Œæˆ | ~98% | SimpleNetç½‘ç»œ, Oracleè®­ç»ƒ |
| **è®­ç»ƒè„šæœ¬** | âœ… å®Œæˆ | ~90% | train.py, trainer.py |
| **è¯„ä¼°è„šæœ¬** | âœ… å®Œæˆ | ~90% | evaluate.py |
| **å·¥å…·å‡½æ•°** | âœ… å®Œæˆ | ~95% | utils.py, metrics.py |
| **å…¼å®¹å±‚** | âœ… è‡ªåŠ¨ç”Ÿæˆ | 100% | paddle_utils.py |

### æ–‡ä»¶ç»“æ„å¯¹æ¯”

```
PhysicsRegression/              PhysicsRegressionPaddle/
â”œâ”€â”€ *.py (PyTorchä»£ç )         â”œâ”€â”€ *.py (PaddlePaddleä»£ç )
â”œâ”€â”€ symbolicregression/         â”œâ”€â”€ symbolicregression/
â”œâ”€â”€ Oracle/                     â”œâ”€â”€ Oracle/
â”œâ”€â”€ physical/                   â”œâ”€â”€ physical/
â”œâ”€â”€ model.pt (PyTorchæ¨¡å‹)     â”œâ”€â”€ model.pdparams (éœ€è½¬æ¢)
â””â”€â”€ CLAUDE.md                   â”œâ”€â”€ paddle_utils.py (æ–°å¢å…¼å®¹å±‚)
                                â””â”€â”€ CLAUDE.md (éœ€æ›´æ–°)
```

---

## æ ¸å¿ƒAPIå˜åŒ–

### æ¨¡å—å¯¼å…¥å˜åŒ–

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

# âœ… PaddlePaddle
import paddle
import paddle.nn as nn
from paddle.io import DataLoader
from paddle_utils import *  # å¯¼å…¥å…¼å®¹å±‚
```

### ç¥ç»ç½‘ç»œæ¨¡å—

#### åŸºç¡€ç±»ç»§æ‰¿

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
class MyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()

# âœ… PaddlePaddle
class MyModel(paddle.nn.Module):  # æˆ– paddle.nn.Layer
    def __init__(self):
        super().__init__()
```

#### çº¿æ€§å±‚

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
self.fc = torch.nn.Linear(128, 64)

# âœ… PaddlePaddle (å…¼å®¹å‘½åç©ºé—´)
self.fc = paddle.compat.nn.Linear(128, 64)

# âš ï¸ æ³¨æ„: ä½¿ç”¨ paddle.compat.nn.Linear è€Œé paddle.nn.Linear
# è¿™æ˜¯PaConvertå·¥å…·çš„å¤„ç†æ–¹å¼,ç¡®ä¿APIå…¼å®¹æ€§
```

#### æ¿€æ´»å‡½æ•°

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
x = torch.tanh(x)
x = torch.nn.functional.relu(x)

# âœ… PaddlePaddle
x = paddle.tanh(x)
x = paddle.nn.functional.relu(x)
```

#### åµŒå…¥å±‚

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
self.embed = torch.nn.Embedding(vocab_size, emb_dim, padding_idx=0)

# âœ… PaddlePaddle
self.embed = paddle.nn.Embedding(vocab_size, emb_dim, padding_idx=0)
```

#### å®¹å™¨æ¨¡å—

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
self.layers = torch.nn.ModuleList([...])

# âœ… PaddlePaddle
self.layers = paddle.nn.ModuleList([...])
```

### å¼ é‡æ“ä½œ

#### å¼ é‡åˆ›å»º

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
x = torch.tensor([1, 2, 3])
x = torch.zeros(3, 4)
x = torch.FloatTensor([1.0, 2.0])

# âœ… PaddlePaddle
x = paddle.to_tensor([1, 2, 3])  # æ³¨æ„: paddle.tensorä¹Ÿå¯ç”¨
x = paddle.zeros([3, 4])
x = paddle.FloatTensor([1.0, 2.0])
```

#### æ•°æ®ç±»å‹

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
x = x.long()
x = x.float()

# âœ… PaddlePaddle
x = x.astype(paddle.long)  # æˆ– paddle.int64
x = x.astype(paddle.float32)
```

#### å¼ é‡æ–¹æ³•å·®å¼‚

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch: ä½¿ç”¨ dim å‚æ•°
x.max(dim=1)
x.sum(dim=0)

# âœ… PaddlePaddle: ä½¿ç”¨ axis å‚æ•°
x.max(axis=1)  # æˆ–ä½¿ç”¨ paddle_utils ä¸­çš„å…¼å®¹æ–¹æ³•
x.sum(axis=0)
```

### ä¼˜åŒ–å™¨

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

# âœ… PaddlePaddle
optimizer = paddle.optimizer.Adam(parameters=model.parameters(), learning_rate=0.001)
optimizer = paddle.optimizer.SGD(parameters=model.parameters(), learning_rate=0.01, momentum=0.9)
```

### æŸå¤±å‡½æ•°

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
loss_fn = torch.nn.MSELoss()
loss = torch.nn.functional.cross_entropy(pred, target)

# âœ… PaddlePaddle
loss_fn = paddle.nn.MSELoss()
loss = paddle.nn.functional.cross_entropy(pred, target)
```

### æ•°æ®åŠ è½½

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
from torch.utils.data import Dataset, DataLoader

class MyDataset(Dataset):
    pass

loader = DataLoader(dataset, batch_size=32, shuffle=True)

# âœ… PaddlePaddle
from paddle.io import Dataset, DataLoader

class MyDataset(Dataset):
    pass

loader = DataLoader(dataset, batch_size=32, shuffle=True)
```

---

## paddle_utils.py å…¼å®¹å±‚

PaConvert è‡ªåŠ¨ç”Ÿæˆçš„å…¼å®¹å±‚æ–‡ä»¶,ç”¨äºå¤„ç†PyTorchå’ŒPaddlePaddleçš„APIå·®å¼‚ã€‚

### æ–‡ä»¶ä½ç½®

```
PhysicsRegressionPaddle/
â””â”€â”€ paddle_utils.py  # é¡¹ç›®æ ¹ç›®å½•
```

### æ ¸å¿ƒåŠŸèƒ½

#### 1. è®¾å¤‡å­—ç¬¦ä¸²è½¬æ¢

**åŠŸèƒ½**: å°†PyTorchçš„è®¾å¤‡å­—ç¬¦ä¸²è½¬æ¢ä¸ºPaddlePaddleæ ¼å¼

```python
def device2int(device):
    """
    è½¬æ¢è®¾å¤‡å­—ç¬¦ä¸²æ ¼å¼

    ç¤ºä¾‹:
        'cuda:0' â†’ 'gpu:0' â†’ 0
        'cuda:1' â†’ 'gpu:1' â†’ 1
    """
    if isinstance(device, str):
        print("Converting device string to int:", device)
        device = device.replace('cuda', 'gpu')
        device = device.replace('gpu:', '')
    return int(device)
```

**ä½¿ç”¨åœºæ™¯**:
```python
# PyTorchä»£ç : device = 'cuda:0'
# PaddlePaddleè½¬æ¢: device = device2int('cuda:0')  # è¿”å› 0
```

#### 2. Tensor.max() æ–¹æ³•é€‚é…

**åŠŸèƒ½**: å¤„ç† `dim`/`axis` å‚æ•°å·®å¼‚

```python
def _Tensor_max(self, *args, **kwargs):
    """
    é€‚é… Tensor.max() æ–¹æ³•

    å¤„ç†:
    1. PyTorch: tensor.max(dim=1)
    2. PaddlePaddle: tensor.max(axis=1)
    3. è¿”å›å€¼å·®å¼‚: (values, indices)
    """
    if "other" in kwargs:
        kwargs["y"] = kwargs.pop("other")
        ret = paddle.maximum(self, *args, **kwargs)
    elif len(args) == 1 and isinstance(args[0], paddle.Tensor):
        ret = paddle.maximum(self, *args, **kwargs)
    else:
        if "dim" in kwargs:
            kwargs["axis"] = kwargs.pop("dim")  # â† å…³é”®è½¬æ¢

        if "axis" in kwargs or len(args) >= 1:
            ret = paddle.max(self, *args, **kwargs), paddle.argmax(self, *args, **kwargs)
        else:
            ret = paddle.max(self, *args, **kwargs)

    return ret

# å°†æ–¹æ³•ç»‘å®šåˆ° Tensor ç±»
setattr(paddle.Tensor, "_max", _Tensor_max)
```

**ä½¿ç”¨ç¤ºä¾‹**:
```python
import paddle
from paddle_utils import *

x = paddle.randn([3, 4])

# PyTorché£æ ¼ (é€šè¿‡å…¼å®¹å±‚è‡ªåŠ¨å¤„ç†)
max_val, max_idx = x._max(dim=1)

# ç­‰ä»·äºPaddlePaddleåŸç”Ÿå†™æ³•:
max_val = x.max(axis=1)
max_idx = x.argmax(axis=1)
```

### ä½¿ç”¨æ–¹æ³•

åœ¨æ¯ä¸ªéœ€è¦å…¼å®¹å¤„ç†çš„æ¨¡å—é¡¶éƒ¨æ·»åŠ :

```python
import paddle
from paddle_utils import *
```

**æ³¨æ„äº‹é¡¹**:
- âš ï¸ `paddle_utils.py` å¿…é¡»ä½äºPythonå¯¼å…¥è·¯å¾„ä¸­
- âš ï¸ å¯¼å…¥é¡ºåº: å…ˆ `import paddle`,å† `from paddle_utils import *`
- âš ï¸ æŸäº›é¡¹ç›®æ–‡ä»¶é€šè¿‡ `sys.path.append` æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„

---

## å…³é”®ä»£ç å¯¹æ¯”

### ç¤ºä¾‹ 1: Transformer MultiHeadAttention

**æ–‡ä»¶**: `symbolicregression/model/transformer.py:54-74`

```python
# ===== PyTorchç‰ˆæœ¬ =====
import torch
import torch.nn as nn

class MultiHeadAttention(nn.Module):
    def __init__(self, n_heads, dim, src_dim, dropout, normalized_attention):
        super().__init__()
        self.q_lin = nn.Linear(dim, dim)
        self.k_lin = nn.Linear(src_dim, dim)
        self.v_lin = nn.Linear(src_dim, dim)
        self.out_lin = nn.Linear(dim, dim)
        if self.normalized_attention:
            self.attention_scale = nn.Parameter(
                torch.tensor(1.0 / math.sqrt(dim // n_heads))
            )

# ===== PaddlePaddleç‰ˆæœ¬ =====
import paddle
from paddle_utils import *

class MultiHeadAttention(paddle.nn.Module):
    def __init__(self, n_heads, dim, src_dim, dropout, normalized_attention):
        super().__init__()
        self.q_lin = paddle.compat.nn.Linear(dim, dim)           # â† ä½¿ç”¨ compat
        self.k_lin = paddle.compat.nn.Linear(src_dim, dim)       # â† ä½¿ç”¨ compat
        self.v_lin = paddle.compat.nn.Linear(src_dim, dim)       # â† ä½¿ç”¨ compat
        self.out_lin = paddle.compat.nn.Linear(dim, dim)         # â† ä½¿ç”¨ compat
        if self.normalized_attention:
            self.attention_scale = paddle.nn.Parameter(
                paddle.tensor(1.0 / math.sqrt(dim // n_heads))  # â† paddle.tensor
            )
```

**å˜åŒ–è¦ç‚¹**:
1. å¯¼å…¥: `torch` â†’ `paddle`
2. ç±»ç»§æ‰¿: `nn.Module` â†’ `paddle.nn.Module`
3. çº¿æ€§å±‚: `nn.Linear` â†’ `paddle.compat.nn.Linear`
4. å‚æ•°: `torch.tensor` â†’ `paddle.tensor`

---

### ç¤ºä¾‹ 2: Oracle SimpleNet

**æ–‡ä»¶**: `Oracle/oracle.py:20-35`

```python
# ===== PyTorchç‰ˆæœ¬ =====
import torch
import torch.nn as nn

class SimpleNet(nn.Module):
    def __init__(self, _in):
        super().__init__()
        self.linear1 = nn.Linear(_in, 128)
        self.linear2 = nn.Linear(128, 128)
        self.linear3 = nn.Linear(128, 64)
        self.linear4 = nn.Linear(64, 64)
        self.linear5 = nn.Linear(64, 1)

    def forward(self, x):
        x = torch.tanh(self.linear1(x))
        x = torch.tanh(self.linear2(x))
        x = torch.tanh(self.linear3(x))
        x = torch.tanh(self.linear4(x))
        x = self.linear5(x)
        return x

# ===== PaddlePaddleç‰ˆæœ¬ =====
import paddle

class SimpleNet(paddle.nn.Module):
    def __init__(self, _in):
        super().__init__()
        self.linear1 = paddle.compat.nn.Linear(_in, 128)
        self.linear2 = paddle.compat.nn.Linear(128, 128)
        self.linear3 = paddle.compat.nn.Linear(128, 64)
        self.linear4 = paddle.compat.nn.Linear(64, 64)
        self.linear5 = paddle.compat.nn.Linear(64, 1)

    def forward(self, x):
        x = paddle.tanh(self.linear1(x))      # â† paddle.tanh
        x = paddle.tanh(self.linear2(x))
        x = paddle.tanh(self.linear3(x))
        x = paddle.tanh(self.linear4(x))
        x = self.linear5(x)
        return x
```

---

### ç¤ºä¾‹ 3: LinearPointEmbedder

**æ–‡ä»¶**: `symbolicregression/model/embedders.py:45-73`

```python
# ===== PyTorchç‰ˆæœ¬ =====
import torch
import torch.nn as nn

class LinearPointEmbedder(Embedder):
    def __init__(self, params, env):
        super().__init__()
        self.embeddings = Embedding(
            len(self.env.float_id2word),
            self.input_dim,
            padding_idx=env.float_word2id["<PAD>"],
        )
        self.activation_fn = torch.nn.functional.relu
        self.hidden_layers = nn.ModuleList()
        self.hidden_layers.append(nn.Linear(size, hidden_size))
        for i in range(self.params.n_emb_layers - 1):
            self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))
        self.fc = nn.Linear(hidden_size, self.output_dim)

# ===== PaddlePaddleç‰ˆæœ¬ =====
import paddle
from paddle_utils import *

class LinearPointEmbedder(Embedder):
    def __init__(self, params, env):
        super().__init__()
        self.embeddings = Embedding(
            len(self.env.float_id2word),
            self.input_dim,
            padding_idx=env.float_word2id["<PAD>"],
        )
        self.activation_fn = paddle.nn.functional.relu  # â† paddle
        self.hidden_layers = paddle.nn.ModuleList()     # â† paddle
        self.hidden_layers.append(paddle.compat.nn.Linear(size, hidden_size))
        for i in range(self.params.n_emb_layers - 1):
            self.hidden_layers.append(paddle.compat.nn.Linear(hidden_size, hidden_size))
        self.fc = paddle.compat.nn.Linear(hidden_size, self.output_dim)
```

---

### ç¤ºä¾‹ 4: è®­ç»ƒå¾ªç¯

```python
# ===== PyTorchç‰ˆæœ¬ =====
import torch

def train_step(model, optimizer, x, y):
    model.train()

    # å‰å‘ä¼ æ’­
    pred = model(x)
    loss = torch.nn.functional.mse_loss(pred, y)

    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss.item()

# ===== PaddlePaddleç‰ˆæœ¬ =====
import paddle

def train_step(model, optimizer, x, y):
    model.train()

    # å‰å‘ä¼ æ’­
    pred = model(x)
    loss = paddle.nn.functional.mse_loss(pred, y)

    # åå‘ä¼ æ’­
    optimizer.clear_grad()  # â† clear_grad è€Œé zero_grad
    loss.backward()
    optimizer.step()

    return loss.item()
```

**å…³é”®å·®å¼‚**:
- `optimizer.zero_grad()` â†’ `optimizer.clear_grad()`

---

## è®¾å¤‡ç®¡ç†å˜åŒ–

### è®¾å¤‡å­—ç¬¦ä¸²æ ¼å¼

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
device = 'cuda:0'
device = 'cuda:1'
device = 'cpu'

# âœ… PaddlePaddle
device = 'gpu:0'   # æˆ–ä½¿ç”¨ device2int() è½¬æ¢ä¸ºæ•´æ•° 0
device = 'gpu:1'   # æˆ–æ•´æ•° 1
device = 'cpu'
```

### æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
model = model.to('cuda:0')
x = x.to('cuda:0')

# âœ… PaddlePaddle (æ–¹å¼1: å­—ç¬¦ä¸²)
model = model.to('gpu:0')
x = x.to('gpu:0')

# âœ… PaddlePaddle (æ–¹å¼2: è®¾å¤‡å¯¹è±¡)
device = paddle.CUDAPlace(0)
model = model.to(device)
x = x.to(device)
```

### æ£€æŸ¥GPUå¯ç”¨æ€§

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
if torch.cuda.is_available():
    device = 'cuda:0'
else:
    device = 'cpu'

# âœ… PaddlePaddle
if paddle.is_compiled_with_cuda():
    device = 'gpu:0'
else:
    device = 'cpu'
```

---

## æ¨¡å‹æ–‡ä»¶æ ¼å¼

### æ¨¡å‹ä¿å­˜

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch: ä¿å­˜ä¸º .pt æˆ– .pth
torch.save(model.state_dict(), 'model.pt')

# âœ… PaddlePaddle: ä¿å­˜ä¸º .pdparams
paddle.save(model.state_dict(), 'model.pdparams')
```

### æ¨¡å‹åŠ è½½

```python
# PyTorch â†’ PaddlePaddle

# âŒ PyTorch
state_dict = torch.load('model.pt', map_location='cpu')
model.load_state_dict(state_dict)

# âœ… PaddlePaddle
state_dict = paddle.load('model.pdparams')
model.set_state_dict(state_dict)
```

### æ¨¡å‹è½¬æ¢

**ä»PyTorchè¿ç§»åˆ°PaddlePaddleæ—¶,æ¨¡å‹æ–‡ä»¶éœ€è¦é‡æ–°è®­ç»ƒæˆ–ä½¿ç”¨è½¬æ¢å·¥å…·**:

```python
# æ–¹æ³•1: é‡æ–°è®­ç»ƒ (æ¨è)
# ä½¿ç”¨ train.py é‡æ–°è®­ç»ƒæ¨¡å‹

# æ–¹æ³•2: æ‰‹åŠ¨è½¬æ¢æƒé‡ (å¤æ‚,ä»…å½“å¿…è¦æ—¶)
# éœ€è¦ç¼–å†™è‡ªå®šä¹‰è½¬æ¢è„šæœ¬åŒ¹é…ç½‘ç»œç»“æ„
```

**æ³¨æ„**: ç”±äºæ¶æ„å·®å¼‚,ç›´æ¥è½¬æ¢`.pt`åˆ°`.pdparams`å¯èƒ½ä¸å¯è¡Œ,å»ºè®®é‡æ–°è®­ç»ƒã€‚

---

## ç‰¹æ®Šå¤„ç†è¯´æ˜

### paddle.compat.nn.Linear

**ä¸ºä»€ä¹ˆä½¿ç”¨ `paddle.compat.nn.Linear`?**

PaConvertå·¥å…·ä½¿ç”¨ `paddle.compat.nn.Linear` ç¡®ä¿APIå…¼å®¹æ€§:

```python
# PyTorchåŸå§‹ä»£ç 
fc = torch.nn.Linear(128, 64)

# PaConvertè½¬æ¢å
fc = paddle.compat.nn.Linear(128, 64)

# è€Œéç›´æ¥ä½¿ç”¨
fc = paddle.nn.Linear(128, 64)  # â† å¯èƒ½å­˜åœ¨ç»†å¾®å·®å¼‚
```

**å…¼å®¹å‘½åç©ºé—´ä½ç½®**:
- æ–‡ä»¶: `symbolicregression/model/transformer.py`
- æ–‡ä»¶: `symbolicregression/model/embedders.py`
- æ–‡ä»¶: `Oracle/oracle.py`

**æ˜¯å¦å¯ä»¥æ”¹ä¸º `paddle.nn.Linear`?**

ç†è®ºä¸Šå¯ä»¥,ä½†éœ€è¦éªŒè¯ä»¥ä¸‹å†…å®¹:
1. æƒé‡åˆå§‹åŒ–æ–¹æ³•æ˜¯å¦ä¸€è‡´
2. biaså¤„ç†æ˜¯å¦ç›¸åŒ
3. å‰å‘ä¼ æ’­æ•°å€¼ç²¾åº¦

### sys.path.append

å¤šä¸ªæ–‡ä»¶åŒ…å«ä»¥ä¸‹ä»£ç ä»¥ç¡®ä¿å¯¼å…¥è·¯å¾„æ­£ç¡®:

```python
import sys
sys.path.append("/home/lkyu/baidu/PhysicsRegressionPaddle")
```

**ä½ç½®**:
- `symbolicregression/model/transformer.py:1-2`
- `symbolicregression/model/embedders.py:1-2`

**ä½œç”¨**: ç¡®ä¿ `paddle_utils.py` å¯ä»¥è¢«æ­£ç¡®å¯¼å…¥

**æ³¨æ„**: å¦‚æœé¡¹ç›®è·¯å¾„æ”¹å˜,éœ€è¦æ›´æ–°è¿™äº›è·¯å¾„

---

## è¿ç§»æ£€æŸ¥æ¸…å•

### ä»£ç å±‚é¢

- [x] æ‰€æœ‰ `import torch` å·²æ›¿æ¢ä¸º `import paddle`
- [x] æ‰€æœ‰ `torch.nn.Module` å·²æ›¿æ¢ä¸º `paddle.nn.Module`
- [x] æ‰€æœ‰ `torch.nn.Linear` å·²æ›¿æ¢ä¸º `paddle.compat.nn.Linear`
- [x] æ‰€æœ‰ `torch.optim.Adam` å·²æ›¿æ¢ä¸º `paddle.optimizer.Adam`
- [x] æ‰€æœ‰æ¿€æ´»å‡½æ•°å·²æ›´æ–° (`torch.tanh` â†’ `paddle.tanh`)
- [x] ä¼˜åŒ–å™¨è°ƒç”¨å·²æ›´æ–° (`zero_grad()` â†’ `clear_grad()`)
- [x] è®¾å¤‡å­—ç¬¦ä¸²å·²æ›´æ–° (`cuda:0` â†’ `gpu:0`)
- [x] å¼ é‡æ–¹æ³•å·²æ›´æ–° (`dim` â†’ `axis`)
- [x] `paddle_utils.py` å·²æ­£ç¡®å¯¼å…¥

### åŠŸèƒ½éªŒè¯

- [ ] **æµ‹è¯•è®­ç»ƒæµç¨‹**: è¿è¡Œ `train.py` ç¡®è®¤æ— é”™è¯¯
- [ ] **æµ‹è¯•è¯„ä¼°æµç¨‹**: è¿è¡Œ `evaluate.py` éªŒè¯æ¨¡å‹æ¨ç†
- [ ] **æµ‹è¯•Oracleæ¨¡å—**: éªŒè¯åˆ†æ²»ç­–ç•¥æ­£å¸¸å·¥ä½œ
- [ ] **æµ‹è¯•MCTS/GP**: ç¡®è®¤ä¼˜åŒ–ç®—æ³•å¯ç”¨
- [ ] **å¯¹æ¯”æ•°å€¼ç²¾åº¦**: PyTorch vs PaddlePaddle è¾“å‡ºå·®å¼‚ < 1e-5
- [ ] **GPUå†…å­˜æµ‹è¯•**: ç¡®è®¤æ˜¾å­˜ä½¿ç”¨åˆç†
- [ ] **å¤šå¡è®­ç»ƒ**: æµ‹è¯•åˆ†å¸ƒå¼è®­ç»ƒåŠŸèƒ½

### æ–‡æ¡£æ›´æ–°

- [ ] æ›´æ–°æ ¹ç›®å½• `CLAUDE.md`
- [ ] æ›´æ–° `symbolicregression/CLAUDE.md`
- [ ] æ›´æ–° `Oracle/CLAUDE.md`
- [ ] æ›´æ–° `physical/CLAUDE.md`
- [x] åˆ›å»º `PADDLE_MIGRATION.md` (æœ¬æ–‡æ¡£)

### ç¯å¢ƒé…ç½®

- [ ] åˆ›å»ºPaddlePaddleç‰ˆæœ¬çš„ `environment.yml`
- [ ] æ›´æ–° `README.md` å®‰è£…è¯´æ˜
- [ ] å‡†å¤‡PaddlePaddleç‰ˆæœ¬çš„é¢„è®­ç»ƒæ¨¡å‹

---

## å·²çŸ¥é—®é¢˜

### é—®é¢˜ 1: æ¨¡å‹æ ¼å¼ä¸å…¼å®¹

**æè¿°**: PyTorchçš„ `.pt` æ¨¡å‹æ–‡ä»¶æ— æ³•ç›´æ¥ç”¨äºPaddlePaddle

**è§£å†³æ–¹æ¡ˆ**:
1. ä½¿ç”¨ç›¸åŒæ•°æ®é‡æ–°è®­ç»ƒæ¨¡å‹
2. æˆ–ç¼–å†™è‡ªå®šä¹‰è½¬æ¢è„šæœ¬ (éœ€è¦æ·±å…¥ç†è§£ç½‘ç»œç»“æ„)

### é—®é¢˜ 2: paddle.compat å‘½åç©ºé—´

**æè¿°**: ä»£ç ä¸­ä½¿ç”¨ `paddle.compat.nn.Linear` å¯èƒ½è®©äººå›°æƒ‘

**è¯´æ˜**:
- è¿™æ˜¯PaConvertå·¥å…·çš„æ ‡å‡†åšæ³•
- ç¡®ä¿APIå…¼å®¹æ€§
- ä¸å½±å“åŠŸèƒ½

### é—®é¢˜ 3: æ•°å€¼ç²¾åº¦å·®å¼‚

**æè¿°**: PaddlePaddleå’ŒPyTorchåœ¨æŸäº›æ“ä½œä¸Šå¯èƒ½æœ‰ç»†å¾®æ•°å€¼å·®å¼‚

**éªŒè¯æ–¹æ³•**:
```python
import paddle
import torch
import numpy as np

# ç›¸åŒè¾“å…¥
x_np = np.random.randn(4, 128).astype('float32')

# PyTorch
x_torch = torch.from_numpy(x_np)
out_torch = torch_model(x_torch).detach().numpy()

# PaddlePaddle
x_paddle = paddle.to_tensor(x_np)
out_paddle = paddle_model(x_paddle).numpy()

# å¯¹æ¯”
diff = np.abs(out_torch - out_paddle).max()
print(f"æœ€å¤§å·®å¼‚: {diff}")  # åº”è¯¥ < 1e-5
```

### é—®é¢˜ 4: ç¡¬ç¼–ç è·¯å¾„

**æè¿°**: éƒ¨åˆ†æ–‡ä»¶åŒ…å«ç¡¬ç¼–ç çš„ç»å¯¹è·¯å¾„

**ä½ç½®**:
```python
sys.path.append("/home/lkyu/baidu/PhysicsRegressionPaddle")
```

**è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨ç›¸å¯¹è·¯å¾„æˆ–ç¯å¢ƒå˜é‡

---

## å‚è€ƒèµ„æº

### PaddlePaddle å®˜æ–¹æ–‡æ¡£

- **APIæ˜ å°„è¡¨**: https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/model_convert/pytorch_api_mapping_cn.html
- **è¿ç§»æŒ‡å—**: https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/model_convert/convert_from_pytorch/pytorch_migration_cn.html
- **PaConvertå·¥å…·**: https://github.com/PaddlePaddle/PaConvert

### é¡¹ç›®ç›¸å…³

- **åŸé¡¹ç›®è®ºæ–‡**: Ying et al., Nature Machine Intelligence (2025)
- **GitHub**: PhysicsRegression (åŸPyTorchç‰ˆæœ¬)
- **Google Drive**: é¢„è®­ç»ƒæ¨¡å‹å’Œæ•°æ®é›†

---

**æœ€åæ›´æ–°**: 2026-01-28
**ç»´æŠ¤è€…**: è¿ç§»é¡¹ç›®å›¢é˜Ÿ
**é—®é¢˜åé¦ˆ**: è¯·åœ¨é¡¹ç›®Issueä¸­æŠ¥å‘Šè¿ç§»ç›¸å…³é—®é¢˜

---

## é™„å½•: å®Œæ•´APIå¯¹ç…§è¡¨

| åŠŸèƒ½ç±»åˆ« | PyTorch | PaddlePaddle | å¤‡æ³¨ |
|---------|---------|--------------|------|
| **æ¨¡å—å¯¼å…¥** | `import torch` | `import paddle` | |
| **ç¥ç»ç½‘ç»œåŸºç±»** | `torch.nn.Module` | `paddle.nn.Module` | æˆ– `paddle.nn.Layer` |
| **çº¿æ€§å±‚** | `torch.nn.Linear` | `paddle.compat.nn.Linear` | âš ï¸ ä½¿ç”¨compat |
| **åµŒå…¥å±‚** | `torch.nn.Embedding` | `paddle.nn.Embedding` | |
| **æ¿€æ´»å‡½æ•°** | `torch.tanh` | `paddle.tanh` | |
| | `torch.nn.functional.relu` | `paddle.nn.functional.relu` | |
| **å‚æ•°** | `torch.nn.Parameter` | `paddle.nn.Parameter` | |
| **å®¹å™¨** | `torch.nn.ModuleList` | `paddle.nn.ModuleList` | |
| **å¼ é‡åˆ›å»º** | `torch.tensor` | `paddle.to_tensor` | æ¨èç”¨æ³• |
| | `torch.zeros` | `paddle.zeros` | |
| | `torch.FloatTensor` | `paddle.FloatTensor` | |
| **æ•°æ®ç±»å‹** | `.long()` | `.astype(paddle.int64)` | |
| **å¼ é‡æ“ä½œ** | `.max(dim=1)` | `.max(axis=1)` | âš ï¸ dimâ†’axis |
| **ä¼˜åŒ–å™¨** | `torch.optim.Adam` | `paddle.optimizer.Adam` | å‚æ•°åä¸åŒ |
| | `.zero_grad()` | `.clear_grad()` | âš ï¸ æ–¹æ³•åä¸åŒ |
| **æŸå¤±å‡½æ•°** | `torch.nn.functional.mse_loss` | `paddle.nn.functional.mse_loss` | |
| **æ•°æ®åŠ è½½** | `torch.utils.data.DataLoader` | `paddle.io.DataLoader` | |
| **è®¾å¤‡ç®¡ç†** | `cuda:0` | `gpu:0` | âš ï¸ å­—ç¬¦ä¸²æ ¼å¼ |
| | `torch.cuda.is_available()` | `paddle.is_compiled_with_cuda()` | |
| **æ¨¡å‹ä¿å­˜** | `torch.save` | `paddle.save` | |
| **æ¨¡å‹åŠ è½½** | `torch.load` | `paddle.load` | |
| | `.load_state_dict` | `.set_state_dict` | âš ï¸ æ–¹æ³•åä¸åŒ |
