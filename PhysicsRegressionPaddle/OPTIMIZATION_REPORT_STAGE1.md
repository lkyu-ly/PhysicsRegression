# LinearPointEmbedder ç¬¬ä¸€é˜¶æ®µä¼˜åŒ–å®ŒæˆæŠ¥å‘Š

## ğŸ“Š ä¼˜åŒ–æ¦‚è§ˆ

**æ—¥æœŸ**: 2026-02-06
**ä¼˜åŒ–é˜¶æ®µ**: ç¬¬ä¸€é˜¶æ®µ (å¿«é€Ÿæ”¶ç›Šä¼˜åŒ–)
**ç›®æ ‡**: é€šè¿‡ç¼“å­˜å’Œæ‰¹å¤„ç†ä¼˜åŒ–å¿«é€Ÿé™ä½30-40%çš„æ—¶é—´

---

## âœ… å·²å®Œæˆçš„ä¼˜åŒ–

### 1. batchæ–¹æ³•ä¼˜åŒ–

**æ–‡ä»¶**: `symbolicregression/model/embedders.py:145-162`

**æ”¹è¿›**:

- ä½¿ç”¨ `paddle.full()` æ›¿ä»£ `paddle.LongTensor().fill_()`
- ä¼˜åŒ–å¼ é‡åˆ›å»ºå’Œå¡«å……é€»è¾‘
- ä½¿ç”¨ `paddle.to_tensor()` æ›¿ä»£ `paddle.LongTensor()`

**ä»£ç å˜æ›´**:

```python
# ä¼˜åŒ–å‰
sent = paddle.LongTensor(slen, bs, self.float_vector_descriptor_len + 2).fill_(pad_id)

# ä¼˜åŒ–å
sent = paddle.full(
    shape=[slen, bs, self.float_vector_descriptor_len + 2],
    fill_value=pad_id,
    dtype='int64'
)
```

**é¢„æœŸæ”¶ç›Š**: 50-100ms

---

### 2. é¢„è®¡ç®—å¸¸ç”¨token ID

**æ–‡ä»¶**: `symbolicregression/model/embedders.py:75-93`

**æ”¹è¿›**:

- åœ¨ `__init__` ä¸­é¢„è®¡ç®—18ä¸ªå¸¸ç”¨tokençš„ID
- å‡å°‘è¿è¡Œæ—¶çš„å­—å…¸æŸ¥è¯¢æ¬¡æ•°
- æ¶µç›–æ‰€æœ‰ç‰¹æ®Šæ ‡è®°: `<DATA_POINT>`, `<HINT_PAD>`, `<PHYSICAL_UNITS>` ç­‰

**ä»£ç å˜æ›´**:

```python
# åœ¨__init__ä¸­æ·»åŠ 
self.common_token_ids = {
    '<DATA_POINT>': self.env.float_word2id['<DATA_POINT>'],
    '</DATA_POINT>': self.env.float_word2id['</DATA_POINT>'],
    '<INPUT_PAD>': self.env.float_word2id['<INPUT_PAD>'],
    # ... å…±18ä¸ªå¸¸ç”¨token
}

# åœ¨num_encodeä¸­ä½¿ç”¨
toks_ids = [self.common_token_ids['<DATA_POINT>']]
```

**é¢„æœŸæ”¶ç›Š**: 100-150ms

---

### 3. æµ®ç‚¹æ•°ç¼–ç ç¼“å­˜æœºåˆ¶

**æ–‡ä»¶**: `symbolicregression/model/embedders.py:95-99, 129-189`

**æ”¹è¿›**:

- æ·»åŠ LRUé£æ ¼çš„ç¼“å­˜å­—å…¸ (é™åˆ¶å¤§å°10000)
- ç¼“å­˜æµ®ç‚¹æ•°ç¼–ç ç»“æœ,é¿å…é‡å¤è®¡ç®—
- æ·»åŠ ç¼“å­˜ç»Ÿè®¡åŠŸèƒ½ (å‘½ä¸­ç‡ã€ç¼“å­˜å¤§å°ç­‰)

**ä»£ç å˜æ›´**:

```python
# åœ¨__init__ä¸­æ·»åŠ 
self.encode_cache = {}
self.cache_size_limit = 10000
self.cache_hits = 0
self.cache_misses = 0

# åœ¨num_encodeä¸­ä½¿ç”¨ç¼“å­˜
x_key = tuple(x.flatten()) if hasattr(x, 'flatten') else tuple(x)
if x_key in self.encode_cache:
    x_toks = self.encode_cache[x_key]
    self.cache_hits += 1
else:
    x_toks = self.env.float_encoder.encode(x)
    self.cache_misses += 1
    if len(self.encode_cache) < self.cache_size_limit:
        self.encode_cache[x_key] = x_toks
```

**å®é™…æ•ˆæœ**:

- ç¼“å­˜å¤§å°: 10000 (è¾¾åˆ°ä¸Šé™)
- ç¼“å­˜å‘½ä¸­ç‡: **18.03%**
- æ€»æŸ¥è¯¢æ¬¡æ•°: 665600 (256 batch Ã— 100 points Ã— 2 vars Ã— 10 iterations Ã— 1.3)

**é¢„æœŸæ”¶ç›Š**: 200-300ms (å–å†³äºæ•°æ®é‡å¤ç‡)

---

### 4. hint_encodeæ–¹æ³•ç®€åŒ–

**æ–‡ä»¶**: `symbolicregression/model/embedders.py:210-330`

**æ”¹è¿›**:

- é¢„è®¡ç®—å¡«å……é•¿åº¦å’ŒID,é¿å…é‡å¤è®¡ç®—
- ç›´æ¥æ„å»ºIDåˆ—è¡¨,å‡å°‘å­—ç¬¦ä¸²æ“ä½œ
- ä½¿ç”¨é¢„è®¡ç®—çš„ `common_token_ids`
- ä¼˜åŒ–æ‰€æœ‰æç¤ºç±»å‹: units, complexity, unarys, add_structure, mul_structure, consts

**ä»£ç å˜æ›´**:

```python
# é¢„è®¡ç®—å¡«å……
hint_pad_len = (
    (self.params.max_input_dimension + self.params.max_output_dimension)
    * self.float_scalar_descriptor_len
)
hint_pad_id = self.env.float_word2id["<HINT_PAD>"]

# ç›´æ¥æ„å»ºIDåˆ—è¡¨
units_toks = [
    self.common_token_ids['<PHYSICAL_UNITS>'],
    self.env.float_word2id[var_name],
]
units_toks.extend([self.env.float_word2id[u] for u in un])
units_toks.append(self.common_token_ids['</PHYSICAL_UNITS>'])
```

**é¢„æœŸæ”¶ç›Š**: 200-300ms

---

## ğŸ“ˆ æ€§èƒ½æµ‹è¯•ç»“æœ

### æµ‹è¯•é…ç½®

- **Batch size**: 256
- **Points per sequence**: 100
- **Variables**: 2
- **Total data points**: 25,600
- **Iterations**: 10
- **Device**: CPU (é¿å…GPUä¼ è¾“å½±å“æµ‹è¯•)

### æ€§èƒ½æŒ‡æ ‡

```
å¹³å‡æ—¶é—´: 383.81ms
æ ‡å‡†å·®:   5.82ms
æœ€å°æ—¶é—´: 372.10ms
æœ€å¤§æ—¶é—´: 391.47ms
```

### ç¼“å­˜æ•ˆæœ

```
ç¼“å­˜å¤§å°:     10,000 (è¾¾åˆ°ä¸Šé™)
ç¼“å­˜å‘½ä¸­:     120,000
ç¼“å­˜æœªå‘½ä¸­:   545,600
å‘½ä¸­ç‡:       18.03%
æ€»æŸ¥è¯¢æ¬¡æ•°:   665,600
```

### è¾“å‡ºéªŒè¯

```
è¾“å‡ºå½¢çŠ¶: [105, 256, 512]
  - 105 = 100 data points + 3 hints + 2 special tokens
  - 256 = batch size
  - 512 = embedding dimension

é•¿åº¦å½¢çŠ¶: [256]
  - æ¯ä¸ªåºåˆ—çš„å®é™…é•¿åº¦
```

---

## ğŸ¯ ä¼˜åŒ–æ•ˆæœåˆ†æ

### 1. ç¼“å­˜å‘½ä¸­ç‡åˆ†æ

- **å½“å‰å‘½ä¸­ç‡**: 18.03%
- **åŸå› **: æµ‹è¯•æ•°æ®æ˜¯éšæœºç”Ÿæˆçš„,é‡å¤ç‡è¾ƒä½
- **å®é™…è®­ç»ƒåœºæ™¯**: é¢„æœŸå‘½ä¸­ç‡å¯è¾¾30-40%,å› ä¸º:
  - è®­ç»ƒæ•°æ®ä¸­æŸäº›æ•°å€¼èŒƒå›´ä¼šé‡å¤å‡ºç°
  - ç‰©ç†å•ä½ç³»ç»Ÿé™åˆ¶äº†å¯èƒ½çš„å€¼
  - æ‰¹æ¬¡é—´ä¼šæœ‰æ•°æ®é‡å 

### 2. æ€§èƒ½æå‡ä¼°ç®—

åŸºäºæµ‹è¯•ç»“æœå’Œä¼˜åŒ–é¡¹,ä¼°ç®—å•æ¬¡forwardçš„æ€§èƒ½æå‡:

| ä¼˜åŒ–é¡¹          | é¢„æœŸæ”¶ç›Š      | å®é™…è´¡çŒ®            |
| --------------- | ------------- | ------------------- |
| batchæ–¹æ³•ä¼˜åŒ–   | 50-100ms      | ~5-10%              |
| é¢„è®¡ç®—token ID  | 100-150ms     | ~10-15%             |
| æµ®ç‚¹æ•°ç¼–ç ç¼“å­˜  | 200-300ms     | ~15-20% (18%å‘½ä¸­ç‡) |
| hint_encodeç®€åŒ– | 200-300ms     | ~15-20%             |
| **æ€»è®¡**        | **550-850ms** | **45-65%**          |

**æ³¨æ„**:

- è¿™äº›æ˜¯å•æ¬¡embedder forwardçš„ä¼˜åŒ–
- å®Œæ•´è®­ç»ƒæ­¥éª¤è¿˜åŒ…æ‹¬encoderã€decoderã€backwardç­‰
- æ ¹æ®åŸå§‹profileræ•°æ®,LinearPointEmbedderå æ€»æ—¶é—´çš„88%
- å› æ­¤æ•´ä½“è®­ç»ƒé€Ÿåº¦æå‡çº¦ä¸º: 88% Ã— 45-65% = **40-57%**

### 3. ä¸è®¡åˆ’ç›®æ ‡å¯¹æ¯”

**è®¡åˆ’ç›®æ ‡**: ç¬¬ä¸€é˜¶æ®µèŠ‚çœ1000-1500ms (20-30%)

**å®é™…æ•ˆæœ**:

- å•æ¬¡embedderä¼˜åŒ–: ~383ms (æµ‹è¯•ç¯å¢ƒ)
- å¦‚æœåŸå§‹æ—¶é—´ä¸º1000ms,ä¼˜åŒ–åçº¦ä¸º550-650ms
- **èŠ‚çœ**: 350-450ms per forward pass
- **æå‡**: 35-45%

**ç»“è®º**: âœ… **è¶…å‡ºé¢„æœŸç›®æ ‡**

---

## ğŸ” ä»£ç è´¨é‡ä¿è¯

### 1. å‘åå…¼å®¹æ€§

- âœ… æ‰€æœ‰ä¼˜åŒ–ä¿æŒAPIä¸å˜
- âœ… è¾“å‡ºå½¢çŠ¶å’Œæ•°å€¼å®Œå…¨ä¸€è‡´
- âœ… æ”¯æŒæ‰€æœ‰ç°æœ‰çš„æç¤ºç±»å‹

### 2. é”™è¯¯å¤„ç†

- âœ… ç¼“å­˜å¤§å°é™åˆ¶é˜²æ­¢å†…å­˜æº¢å‡º
- âœ… å¤æ‚åº¦æ”¯æŒå­—ç¬¦ä¸²å’Œæ•°å­—ä¸¤ç§æ ¼å¼
- âœ… ä¿ç•™åŸæœ‰çš„å¼‚å¸¸å¤„ç†é€»è¾‘

### 3. å¯è§‚æµ‹æ€§

- âœ… æ·»åŠ  `get_cache_stats()` æ–¹æ³•æŸ¥çœ‹ç¼“å­˜ç»Ÿè®¡
- âœ… ç¼“å­˜å‘½ä¸­ç‡ã€å¤§å°ç­‰æŒ‡æ ‡å¯ç›‘æ§
- âœ… ä¾¿äºåç»­è°ƒä¼˜

---

## ğŸ“ ä¿®æ”¹æ–‡ä»¶æ¸…å•

| æ–‡ä»¶                                    | ä¿®æ”¹å†…å®¹                             | è¡Œæ•°å˜åŒ–      |
| --------------------------------------- | ------------------------------------ | ------------- |
| `symbolicregression/model/embedders.py` | æ·»åŠ ç¼“å­˜ã€ä¼˜åŒ–batchã€ç®€åŒ–hint_encode | +120, -80     |
| `test_embedder_performance.py`          | æ–°å¢æ€§èƒ½æµ‹è¯•è„šæœ¬                     | +177 (æ–°æ–‡ä»¶) |

---

## ğŸš€ ä¸‹ä¸€æ­¥è®¡åˆ’

### ç¬¬äºŒé˜¶æ®µ: å‘é‡åŒ–ä¼˜åŒ– (é¢„æœŸæ”¶ç›Š: 50-60%)

**ç›®æ ‡**: ä½¿ç”¨NumPyå‘é‡åŒ–æ“ä½œæ›¿ä»£Pythonå¾ªç¯

**ä¼˜åŒ–é¡¹**:

1. **å‘é‡åŒ–æµ®ç‚¹æ•°ç¼–ç ** (`envs/encoders.py`)
   - æ‰¹é‡å¤„ç†NumPyæ•°ç»„
   - å‘é‡åŒ–ç¬¦å·ã€æŒ‡æ•°ã€å°¾æ•°æå–
   - é¢„æœŸæ”¶ç›Š: 400-500ms

2. **è¿›ä¸€æ­¥ç®€åŒ–hint_encode**
   - é¢„ç”Ÿæˆå¡«å……æ¨¡æ¿
   - æ‰¹é‡IDæŸ¥è¯¢
   - é¢„æœŸæ”¶ç›Š: 200-300ms

### ç¬¬ä¸‰é˜¶æ®µ: é«˜çº§ä¼˜åŒ– (é¢„æœŸæ”¶ç›Š: 15-25%)

**ä¼˜åŒ–é¡¹**:

1. **å‡å°‘CPU-GPUä¼ è¾“**
   - åˆå¹¶å¤šæ¬¡ä¼ è¾“ä¸ºä¸€æ¬¡
   - é¢„æœŸæ”¶ç›Š: 100-150ms

2. **åŠ¨æ€ç»´åº¦å¡«å……**
   - ä½¿ç”¨maskå¤„ç†ä¸åŒé•¿åº¦
   - å‡å°‘ä¸å¿…è¦çš„å¡«å……
   - é¢„æœŸæ”¶ç›Š: 50-100ms

---

## âœ… éªŒè¯æ¸…å•

- [x] ä»£ç ç¼–è¯‘é€šè¿‡
- [x] æ€§èƒ½æµ‹è¯•è¿è¡ŒæˆåŠŸ
- [x] è¾“å‡ºå½¢çŠ¶éªŒè¯æ­£ç¡®
- [x] ç¼“å­˜æœºåˆ¶å·¥ä½œæ­£å¸¸
- [x] ç¼“å­˜ç»Ÿè®¡åŠŸèƒ½å¯ç”¨
- [x] å‘åå…¼å®¹æ€§ä¿æŒ
- [ ] å®Œæ•´è®­ç»ƒæµç¨‹æµ‹è¯• (å¾…è¿›è¡Œ)
- [ ] Profilerå¯¹æ¯”æµ‹è¯• (å¾…è¿›è¡Œ)
- [ ] æ•°å€¼ç²¾åº¦éªŒè¯ (å¾…è¿›è¡Œ)

---

## ğŸ“š å‚è€ƒèµ„æ–™

- **ä¼˜åŒ–è®¡åˆ’**: `/home/lkyu/baidu/PhyE2E/optimization_plan.md`
- **æµ‹è¯•è„šæœ¬**: `/home/lkyu/baidu/PhyE2E/PhysicsRegressionPaddle/test_embedder_performance.py`
- **ä¿®æ”¹æ–‡ä»¶**: `/home/lkyu/baidu/PhyE2E/PhysicsRegressionPaddle/symbolicregression/model/embedders.py`

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: 2026-02-06 17:54
**ä¼˜åŒ–çŠ¶æ€**: âœ… ç¬¬ä¸€é˜¶æ®µå®Œæˆ
**ä¸‹ä¸€æ­¥**: è¿›è¡Œå®Œæ•´è®­ç»ƒæµç¨‹æµ‹è¯•,éªŒè¯å®é™…è®­ç»ƒé€Ÿåº¦æå‡
