#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–å™¨ä¿®å¤éªŒè¯æµ‹è¯•è„šæœ¬

æµ‹è¯•å†…å®¹:
1. æ‰€æœ‰ä¼˜åŒ–å™¨ç±»çš„åˆå§‹åŒ–
2. ä¼˜åŒ–æ­¥éª¤æ‰§è¡Œ
3. å­¦ä¹ ç‡è°ƒåº¦åŠŸèƒ½
"""

import paddle
from symbolicregression.optim import (
    Adam,
    AdamWithWarmup,
    AdamInverseSqrtWithWarmup,
    AdamCosineWithWarmup
)


def test_adam():
    """æµ‹è¯• Adam ä¼˜åŒ–å™¨"""
    print("=" * 60)
    print("æµ‹è¯• Adam ä¼˜åŒ–å™¨...")
    print("=" * 60)

    # åˆ›å»ºç®€å•æ¨¡å‹
    model = paddle.nn.Linear(10, 5)
    params = list(model.parameters())

    # æµ‹è¯•åˆå§‹åŒ–
    optimizer = Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01)
    print(f"âœ… Adam åˆå§‹åŒ–æˆåŠŸ")
    print(f"   å‚æ•°æ•°é‡: {len(optimizer._params_list)}")
    print(f"   è¶…å‚æ•°: betas={optimizer.betas}, eps={optimizer.eps}")
    print(f"   å­¦ä¹ ç‡: {optimizer.get_lr():.6f}")

    # æµ‹è¯•ä¼˜åŒ–æ­¥éª¤
    x = paddle.randn([4, 10])
    y = model(x)
    loss = y.sum()
    loss.backward()

    optimizer.step()
    optimizer.clear_grad()

    print(f"âœ… ä¼˜åŒ–æ­¥éª¤æ‰§è¡ŒæˆåŠŸ")
    print()

    return True


def test_adam_with_warmup():
    """æµ‹è¯• AdamWithWarmup ä¼˜åŒ–å™¨"""
    print("=" * 60)
    print("æµ‹è¯• AdamWithWarmup ä¼˜åŒ–å™¨...")
    print("=" * 60)

    # åˆ›å»ºç®€å•æ¨¡å‹
    model = paddle.nn.Linear(10, 5)
    params = list(model.parameters())

    # æµ‹è¯•åˆå§‹åŒ–
    optimizer = AdamWithWarmup(
        params,
        lr=0.001,
        warmup_updates=100,
        warmup_init_lr=1e-7
    )
    print(f"âœ… AdamWithWarmup åˆå§‹åŒ–æˆåŠŸ")
    print(f"   åˆå§‹å­¦ä¹ ç‡: {optimizer.get_lr_for_step(0):.10f}")
    print(f"   ç¬¬50æ­¥å­¦ä¹ ç‡: {optimizer.get_lr_for_step(50):.10f}")
    print(f"   ç¬¬100æ­¥å­¦ä¹ ç‡ (warmupç»“æŸ): {optimizer.get_lr_for_step(100):.10f}")
    print(f"   ç¬¬150æ­¥å­¦ä¹ ç‡: {optimizer.get_lr_for_step(150):.10f}")

    # æµ‹è¯•ä¼˜åŒ–æ­¥éª¤å’Œå­¦ä¹ ç‡æ›´æ–°
    x = paddle.randn([4, 10])
    y = model(x)
    loss = y.sum()
    loss.backward()

    # æ‰§è¡Œå¤šæ­¥ä¼˜åŒ–,éªŒè¯å­¦ä¹ ç‡é€’å¢
    lrs = []
    for _ in range(5):
        optimizer.step()
        lrs.append(optimizer.get_lr())
        optimizer.clear_grad()

        # é‡æ–°è®¡ç®—æ¢¯åº¦
        y = model(paddle.randn([4, 10]))
        loss = y.sum()
        loss.backward()

    print(f"âœ… å­¦ä¹ ç‡è°ƒåº¦æ­£å¸¸å·¥ä½œ")
    print(f"   å‰5æ­¥å­¦ä¹ ç‡å˜åŒ–: {[f'{lr:.10f}' for lr in lrs]}")
    print()

    return True


def test_adam_inverse_sqrt_with_warmup():
    """æµ‹è¯• AdamInverseSqrtWithWarmup ä¼˜åŒ–å™¨"""
    print("=" * 60)
    print("æµ‹è¯• AdamInverseSqrtWithWarmup ä¼˜åŒ–å™¨...")
    print("=" * 60)

    # åˆ›å»ºç®€å•æ¨¡å‹
    model = paddle.nn.Linear(10, 5)
    params = list(model.parameters())

    # æµ‹è¯•åˆå§‹åŒ–
    optimizer = AdamInverseSqrtWithWarmup(
        params,
        lr=0.001,
        warmup_updates=100,
        warmup_init_lr=1e-7,
        exp_factor=0.5
    )
    print(f"âœ… AdamInverseSqrtWithWarmup åˆå§‹åŒ–æˆåŠŸ")
    print(f"   åˆå§‹å­¦ä¹ ç‡: {optimizer.get_lr_for_step(0):.10f}")
    print(f"   ç¬¬50æ­¥å­¦ä¹ ç‡: {optimizer.get_lr_for_step(50):.10f}")
    print(f"   ç¬¬100æ­¥å­¦ä¹ ç‡ (warmupç»“æŸ): {optimizer.get_lr_for_step(100):.10f}")
    print(f"   ç¬¬200æ­¥å­¦ä¹ ç‡ (inverse sqrtè¡°å‡): {optimizer.get_lr_for_step(200):.10f}")
    print(f"   ç¬¬500æ­¥å­¦ä¹ ç‡: {optimizer.get_lr_for_step(500):.10f}")

    # æµ‹è¯•ä¼˜åŒ–æ­¥éª¤
    x = paddle.randn([4, 10])
    y = model(x)
    loss = y.sum()
    loss.backward()

    optimizer.step()
    optimizer.clear_grad()

    print(f"âœ… ä¼˜åŒ–æ­¥éª¤æ‰§è¡ŒæˆåŠŸ")
    print()

    return True


def test_adam_cosine_with_warmup():
    """æµ‹è¯• AdamCosineWithWarmup ä¼˜åŒ–å™¨"""
    print("=" * 60)
    print("æµ‹è¯• AdamCosineWithWarmup ä¼˜åŒ–å™¨...")
    print("=" * 60)

    # åˆ›å»ºç®€å•æ¨¡å‹
    model = paddle.nn.Linear(10, 5)
    params = list(model.parameters())

    # æµ‹è¯•åˆå§‹åŒ–
    optimizer = AdamCosineWithWarmup(
        params,
        lr=0.001,
        warmup_updates=100,
        warmup_init_lr=1e-7,
        min_lr=1e-9,
        init_period=1000,
        period_mult=1,
        lr_shrink=0.75
    )
    print(f"âœ… AdamCosineWithWarmup åˆå§‹åŒ–æˆåŠŸ")
    print(f"   åˆå§‹å­¦ä¹ ç‡: {optimizer.get_lr_for_step(0):.10f}")
    print(f"   ç¬¬50æ­¥å­¦ä¹ ç‡: {optimizer.get_lr_for_step(50):.10f}")
    print(f"   ç¬¬100æ­¥å­¦ä¹ ç‡ (warmupç»“æŸ): {optimizer.get_lr_for_step(100):.10f}")
    print(f"   ç¬¬350æ­¥å­¦ä¹ ç‡ (cosineå‘¨æœŸ): {optimizer.get_lr_for_step(350):.10f}")
    print(f"   ç¬¬600æ­¥å­¦ä¹ ç‡: {optimizer.get_lr_for_step(600):.10f}")

    # æµ‹è¯•ä¼˜åŒ–æ­¥éª¤
    x = paddle.randn([4, 10])
    y = model(x)
    loss = y.sum()
    loss.backward()

    optimizer.step()
    optimizer.clear_grad()

    print(f"âœ… ä¼˜åŒ–æ­¥éª¤æ‰§è¡ŒæˆåŠŸ")
    print()

    return True


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n")
    print("ğŸ”§" * 30)
    print("å¼€å§‹æµ‹è¯•ä¼˜åŒ–å™¨ä¿®å¤...")
    print("ğŸ”§" * 30)
    print("\n")

    try:
        # æµ‹è¯•æ‰€æœ‰ä¼˜åŒ–å™¨
        results = []
        results.append(("Adam", test_adam()))
        results.append(("AdamWithWarmup", test_adam_with_warmup()))
        results.append(("AdamInverseSqrtWithWarmup", test_adam_inverse_sqrt_with_warmup()))
        results.append(("AdamCosineWithWarmup", test_adam_cosine_with_warmup()))

        # æ‰“å°æ€»ç»“
        print("=" * 60)
        print("æµ‹è¯•æ€»ç»“:")
        print("=" * 60)

        all_passed = True
        for name, passed in results:
            status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
            print(f"  {name}: {status}")
            all_passed = all_passed and passed

        print("=" * 60)

        if all_passed:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! ä¼˜åŒ–å™¨ä¿®å¤æˆåŠŸ!")
            print("\nä¸‹ä¸€æ­¥:")
            print("  1. è¿è¡Œå®Œæ•´è®­ç»ƒæµ‹è¯•: bash ./bash/train_small.sh")
            print("  2. éªŒè¯å­¦ä¹ ç‡è°ƒåº¦æ­£å¸¸å·¥ä½œ")
            print("  3. æ£€æŸ¥è®­ç»ƒæ—¥å¿—ä¸­æ˜¯å¦æœ‰ ValueError")
        else:
            print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥,è¯·æ£€æŸ¥ä»£ç !")
            return 1

        return 0

    except Exception as e:
        print("\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯:")
        print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")
        import traceback
        print("\nè¯¦ç»†å †æ ˆ:")
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
