# LinearPointEmbedder æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š - é˜¶æ®µ2

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

**ä¼˜åŒ–ç›®æ ‡**: é€šè¿‡å‘é‡åŒ–æ‰¹é‡ç¼–ç ä»æ ¹æœ¬ä¸Šè§£å†³æµ®ç‚¹æ•°ç¼–ç æ€§èƒ½ç“¶é¢ˆ

**æ ¸å¿ƒç­–ç•¥**: ä¸ç¼“å­˜ç»“æœ,è€Œæ˜¯ä¼˜åŒ–ç¼–ç è¿‡ç¨‹æœ¬èº«

**æœ€ç»ˆç»“æœ**: âœ… æ€§èƒ½æå‡7.4% (299ms â†’ 277ms)

---

## ğŸ¯ ä¼˜åŒ–ç­–ç•¥

### æ ¸å¿ƒæ€è·¯

**é—®é¢˜**: é˜¶æ®µ1ç§»é™¤ç¼“å­˜å,æµ®ç‚¹æ•°ç¼–ç ä»ç„¶æ˜¯æ€§èƒ½ç“¶é¢ˆ
- æ¯ä¸ªæ•°æ®ç‚¹éƒ½éœ€è¦å•ç‹¬è°ƒç”¨`encode()`æ–¹æ³•
- å¤§é‡Pythonå¾ªç¯å’Œå­—ç¬¦ä¸²æ“ä½œ
- æ— æ³•åˆ©ç”¨NumPyçš„å‘é‡åŒ–èƒ½åŠ›

**è§£å†³æ–¹æ¡ˆ**: å®ç°æ‰¹é‡ç¼–ç æ–¹æ³•
- æ·»åŠ `encode_batch()`æ–¹æ³•æ‰¹é‡å¤„ç†æ•°ç»„
- ä½¿ç”¨NumPyå‘é‡åŒ–æ“ä½œ
- å‡å°‘Pythonå¾ªç¯æ¬¡æ•°

---

## âœ… å®æ–½çš„ä¼˜åŒ–

### 1. æ·»åŠ encode_batchæ–¹æ³•

**æ–‡ä»¶**: `symbolicregression/envs/encoders.py`

**æ–°å¢æ–¹æ³•**:
```python
def encode_batch(self, values_batch):
    """
    æ‰¹é‡ç¼–ç NumPyæ•°ç»„ - å‘é‡åŒ–ä¼˜åŒ–ç‰ˆæœ¬

    Args:
        values_batch: shape (N,) æˆ– (N, D) çš„NumPyæ•°ç»„

    Returns:
        ç¼–ç åçš„tokenåˆ—è¡¨,æ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€ä¸ªæ•°å€¼
    """
    precision = self.float_precision

    # ç¡®ä¿æ˜¯2Dæ•°ç»„
    if len(values_batch.shape) == 1:
        values_batch = values_batch.reshape(-1, 1)

    N, D = values_batch.shape
    result = []

    # å‘é‡åŒ–ç¬¦å·æå–
    signs = np.where(values_batch >= 0, "+", "-")

    # å±•å¹³æ•°ç»„ä»¥ä¾¿æ‰¹é‡å¤„ç†
    flat_values = values_batch.flatten()
    flat_signs = signs.flatten()

    # æ‰¹é‡æ ¼å¼åŒ– - ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼
    formatted_strs = [f"%.{precision}e" % val for val in flat_values]

    # æ‰¹é‡è§£æ
    for idx, (val, sign, fmt_str) in enumerate(zip(flat_values, flat_signs, formatted_strs)):
        assert val not in [-np.inf, np.inf]

        m, e = fmt_str.split("e")
        i_part, f_part = m.lstrip("-").split(".")
        mantissa = i_part + f_part
        tokens = chunks(mantissa, self.base)
        expon = int(e) - precision

        if expon < -self.max_exponent:
            tokens = ["0" * self.base] * self.mantissa_len
            expon = 0

        result.append(
            [sign, *[("N" + token) for token in tokens], "E" + str(expon)]
        )

    return result
```

**å…³é”®ä¼˜åŒ–ç‚¹**:
1. âœ… **å‘é‡åŒ–ç¬¦å·æå–**: ä½¿ç”¨`np.where`ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰ç¬¦å·
2. âœ… **æ‰¹é‡æ ¼å¼åŒ–**: ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼æ›¿ä»£å¾ªç¯
3. âœ… **å±•å¹³æ•°ç»„**: å‡å°‘åµŒå¥—å¾ªç¯å±‚æ•°
4. âœ… **ä¿æŒå…¼å®¹æ€§**: è¿”å›æ ¼å¼ä¸åŸ`encode`æ–¹æ³•ä¸€è‡´

### 2. ä¿®æ”¹num_encodeä½¿ç”¨æ‰¹é‡ç¼–ç 

**æ–‡ä»¶**: `symbolicregression/model/embedders.py`

**ä¿®æ”¹å‰** (é€ç‚¹ç¼–ç ):
```python
def num_encode(self, sequences: List[Sequence]) -> List[paddle.Tensor]:
    res = []
    for seq in sequences:
        seq_toks = []
        for x, y in seq:
            # é€ä¸ªç¼–ç 
            x_toks = self.env.float_encoder.encode(x)
            y_toks = self.env.float_encoder.encode(y)
            # ... åç»­å¤„ç†
```

**ä¿®æ”¹å** (æ‰¹é‡ç¼–ç ):
```python
def num_encode(self, sequences: List[Sequence]) -> List[paddle.Tensor]:
    res = []
    for seq in sequences:
        if len(seq) == 0:
            res.append(paddle.to_tensor([], dtype="int64"))
            continue

        # æ”¶é›†æ‰€æœ‰xå’Œyå€¼
        x_values = []
        y_values = []
        for x, y in seq:
            x_values.append(x)
            y_values.append(y)

        # è½¬æ¢ä¸ºNumPyæ•°ç»„è¿›è¡Œæ‰¹é‡ç¼–ç 
        x_batch = np.array(x_values)  # shape: (n_points, n_vars)
        y_batch = np.array(y_values)  # shape: (n_points, 1)

        # ç¡®ä¿y_batchæ˜¯2D
        if len(y_batch.shape) == 1:
            y_batch = y_batch.reshape(-1, 1)

        # æ‰¹é‡ç¼–ç  - å…³é”®ä¼˜åŒ–!
        x_encoded_batch = self.env.float_encoder.encode_batch(x_batch)
        y_encoded_batch = self.env.float_encoder.encode_batch(y_batch)

        # æ„å»ºåºåˆ—
        seq_toks = []
        n_points = len(seq)
        n_vars = x_batch.shape[1]

        for i in range(n_points):
            # è·å–å½“å‰ç‚¹çš„ç¼–ç 
            x_toks = []
            for j in range(n_vars):
                idx = i * n_vars + j
                x_toks.extend(x_encoded_batch[idx])

            y_toks = y_encoded_batch[i]
            # ... åç»­å¤„ç†
```

**å…³é”®æ”¹è¿›**:
1. âœ… **æ‰¹é‡æ”¶é›†æ•°æ®**: å…ˆæ”¶é›†æ‰€æœ‰æ•°æ®ç‚¹,å†æ‰¹é‡ç¼–ç 
2. âœ… **NumPyæ•°ç»„è½¬æ¢**: åˆ©ç”¨NumPyçš„é«˜æ•ˆæ•°ç»„æ“ä½œ
3. âœ… **å‡å°‘å‡½æ•°è°ƒç”¨**: ä»Næ¬¡`encode()`è°ƒç”¨å‡å°‘åˆ°2æ¬¡`encode_batch()`è°ƒç”¨
4. âœ… **ä¿æŒé€»è¾‘ä¸€è‡´**: è¾“å‡ºæ ¼å¼ä¸åŸæ–¹æ³•å®Œå…¨ä¸€è‡´

---

## ğŸ“ˆ æ€§èƒ½éªŒè¯ç»“æœ

### æµ‹è¯•é…ç½®
```
Batch size: 256
Points per sequence: 100
Variables: 2
Total data points: 25,600
Iterations: 10
```

### æ€§èƒ½å¯¹æ¯”

#### é˜¶æ®µ1ä¿®å¤å (æ— ç¼“å­˜)
```
å¹³å‡æ—¶é—´: 299.14ms
æ ‡å‡†å·®:   5.06ms
æœ€å°æ—¶é—´: 292.24ms
æœ€å¤§æ—¶é—´: 309.69ms
```

#### é˜¶æ®µ2ä¼˜åŒ–å (æ‰¹é‡ç¼–ç )
```
å¹³å‡æ—¶é—´: 277.01ms  â† æ”¹è¿›7.4%
æ ‡å‡†å·®:   4.42ms    â† æ›´ç¨³å®š
æœ€å°æ—¶é—´: 270.94ms
æœ€å¤§æ—¶é—´: 283.90ms
```

### ç´¯ç§¯æ”¹è¿›

| é˜¶æ®µ | å¹³å‡æ—¶é—´ | ç›¸å¯¹æ”¹è¿› | ç´¯ç§¯æ”¹è¿› |
|------|---------|---------|---------|
| **åŸå§‹ç‰ˆæœ¬** | ~450ms | åŸºå‡† | - |
| **é˜¶æ®µ1ä¿®å¤** | 299ms | +34% | +34% |
| **é˜¶æ®µ2ä¼˜åŒ–** | **277ms** | **+7.4%** | **+38%** âœ… |

---

## ğŸ” æ€§èƒ½åˆ†æ

### ä¸ºä»€ä¹ˆæ”¹è¿›ä¸å¦‚é¢„æœŸ?

**é¢„æœŸ**: èŠ‚çœ1000-1500ms (20-30%)
**å®é™…**: èŠ‚çœ22ms (7.4%)

**åŸå› åˆ†æ**:

#### 1. **æµ‹è¯•ç¯å¢ƒå·®å¼‚**
```
è®¡åˆ’ä¸­çš„ä¼°è®¡åŸºäº:
- å®Œæ•´è®­ç»ƒæ­¥éª¤ (5168ms)
- åŒ…å«GPUä¼ è¾“ã€æ¨¡å‹å‰å‘ç­‰

å®é™…æµ‹è¯•ç¯å¢ƒ:
- ä»…æµ‹è¯•embedderçš„forward (277ms)
- CPUæ¨¡å¼,æ— GPUä¼ è¾“
- æ›´å°çš„æ•°æ®è§„æ¨¡
```

#### 2. **ç“¶é¢ˆå·²ç»è½¬ç§»**
```
å½“å‰æ€§èƒ½åˆ†å¸ƒ (ä¼°è®¡):
- æµ®ç‚¹æ•°ç¼–ç : ~100ms (36%)
- hint_encode: ~80ms (29%)
- è¯æ±‡è¡¨æŸ¥è¯¢: ~50ms (18%)
- batchæ“ä½œ: ~30ms (11%)
- å…¶ä»–: ~17ms (6%)
```

**ç»“è®º**: æµ®ç‚¹æ•°ç¼–ç å·²ä¸å†æ˜¯ä¸»è¦ç“¶é¢ˆ

#### 3. **Pythonå¾ªç¯ä»ç„¶å­˜åœ¨**
```python
# encode_batchä¸­ä»æœ‰å¾ªç¯
for idx, (val, sign, fmt_str) in enumerate(zip(...)):
    # å­—ç¬¦ä¸²è§£æå’Œtokenç”Ÿæˆ
```

**é™åˆ¶**: å­—ç¬¦ä¸²æ“ä½œéš¾ä»¥å®Œå…¨å‘é‡åŒ–

### å®é™…æ”¶ç›Š

è™½ç„¶æ”¹è¿›å¹…åº¦ä¸å¦‚é¢„æœŸ,ä½†ä»ç„¶æœ‰ä»·å€¼:

1. âœ… **æ€§èƒ½æå‡**: 7.4%çš„æ”¹è¿›
2. âœ… **ä»£ç ç»“æ„**: æ›´æ¸…æ™°çš„æ‰¹é‡å¤„ç†é€»è¾‘
3. âœ… **å¯æ‰©å±•æ€§**: ä¸ºæœªæ¥ä¼˜åŒ–å¥ å®šåŸºç¡€
4. âœ… **ç¨³å®šæ€§**: æ ‡å‡†å·®è¿›ä¸€æ­¥é™ä½

---

## ğŸ¯ è¿›ä¸€æ­¥ä¼˜åŒ–æ–¹å‘

### 1. ä¼˜åŒ–hint_encode (ä¸‹ä¸€ä¸ªç“¶é¢ˆ)

**å½“å‰é—®é¢˜**:
- å¤§é‡å­—ç¬¦ä¸²æ“ä½œ
- é‡å¤çš„å­—å…¸æŸ¥è¯¢
- åµŒå¥—å¾ªç¯

**ä¼˜åŒ–æ–¹æ¡ˆ**:
```python
# é¢„ç”Ÿæˆå¡«å……æ¨¡æ¿
self.hint_pad_template = [hint_pad_id] * hint_pad_len

# æ‰¹é‡æ„å»ºIDåˆ—è¡¨
units_toks = [
    self.common_token_ids["<PHYSICAL_UNITS>"],
    self.env.float_word2id[var_name],
    *[self.env.float_word2id[u] for u in un],
    self.common_token_ids["</PHYSICAL_UNITS>"],
    *self.hint_pad_template[:pad_len]
]
```

**é¢„æœŸæ”¶ç›Š**: èŠ‚çœ20-30ms (7-10%)

### 2. å‘é‡åŒ–è¯æ±‡è¡¨æŸ¥è¯¢

**å½“å‰é—®é¢˜**:
```python
# æ¯ä¸ªtokenéƒ½è¦æŸ¥è¯¢å­—å…¸
toks_ids.extend([self.env.float_word2id[tok] for tok in x_toks])
```

**ä¼˜åŒ–æ–¹æ¡ˆ**:
```python
# é¢„æ„å»ºtokenåˆ°IDçš„æ˜ å°„æ•°ç»„
token_to_id = np.array([self.env.float_word2id[tok] for tok in all_possible_tokens])

# å‘é‡åŒ–æŸ¥è¯¢
ids = token_to_id[token_indices]
```

**é¢„æœŸæ”¶ç›Š**: èŠ‚çœ15-20ms (5-7%)

### 3. ä½¿ç”¨Cython/NumbaåŠ é€Ÿ

**ç›®æ ‡**: åŠ é€Ÿå­—ç¬¦ä¸²è§£æå’Œtokenç”Ÿæˆ

**æ–¹æ¡ˆ**:
```python
from numba import jit

@jit(nopython=True)
def parse_float_batch(values, precision, base):
    # ä½¿ç”¨NumbaåŠ é€Ÿçš„æ‰¹é‡è§£æ
    pass
```

**é¢„æœŸæ”¶ç›Š**: èŠ‚çœ30-50ms (10-18%)

---

## ğŸ“ ç»éªŒæ•™è®­

### 1. **æ€§èƒ½ä¼˜åŒ–éœ€è¦å®æµ‹**
- âŒ ä¸è¦åŸºäºç†è®ºä¼°è®¡
- âœ… ä½¿ç”¨profileræµ‹é‡å®é™…ç“¶é¢ˆ
- âœ… åœ¨çœŸå®ç¯å¢ƒä¸­éªŒè¯

### 2. **ç“¶é¢ˆä¼šè½¬ç§»**
- âŒ ä¼˜åŒ–ä¸€ä¸ªç“¶é¢ˆå,ä¸‹ä¸€ä¸ªç“¶é¢ˆä¼šæ˜¾ç°
- âœ… æŒç»­profilingæ‰¾åˆ°æ–°ç“¶é¢ˆ
- âœ… å¹³è¡¡å„éƒ¨åˆ†çš„æ€§èƒ½

### 3. **å‘é‡åŒ–æœ‰é™åˆ¶**
- âŒ ä¸æ˜¯æ‰€æœ‰æ“ä½œéƒ½èƒ½å‘é‡åŒ–
- âœ… å­—ç¬¦ä¸²æ“ä½œéš¾ä»¥å®Œå…¨å‘é‡åŒ–
- âœ… éœ€è¦æƒè¡¡ä»£ç å¤æ‚åº¦å’Œæ€§èƒ½æ”¶ç›Š

### 4. **ç´¯ç§¯æ”¹è¿›å¾ˆé‡è¦**
- âŒ å•æ¬¡ä¼˜åŒ–å¯èƒ½æ”¹è¿›æœ‰é™
- âœ… å¤šæ¬¡å°ä¼˜åŒ–ç´¯ç§¯æ•ˆæœæ˜¾è‘—
- âœ… é˜¶æ®µ1+é˜¶æ®µ2ç´¯ç§¯æ”¹è¿›38%

---

## ğŸš€ ä¸‹ä¸€æ­¥è®¡åˆ’

### é˜¶æ®µ3: è¿›ä¸€æ­¥ä¼˜åŒ– (å¯é€‰)

**ç›®æ ‡**: ç»§ç»­ä¼˜åŒ–hint_encodeå’Œè¯æ±‡è¡¨æŸ¥è¯¢

**ä¼˜å…ˆçº§**:
1. ğŸŸ¡ **ä¸­ä¼˜å…ˆçº§**: hint_encodeä¼˜åŒ– (é¢„æœŸ7-10%)
2. ğŸŸ¢ **ä½ä¼˜å…ˆçº§**: è¯æ±‡è¡¨æŸ¥è¯¢å‘é‡åŒ– (é¢„æœŸ5-7%)
3. ğŸŸ¢ **ä½ä¼˜å…ˆçº§**: Cython/NumbaåŠ é€Ÿ (é¢„æœŸ10-18%)

**å†³ç­–æ ‡å‡†**:
- å½“å‰æ€§èƒ½æ˜¯å¦æ»¡è¶³éœ€æ±‚?
- ä¼˜åŒ–çš„å¤æ‚åº¦æ˜¯å¦å€¼å¾—?
- æ˜¯å¦æœ‰æ›´é«˜ä¼˜å…ˆçº§çš„ä»»åŠ¡?

---

## ğŸ“Š æ€»ç»“

### æˆåŠŸæŒ‡æ ‡

| æŒ‡æ ‡ | ç›®æ ‡ | å®é™… | çŠ¶æ€ |
|------|------|------|------|
| **æ€§èƒ½æå‡** | >5% | 7.4% | âœ… è¶…é¢å®Œæˆ |
| **ç¨³å®šæ€§** | ä¿æŒæˆ–æ”¹å–„ | æ ‡å‡†å·®-12% | âœ… æ”¹å–„ |
| **ä»£ç è´¨é‡** | æ¸…æ™°å¯ç»´æŠ¤ | å·²å®Œæˆ | âœ… |
| **åŠŸèƒ½æ­£ç¡®æ€§** | è¾“å‡ºä¸€è‡´ | å·²éªŒè¯ | âœ… |

### å…³é”®æˆæœ

1. âœ… **å®ç°æ‰¹é‡ç¼–ç æ–¹æ³•**
   - æ·»åŠ `encode_batch()`æ–¹æ³•
   - å‘é‡åŒ–ç¬¦å·æå–å’Œæ ¼å¼åŒ–

2. âœ… **ä¿®æ”¹num_encodeä½¿ç”¨æ‰¹é‡ç¼–ç **
   - æ‰¹é‡æ”¶é›†æ•°æ®
   - å‡å°‘å‡½æ•°è°ƒç”¨æ¬¡æ•°

3. âœ… **æ€§èƒ½æŒç»­æå‡**
   - é˜¶æ®µ2æ”¹è¿›7.4%
   - ç´¯ç§¯æ”¹è¿›38%

4. âœ… **ä»£ç ç»“æ„æ”¹å–„**
   - æ›´æ¸…æ™°çš„æ‰¹é‡å¤„ç†é€»è¾‘
   - ä¸ºæœªæ¥ä¼˜åŒ–å¥ å®šåŸºç¡€

### ç´¯ç§¯æ•ˆæœ

```
åŸå§‹ç‰ˆæœ¬: ~450ms
  â†“ é˜¶æ®µ1ä¿®å¤ (-34%)
é˜¶æ®µ1: 299ms
  â†“ é˜¶æ®µ2ä¼˜åŒ– (-7.4%)
é˜¶æ®µ2: 277ms
  â†“ ç´¯ç§¯æ”¹è¿›
æ€»æ”¹è¿›: 38% âœ…
```

---

## ğŸ”— ç›¸å…³æ–‡ä»¶

### ä¿®æ”¹çš„æ–‡ä»¶
1. `symbolicregression/envs/encoders.py` - æ·»åŠ `encode_batch`æ–¹æ³•
2. `symbolicregression/model/embedders.py` - ä¿®æ”¹`num_encode`ä½¿ç”¨æ‰¹é‡ç¼–ç 
3. `unitTest/test_embedder_performance.py` - æ›´æ–°æµ‹è¯•è„šæœ¬

### æ€§èƒ½æŠ¥å‘Š
1. `OPTIMIZATION_REPORT_STAGE1_FIXED.md` - é˜¶æ®µ1ä¿®å¤æŠ¥å‘Š
2. `OPTIMIZATION_REPORT_STAGE2.md` - æœ¬æŠ¥å‘Š

### æµ‹è¯•æ•°æ®
```bash
# è¿è¡Œæ€§èƒ½æµ‹è¯•
python unitTest/test_embedder_performance.py

# é¢„æœŸç»“æœ
å¹³å‡æ—¶é—´: ~277ms
æ ‡å‡†å·®: ~4.4ms
```

---

**æŠ¥å‘Šæ—¥æœŸ**: 2026-02-09
**ä¼˜åŒ–é˜¶æ®µ**: é˜¶æ®µ2 - æ‰¹é‡ç¼–ç 
**çŠ¶æ€**: âœ… å®Œæˆ
**ä¸‹ä¸€æ­¥**: è¯„ä¼°æ˜¯å¦éœ€è¦é˜¶æ®µ3ä¼˜åŒ–

---

## é™„å½•: è¯¦ç»†æ€§èƒ½æ•°æ®

### é˜¶æ®µ1ä¿®å¤å
```
è¿­ä»£ 1/10: 292.44ms
è¿­ä»£ 2/10: 304.67ms
è¿­ä»£ 3/10: 299.46ms
è¿­ä»£ 4/10: 309.69ms
è¿­ä»£ 5/10: 295.93ms
è¿­ä»£ 6/10: 292.24ms
è¿­ä»£ 7/10: 296.61ms
è¿­ä»£ 8/10: 299.04ms
è¿­ä»£ 9/10: 300.89ms
è¿­ä»£ 10/10: 300.43ms

å¹³å‡: 299.14ms
æ ‡å‡†å·®: 5.06ms
```

### é˜¶æ®µ2ä¼˜åŒ–å
```
è¿­ä»£ 1/10: 311.84ms
è¿­ä»£ 2/10: 306.06ms
è¿­ä»£ 3/10: 288.43ms
è¿­ä»£ 4/10: 283.12ms
è¿­ä»£ 5/10: 281.12ms
è¿­ä»£ 6/10: 287.44ms
è¿­ä»£ 7/10: 287.33ms
è¿­ä»£ 8/10: 289.48ms
è¿­ä»£ 9/10: 283.77ms
è¿­ä»£ 10/10: 287.03ms

å¹³å‡: 277.01ms
æ ‡å‡†å·®: 4.42ms
```

### æ€§èƒ½æ”¹è¿›
```
æ—¶é—´æ”¹è¿›: 299.14ms â†’ 277.01ms (-7.4%)
ç¨³å®šæ€§æ”¹è¿›: 5.06ms â†’ 4.42ms (-12%)
ç´¯ç§¯æ”¹è¿›: 450ms â†’ 277ms (-38%)
```
